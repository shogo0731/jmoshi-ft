# Moshi/J-Moshi Finetuning

[**English README**](README.md) | [**æ—¥æœ¬èª README**](README-ja.md)

Kyutai ãŒææ¡ˆã—ãŸ Full-duplex éŸ³å£°å¯¾è©±ãƒ¢ãƒ‡ãƒ« Moshi ã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ãŸã‚ã®éå…¬å¼ãƒªãƒã‚¸ãƒˆãƒªã§ã™ã€‚RQ-Transformer ã‚’æ‰€æœ›ã®éŸ³å£°å¯¾è©±ãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ï¼ã¾ãŸï¼ŒMoshi ã‚’ãƒ™ãƒ¼ã‚¹ã«æ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’ã•ã‚ŒãŸ J-Moshiï¼ˆè©³ã—ãã¯ï¼Œ[Finetuned Model](#finetuned-model)ã‚’å‚ç…§ï¼‰ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚‚å¯èƒ½ã§ã™ï¼æœ¬ãƒªãƒã‚¸ãƒˆãƒªã®å­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ï¼Œä»¥ä¸‹ã§å…¬é–‹ã•ã‚Œã¦ã„ã‚‹ï¼ŒMoshiã®å…¬å¼ã®[ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«ãƒ¬ãƒãƒ¼ãƒˆ](https://arxiv.org/abs/2410.00037)ãŠã‚ˆã³ [Pytorch ãƒ¢ãƒ‡ãƒ«](https://github.com/kyutai-labs/moshi)ã‚’ãƒ™ãƒ¼ã‚¹ã«å†ç¾å®Ÿè£…ã•ã‚Œã¾ã—ãŸï¼š

ãªãŠï¼Œ[å…¬å¼ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚³ãƒ¼ãƒ‰](https://github.com/kyutai-labs/moshi-finetune) ã¨æ¯”è¼ƒã—ãŸéš›ã®ï¼Œæœ¬ãƒªãƒã‚¸ãƒˆãƒªã®ç‰¹å¾´ã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™ï¼š
- åˆ†æ•£å­¦ç¿’ã®å®Ÿè£…ã«ãŠã„ã¦ã¯ï¼Œå…¬å¼ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã§ç”¨ã„ã‚‰ã‚Œã¦ã„ã‚‹ [Fully Sharded Data Parallel (FSDP)](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/) ã§ã¯ãªãï¼Œ[ğŸ¤— Accelerate](https://github.com/huggingface/accelerate) ã¨ [DeepSpeed](https://github.com/deepspeedai/DeepSpeed) ã‚’ç”¨ã„ã¦ã„ã¾ã™ï¼ã“ã‚Œã‚‰ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã«æ…£ã‚Œã¦ã„ã‚‹æ–¹ã«ã¯ãŠã™ã™ã‚ã§ã™ï¼
- æˆ‘ã€…ã®ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã§ã¯ï¼Œå…¬å¼ã®ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«ãƒ¬ãƒãƒ¼ãƒˆã¨åŒæ§˜ã«ï¼ŒMoshi è‡ªèº«ã®ã‚¹ãƒˆãƒªãƒ¼ãƒ ã¨ä¸¦åˆ—ã—ã¦ãƒ¦ãƒ¼ã‚¶ã®éŸ³å£°ã‚¹ãƒˆãƒªãƒ¼ãƒ ã®ç”Ÿæˆã‚‚å­¦ç¿’ã—ã¾ã™ï¼ã“ã‚Œã«ã‚ˆã£ã¦ï¼Œprompted dialogue continuation ãªã©ã®è‡ªå‹•è©•ä¾¡ãŒå®¹æ˜“ã«ãªã‚Šã¾ã™ï¼

## Finetuned Model
ã“ã®ãƒªãƒã‚¸ãƒˆãƒªã‚’ç”¨ã„ãŸ Moshi ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã«ã‚ˆã£ã¦ï¼Œæ—¥æœ¬èªç‰ˆãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚‹ J-Moshi ãŒæ§‹ç¯‰ã•ã‚Œã¾ã—ãŸï¼J-Moshi ã¯ï¼Œ69k æ™‚é–“ã®æ—¥æœ¬èªéŸ³å£°å¯¾è©±ã‚’å«ã‚€ [J-CHAT ã‚³ãƒ¼ãƒ‘ã‚¹](https://huggingface.co/datasets/sarulab-speech/J-CHAT)ï¼‰ï¼ŒãŠã‚ˆã³ï¼Œæ•°ç™¾æ™‚é–“ã®æ—¥æœ¬èªéŸ³å£°å¯¾è©±ã‚³ãƒ¼ãƒ‘ã‚¹ã§å­¦ç¿’ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã§ã™ï¼J-CHAT ã®å­¦ç¿’ã§ã¯ï¼Œ128æšã®NVIDIA V100 32GB GPU ã‚’ç”¨ã„ã¦ï¼ŒãŠã‚ˆã36æ™‚é–“ã‚’è¦ã—ã¾ã—ãŸï¼J-Moshi ã®è©³ç´°ã¯ä»¥ä¸‹ã®ãƒªãƒ³ã‚¯ã‚’å‚ç…§ã—ã¦ãã ã•ã„ï¼š
- [ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆ](https://nu-dialogue.github.io/j-moshi)
- [å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«](https://huggingface.co/nu-dialogue/j-moshi-ext)
- [å­¦ç¿’ãƒ­ã‚°](https://api.wandb.ai/links/ohashi56225/ty0dw2il)


## Environment Setup
Python 3.12+ required

### Dependencies
#### Option 1. Install with uv (recommended)
```bash
uv sync --python 3.12
```
uv ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ–¹æ³•ã‚„ä½¿ç”¨æ–¹æ³•ã¯ï¼Œ[å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://docs.astral.sh/uv/getting-started/)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ï¼

#### Option 2. Install with pip
```bash
pip install -r requirements.txt
```

### Experiment Tracking
å­¦ç¿’ãƒ­ã‚°ã®ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã«ã¯ [Weights & Biases (W&B)](https://wandb.ai/site) ã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ï¼W&B ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã¯ï¼Œä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã‚’ç”¨ã„ã¦ï¼ŒW&Dã®ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã«ãƒ­ã‚°ã‚¤ãƒ³ã—ã¦ãã ã•ã„ï¼š
```bash
wandb login
```

## Usage
ã“ã“ã§ã¯ï¼Œ[kyutai/moshiko-pytorch-bf16](https://huggingface.co/kyutai/moshiko-pytorch-bf16) ã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ãŸã‚ã®æ‰‹é †ã‚’èª¬æ˜ã—ã¾ã™ï¼ãªãŠï¼Œã“ã®æ‰‹é †ã§ã¯ï¼Œuv ã‚’ä½¿ç”¨ã—ãŸä¾‹ã‚’ç¤ºã—ã¾ã™ï¼uv ã‚’ä½¿ç”¨ã—ãªã„å ´åˆã¯ï¼Œ`uv run` ã‚’ `python` ã«ç½®ãæ›ãˆã‚‹ãªã©ï¼Œè‡ªåˆ†ã®ç’°å¢ƒã«åˆã£ãŸã‚³ãƒãƒ³ãƒ‰ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ï¼

### 1. Data Preparation
éŸ³å£°å¯¾è©±ãƒ‡ãƒ¼ã‚¿ï¼ŒãŠã‚ˆã³ï¼Œãã®æ›¸ãèµ·ã“ã—ãƒ†ã‚­ã‚¹ãƒˆã‚’é›¢æ•£ãƒˆãƒ¼ã‚¯ãƒ³ã«ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºã—ã¦ï¼Œãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ§‹ç¯‰ã—ã¾ã™ï¼ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ï¼Œ[SpokenWOZ](https://arxiv.org/abs/2305.13040) ã‹ã‚‰æŠ½å‡ºã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¾ã™ï¼è©³ç´°ã¯ [`data/spokenwoz_sample`](data/spokenwoz_sample) ã‚’å‚ç…§ã—ã¦ãã ã•ã„ï¼

#### 1.1. Audio Tokenization
éŸ³å£°å¯¾è©±ãƒ‡ãƒ¼ã‚¿ã¯ï¼Œå„å¯¾è©±ã«ãŠã‘ã‚‹2è©±è€…ï¼ˆA and Bï¼‰ã®éŸ³å£°ãŒãã‚Œãã‚Œã®ãƒãƒ£ãƒãƒ«ã«åˆ†é›¢ã•ã‚ŒãŸwavãƒ•ã‚¡ã‚¤ãƒ«ã§ã™ï¼å·¦ãƒãƒ£ãƒãƒ«ã«è©±è€…Aã®éŸ³å£°ï¼Œå³ãƒãƒ£ãƒãƒ«ã«è©±è€…Bã®éŸ³å£°ãŒå«ã¾ã‚Œã¦ã„ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ï¼1ã¤ã®wavãƒ•ã‚¡ã‚¤ãƒ«ã«1ã¤ã®å¯¾è©±ãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚“ã§ãã ã•ã„ï¼
å…·ä½“çš„ãªãƒ‡ãƒ¼ã‚¿ã®ä¾‹ã¯ `data/spokenwoz_sample/audio/*.wav` ã‚’å‚ç…§ã—ã¦ãã ã•ã„ï¼

ä»¥ä¸‹ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ä½¿ç”¨ã—ã¦ï¼Œãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ã™ã¹ã¦ã®wavãƒ•ã‚¡ã‚¤ãƒ«ã‚’ Mimi ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã«ã‚ˆã£ã¦é›¢æ•£ãƒˆãƒ¼ã‚¯ãƒ³ã«å¤‰æ›ã—ã¾ã™ï¼
```bash
uv run -m tools.tokenize_audio \
    --audio_dir data/spokenwoz_sample/audio \
    --output_dir data/spokenwoz_sample/tokenized_audio
```

ã“ã‚Œã«ã‚ˆã‚Šï¼Œ`data/spokenwoz_sample/tokenized_audio/*.npz` ãŒä½œæˆã•ã‚Œã¾ã™ï¼å„npzãƒ•ã‚¡ã‚¤ãƒ«ã«ã¯ï¼ŒAã¨Bã®éŸ³å£°ãƒˆãƒ¼ã‚¯ãƒ³ãŒä¿å­˜ã•ã‚Œã¦ã„ã¾ã™ï¼š
```python
>>> import numpy as np
>>> npz = np.load("data/spokenwoz_sample/tokenized_audio/SNG0072.npz")
>>> npz["A"].shape[0]
8 # levels of residual vector quantization
>>> npz["A"].shape[1]
1271 # frames of audio token streams (12.5Hz)
```

#### 1.2. Text Tokenization
ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ï¼ˆjsonï¼‰ã¯ï¼Œã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ãŒä»˜ä¸ã•ã‚ŒãŸå˜èªå˜ä½ã®æ›¸ãèµ·ã“ã—ãƒ‡ãƒ¼ã‚¿ã§ã™ï¼1ã¤ã®jsonãƒ•ã‚¡ã‚¤ãƒ«ã«1ã¤ã®å¯¾è©±ãƒ‡ãƒ¼ã‚¿ï¼Œã™ãªã‚ã¡ï¼Œ2è©±è€…ï¼ˆAã¨Bï¼‰ã®ä¸¡æ–¹ã®æ›¸ãèµ·ã“ã—ãŒå«ã¾ã‚Œã¦ã„ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ï¼ãƒ‡ãƒ¼ã‚¿ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™ï¼š
```json
[
  {"speaker": "A", "word": "hello", "start": 0.46, "end": 1.52},
  {"speaker": "B", "word": "hi", "start": 1.82, "end": 2.04},
  {"speaker": "B", "word": "customer", "start": 2.04, "end": 2.703},
  {"speaker": "B", "word": "service", "start": 2.703, "end": 3.145},
  {"speaker": "B", "word": "how", "start": 3.145, "end": 3.366},
  ...
]
```
ã“ã®ä¾‹ã®ã‚ˆã†ã«ï¼Œå„å˜èªã«ã¯ï¼Œè©±è€…ï¼ˆ`speaker`ï¼‰ï¼Œå˜èªï¼ˆ`word`ï¼‰ï¼Œé–‹å§‹æ™‚é–“ï¼ˆ`start`ï¼‰ï¼Œçµ‚äº†æ™‚é–“ï¼ˆ`end`ï¼‰ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ï¼`start`ãŠã‚ˆã³`end`ã¯ï¼Œå¯¾å¿œã™ã‚‹wavãƒ•ã‚¡ã‚¤ãƒ«ã«ãŠã‘ã‚‹ç§’æ•°ã‚’ç¤ºã—ã¾ã™ï¼å…·ä½“çš„ãªãƒ‡ãƒ¼ã‚¿ã®ä¾‹ã¯ `data/spokenwoz_sample/text/*.json` ã‚’å‚ç…§ã—ã¦ãã ã•ã„ï¼ãªãŠï¼Œãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒå˜èªå˜ä½ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’å«ã‚“ã§ã„ãªã„å ´åˆã¯ï¼Œ[forced alignment](https://pytorch.org/audio/stable/tutorials/forced_alignment_tutorial.html) ãŒå®Ÿè£…ã•ã‚ŒãŸå¤–éƒ¨ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼ˆä¾‹ãˆã°ï¼Œ[WhisperX](https://github.com/m-bain/whisperX)ãªã©ï¼‰ã‚’ä½¿ç”¨ã—ã¦ä½œæˆã—ã¦ãã ã•ã„ï¼ã“ã‚Œã‚‰ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®è©³ã—ã„ä½¿ã„æ–¹ã«ã¤ã„ã¦ã¯ï¼Œãã‚Œãã‚Œã®ãƒªãƒã‚¸ãƒˆãƒªã‚’å‚ç…§ã—ã¦ãã ã•ã„ï¼

ä»¥ä¸‹ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ä½¿ç”¨ã—ã¦ï¼Œãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ã™ã¹ã¦ã®jsonãƒ•ã‚¡ã‚¤ãƒ«ã«ã¤ã„ã¦ï¼Œãã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³ã«å¤‰æ›ã—ã¾ã™ï¼š
```bash
uv run -m tools.tokenize_text \
    --word_transcript data/spokenwoz_sample/text \
    --output_dir data/spokenwoz_sample/tokenized_text
```

ã“ã‚Œã«ã‚ˆã‚Šï¼Œ`data/spokenwoz_sample/tokenized_text/*.npz` ãŒä½œæˆã•ã‚Œã¾ã™ï¼å„npzãƒ•ã‚¡ã‚¤ãƒ«ã«ã¯ï¼ŒAã¨Bã®ãƒ†ã‚­ã‚¹ãƒˆãƒˆãƒ¼ã‚¯ãƒ³ãŒä¿å­˜ã•ã‚Œã¦ã„ã¾ã™ï¼š
```python
>>> import numpy as np
>>> npz = np.load("data/spokenwoz_sample/tokenized_audio/SNG0072.npz")
>>> npz["A"].shape
(1271,) # frames of text token stream (12.5Hz)
```

#### Tips: Use Other Text Tokenizers
KyutaiãŒæä¾›ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ä»¥å¤–ã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã‚’ä½¿ç”¨ã—ãŸã„å ´åˆã¯ï¼Œ`--text_tokenizer_repo` ãŠã‚ˆã³ `--text_tokenizer_name` ã‚’å¤‰æ›´ã—ã¦ãã ã•ã„ï¼ãªãŠãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã¨ã—ã¦ã¯ SentencePieceãƒ¢ãƒ‡ãƒ«ã®ã¿ãŒã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã™ï¼ä¾‹ãˆã°ï¼ŒJ-Moshi ã§ã¯ä»¥ä¸‹ã®è¨­å®šã‚’ç”¨ã„ã¾ã—ãŸï¼š
```bash
uv run -m tools.tokenize_text \
    --word_transcript /path/to/japanese_corpus/text \
    --output_dir /path/to/japanese_corpus/tokenized_text \
    --text_tokenizer_repo rinna/japanese-gpt2-medium \
    --text_tokenizer_name spiece.model \
    --text_padding_id 3 \
    --end_of_text_padding_id 0 \
    --no_whitespace_before_word
```

> [!IMPORTANT]
> ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã®å¤‰æ›´ã«ä¼´ã„ï¼Œpadding ãƒˆãƒ¼ã‚¯ãƒ³ã® IDï¼ˆ`--text_padding_id`ï¼‰ï¼ŒãŠã‚ˆã³ end of padding ãƒˆãƒ¼ã‚¯ãƒ³ã® ID ï¼ˆ`--end_of_text_padding_id`ï¼‰ã‚‚å¤‰æ›´ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ï¼ã¾ãŸï¼Œæ—¥æœ¬èªã‚„ä¸­å›½èªãªã©ï¼Œå˜èªé–“ã«ã‚¹ãƒšãƒ¼ã‚¹ãŒãªã„è¨€èªã®å ´åˆ `--no_whitespace_before_word` ãƒ•ãƒ©ã‚°ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ï¼

#### 1.3. Concatenation of Audio and Text Tokens
éŸ³å£°ãƒˆãƒ¼ã‚¯ãƒ³åˆ—ã¨ãƒ†ã‚­ã‚¹ãƒˆãƒˆãƒ¼ã‚¯ãƒ³åˆ—ã‚’çµåˆã—ï¼Œãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆã—ã¾ã™ï¼
```bash
uv run -m tools.prepare_dataset \
    --tokenized_text_dir data/spokenwoz_sample/tokenized_text \
    --tokenized_audio_dir data/spokenwoz_sample/tokenized_audio \
    --output_prefix processed_data/spokenwoz_sample/train
```
ä»¥ä¸Šã®ã‚³ãƒãƒ³ãƒ‰ã«ã‚ˆã‚Šï¼Œ`processed_data/spokenwoz_sample/train-001-of-001.parquet` ãŒä½œæˆã•ã‚Œã¾ã™ï¼ä¸€ã¤ã®parquetãƒ•ã‚¡ã‚¤ãƒ«ã«ã¯ï¼Œæœ€å¤§100,000ã®å¯¾è©±ãŒå«ã¾ã‚Œã¾ã™ï¼ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ§‹é€ ã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™ï¼š
```python
>>> import numpy as np
>>> from datasets import load_dataset
>>> dataset = load_dataset("parquet", data_files="processed_data/spokenwoz_sample/train-001-of-001.parquet")["train"]
>>> dataset
Dataset({
    features: ['dialogue_id', 'A', 'B'],
    num_rows: 10
})
>>> dataset[0]["dialogue_id"]
'processed_data/spokenwoz_sample/train/SNG1640'
>>> np.array(dataset[0]["A"]).shape[0]
9 # 1 text stream + 8 audio streams
>>> np.array(dataset[0]["A"]).shape[1]
1036 # frames of text/audio token stream (12.5Hz)
```


### 2. Model Initialization
ã“ã“ã§ã¯ï¼ŒKyutaãŒå…¬é–‹ã—ã¦ã„ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’ï¼Œãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨ã«åˆæœŸåŒ–ãƒ»ç·¨é›†ã—ã¾ã™ï¼ã“ã®å‡¦ç†ã«ã¯ä¸»ã«ä»¥ä¸‹ã®è¦ç´ ãŒå«ã¾ã‚Œã¾ã™ï¼š
- ï¼ˆãƒ†ã‚­ã‚¹ãƒˆãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã‚’å¤‰æ›´ã—ãŸå ´åˆï¼‰ãƒ†ã‚­ã‚¹ãƒˆãƒˆãƒ¼ã‚¯ãƒ³ã®åŸ‹ã‚è¾¼ã¿ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’åˆæœŸåŒ–
    - `--init_text_embeddings` ãƒ•ãƒ©ã‚°ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ï¼
    - **ç¾çŠ¶ã§ã¯ï¼Œèªå½™ã‚µã‚¤ã‚ºã®å¤‰æ›´ã«ã¯å¯¾å¿œã—ã¦ã„ã¾ã›ã‚“ï¼å¿…ãšèªå½™ã‚µã‚¤ã‚ºãŒåŒã˜ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ï¼**
- Depth Transformer ã«å¯¾ã—ã¦ï¼Œãƒ¦ãƒ¼ã‚¶ã®éŸ³å£°ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’å‡ºåŠ›ã™ã‚‹ãŸã‚ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’è¿½åŠ 
    - `--extend_modules_for_user_stream` ãƒ•ãƒ©ã‚°ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„
- DeepSpeed Zero 3 ã§å­¦ç¿’ã™ã‚‹ãŸã‚ï¼ŒTransformer å†…ã®ä¸€éƒ¨ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ä¿®æ­£ï¼ˆMonkey patchï¼‰

ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã—ã¦ï¼Œãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–ã—ã¾ã™ï¼
```bash
uv run -m tools.init_moshi_for_ft \
    --moshi_lm_repo kyutai/moshiko-pytorch-bf16 \
    --save_dir init_models/moshiko-both_streams-float32 \
    --model_dtype float32 \
    --extend_modules_for_user_stream
```
ã“ã‚Œã«ã‚ˆã‚Šï¼Œ`init_models/moshiko-both_streams-float32` ã«åˆæœŸåŒ–ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ï¼ˆ`model.safetensors`ï¼‰ï¼ŒãŠã‚ˆã³ãã®è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ`moshi_lm_kwargs.json`ï¼‰ãŒä¿å­˜ã•ã‚Œã¾ã™ï¼
ãªãŠï¼Œbfloat16 ã«å¯¾å¿œã—ãªã„GPUã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã¯ï¼Œ`--model_dtype float32` ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ï¼


### 3. Training
1ãŠã‚ˆã³2ã§ä½œæˆã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨åˆæœŸåŒ–ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’ç”¨ã„ã¦ï¼Œãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿè¡Œã—ã¾ã™ï¼åŸºæœ¬çš„ã«ã¯ï¼ŒğŸ¤— Accelerate ã®ãƒ©ãƒ³ãƒãƒ£ãƒ¼ã‚’ä½¿ç”¨ã—ï¼Œæ‰€æœ›ã®ãƒ—ãƒ­ã‚»ã‚¹æ•°ã§å®Ÿè¡Œã—ã¦ãã ã•ã„ï¼š
```bash
uv run accelerate launch \
    --num_machines 1 \
    --num_processes 8 \
    --use_deepspeed \
    --deepspeed_config_file ds_configs/zero3-fp16-warmlr-act_ckpt.json \
    finetune.py \
        --output_dir "output/moshiko-finetuned" \
        --train_data_files "processed_data/spokenwoz_sample/train-*.parquet" \
        --model_dir "init_models/moshiko-both_streams-float32" \
        ...
```
ç¾åœ¨ DeepSpeed ä»¥å¤–ã§ã®å­¦ç¿’ã¯ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ãªã„ãŸã‚ï¼Œå¿…ãš `--use_deepspeed` ãŠã‚ˆã³ `--deepspeed_config_file` ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ï¼
ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚„å­¦ç¿’ç‡ï¼Œå„ãƒˆãƒ¼ã‚¯ãƒ³ã®æå¤±ã®é‡ã¿ç­‰ã¯ï¼Œ`finetune.py` ã®å¼•æ•°ã¨ã—ã¦æŒ‡å®šå¯èƒ½ã§ã™ï¼è©³ç´°ã¯ `uv run finetune.py --help` ã§ç¢ºèªã—ã¦ãã ã•ã„ï¼

> [!NOTE]
> å…·ä½“çš„ãªå®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰ã¯ï¼Œ`examples/finetune_accelerate.sh` ã‚’å‚ç…§ã—ã¦ãã ã•ã„ï¼

å­¦ç¿’ä¸­ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã¯ï¼ŒDeepSpeed ã®çŠ¶æ…‹ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜ã•ã‚Œã¾ã™ï¼å¾Œæ®µã®æ¨è«–ã§ä½¿ç”¨ã™ã‚‹ãŸã‚ã«ã¯ï¼Œä»¥ä¸‹ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ç”¨ã„ã¦ï¼Œãã®çŠ¶æ…‹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ safetensors å½¢å¼ã«å¤‰æ›ã—ã¦ãã ã•ã„ï¼š
```bash
uv run -m tools.zero_to_fp32 \
    output/moshiko-finetuned/step_10000 \
    output/moshiko-finetuned/step_10000_fp32 \
    --moshi_lm_kwargs_path init_models/moshiko-both_streams-float32/moshi_lm_kwargs.json
```
ä»¥ä¸Šã«ã‚ˆã‚Šï¼Œ`output/moshiko-finetuned/step_10000_fp32` ã«ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ `model.safetensors` ãŠã‚ˆã³ãã®è¨­å®šãƒ•ã‚¡ã‚¤ãƒ« `moshi_lm_kwargs.json` ãŒä¿å­˜ã•ã‚Œã¾ã™ï¼

#### Tips: Multi-Node Training
è¤‡æ•°ãƒãƒ¼ãƒ‰ã§ã®å­¦ç¿’ã‚’è¡Œã†å ´åˆã¯ï¼Œã™ã¹ã¦ã®ãƒãƒ¼ãƒ‰ã§ï¼Œ`--machine_rank`, `--main_process_ip`, `--main_process_port`, ãã—ã¦ `--deepspeed_multinode_launcher standard` ã®å¼•æ•°ã‚’è¿½åŠ ã—ãŸ accelerate launch ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ï¼ğŸ¤— Accelerate ã«ã‚ˆã‚‹ãƒãƒ«ãƒãƒãƒ¼ãƒ‰å­¦ç¿’ã«é–¢ã™ã‚‹ã•ã‚‰ãªã‚‹è©³ç´°ãªä½¿ã„æ–¹ã¯ï¼Œ[å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://huggingface.co/docs/accelerate/basic_tutorials/launch#multi-node-training)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ï¼

> [!NOTE]
> è¤‡æ•°ãƒãƒ¼ãƒ‰ OpenMPI ã® mpirun ã§åˆ¶å¾¡ã™ã‚‹å ´åˆã¯ï¼Œ`examples/finetune_mpi_accelerate.sh` ã‚’å‚ç…§ã—ã¦ãã ã•ã„ï¼ã‚‚ã— accelerate launch ã‚’ä½¿ç”¨ã›ãšï¼Œå…¨ã¦ã®ãƒ—ãƒ­ã‚»ã‚¹ã‚’ mpirun ã§èµ·å‹•ã™ã‚‹å ´åˆã¯ï¼Œ`examples/finetune_mpi.sh` ã‚’å‚è€ƒã«ã—ã¦ãã ã•ã„ï¼


## Inference
å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’ç”¨ã„ã¦ï¼ŒéŸ³å£°å¯¾è©±ã®ç”Ÿæˆã‚„ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å¯¾è©±ã‚’è¡Œã†ã“ã¨ãŒã§ãã¾ã™ï¼ä»¥ä¸‹ã§ã¯ï¼Œå­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ç”¨ã„ãŸä¾‹ã‚’ç¤ºã—ã¾ã™ï¼

### 1. Prompted Dialogue Continuation
[Data Preparation](#1-data-preparation) ã§ä½œæˆã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«å«ã¾ã‚Œã‚‹æ•°ç§’ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‹ã‚‰ï¼Œãã®ç¶šãã‚’ç”Ÿæˆã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ï¼ accelerate launch ã‚’ä½¿ç”¨ã—ã¦ï¼Œä»¥ä¸‹ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ï¼š
```bash
model_dir="output/moshiko-finetuned/step_10000_fp32"
uv run accelerate launch \
    --num_machines 1 \
    --num_processes 4 \
    generate.py \
        --output_dir "${model_dir}/continuation" \
        --model_dir "${model_dir}" \
        --eval_data_files "processed_data/spokenwoz_sample/test-*.parquet" \
        --prompt_length 125 \
        --generation_length 250 \
        --temperature 0.8 \
        ...
```
`--prompt_length` ãŠã‚ˆã³ `--generation_length` ã‚’ç”¨ã„ã¦ï¼Œãã‚Œãã‚Œãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®é•·ã•ã¨ç”Ÿæˆãƒ†ã‚­ã‚¹ãƒˆã®é•·ã•ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ï¼ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«å«ã¾ã‚Œã‚‹å„ã‚µãƒ³ãƒ—ãƒ«ã«ã¤ã„ã¦ï¼Œãã®å§‹ã¾ã‚Šã‹ã‚‰ `--prompt_lenght` ã§æŒ‡å®šã•ã‚ŒãŸé•·ã•ã¾ã§ã®éƒ¨åˆ†ãŒãƒ¢ãƒ‡ãƒ«ã«å…¥åŠ›ã•ã‚Œã¾ã™ï¼å˜ä½ã¯ Mimi ã®1ãƒ•ãƒ¬ãƒ¼ãƒ ï¼ˆ80msï¼‰ã§ã™ï¼ãã®ä»–ï¼Œãƒãƒƒãƒã‚µã‚¤ã‚ºã‚„æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç­‰ã®è¨­å®šã¯ï¼Œ`generate.py` ã®å¼•æ•°ã¨ã—ã¦æŒ‡å®šå¯èƒ½ã§ã™ï¼è©³ç´°ã¯ `uv run generate.py --help` ã§ç¢ºèªã—ã¦ãã ã•ã„ï¼

> [!NOTE]
> å…·ä½“çš„ãªå®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰ã¯ï¼Œ`examples/generate_accelerate.sh` ã‚’å‚ç…§ã—ã¦ãã ã•ã„ï¼

#### Tips: Decode generated audio tokens to wav
ç”Ÿæˆã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³ã¯ï¼Œgenerate.py ã®å¼•æ•°ã® `--output_dir` ã§æŒ‡å®šã—ãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä¸‹ã® `generated_tokens`å†…ã«ï¼Œå¯¾è©±æ¯ã® npy ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜ã•ã‚Œã¾ã™ï¼ä»¥ä¸‹ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ä½¿ç”¨ã—ã¦ï¼Œã“ã‚Œã‚‰ã® npy ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ wav ãƒ•ã‚¡ã‚¤ãƒ«ã«å¤‰æ›å¯èƒ½ã§ã™ï¼š
```bash
uv run -m tools.decode_tokens \
    --tokens_dir "${model_dir}/continuation/generated_tokens" \
    --output_dir "${model_dir}/continuation/generated_wavs"
```
ã“ã‚Œã«ã‚ˆã‚Šï¼Œå¯¾è©±æ¯ã®wavãƒ•ã‚¡ã‚¤ãƒ«ãŒä¿å­˜ã•ã‚Œã¾ã™ï¼å„wavãƒ•ã‚¡ã‚¤ãƒ«ã¯ï¼Œå·¦ãƒãƒ£ãƒãƒ«ã«ã‚·ã‚¹ãƒ†ãƒ ã®éŸ³å£°ï¼Œå³ãƒãƒ£ãƒãƒ«ã«ãƒ¦ãƒ¼ã‚¶ã®éŸ³å£°ã‚’ãã‚Œãã‚Œå«ã‚“ã§ã„ã¾ã™ï¼


### 2. Interactive Demo
[moshi ãƒ©ã‚¤ãƒ–ãƒ©ãƒª](https://github.com/kyutai-labs/moshi)ã® web appï¼ˆ`moshi.server`ï¼‰ã‚’ä½¿ç”¨ã—ã¦ï¼Œå­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã¨ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã«å¯¾è©±ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ï¼ä»¥ä¸‹ã®æ‰‹é †ã«å¾“ã£ã¦ãã ã•ã„ï¼

#### 2.1 Cleaning the finetuned model
`moshi.server` ã§ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚€ã«ã¯ï¼Œã¾ãšï¼Œ[Model Initialization](#2-model-initialization) ã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨ã«ç·¨é›†ã—ãŸãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ï¼Œã‚ªãƒªã‚¸ãƒŠãƒ«ã®Moshiãƒ¢ãƒ‡ãƒ«ã®å½¢å¼ã«æˆ»ã™å¿…è¦ãŒã‚ã‚Šã¾ã™ï¼å…·ä½“çš„ã«ã¯ï¼Œä»¥ä¸‹ã®å‡¦ç†ãŒå«ã¾ã‚Œã¾ã™ï¼š
- Depth Transformer ã«è¿½åŠ ã—ãŸï¼Œãƒ¦ãƒ¼ã‚¶ã®éŸ³å£°ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’å‡ºåŠ›ã™ã‚‹ãŸã‚ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®å‰Šé™¤
- DeepSpeed Zero 3 ã®ãŸã‚ã«ç·¨é›†ã—ãŸãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ä¿®æ­£

ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã—ã¦ï¼Œãƒ¢ãƒ‡ãƒ«ã‚’å…ƒã®å½¢å¼ã«æˆ»ã—ã¦ãã ã•ã„ï¼š
```bash
uv run -m tools.clean_moshi \
    --moshi_ft_dir output/moshiko-finetuned/step_10000_fp32 \
    --save_dir output/moshiko-finetuned/step_10000_cleaned \
    --model_dtype float32 \
    --remove_modules_for_user_stream
```
ã“ã‚Œã«ã‚ˆã‚Šï¼Œ`output/moshiko-finetuned/step_10000_cleaned` ã«å…ƒã®å½¢å¼ã®ãƒ¢ãƒ‡ãƒ«ï¼ˆ`model.safetensors`ï¼‰ãŠã‚ˆã³ãã®è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ`moshi_lm_kwargs.json`ï¼‰ãŒä¿å­˜ã•ã‚Œã¾ã™ï¼

#### 2.2 Running the server
ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã—ã¦ï¼Œã‚µãƒ¼ãƒã‚’èµ·å‹•ã—ã¦ãã ã•ã„ï¼š
```bash
uv run -m moshi.server \
    --moshi-weight output/moshiko-finetuned/step_10000_cleaned
```
ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã¯ `http://localhost:8998` ã§ã‚µãƒ¼ãƒãŒèµ·å‹•ã—ã¾ã™ï¼ãƒ–ãƒ©ã‚¦ã‚¶ã§ã“ã®URLã«ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ã“ã¨ã§ï¼Œå¯¾è©±ã‚’é–‹å§‹ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ï¼è©³ã—ã„ä½¿ã„æ–¹ã¯ï¼ŒKyutaiå…¬å¼ã®ãƒªãƒã‚¸ãƒˆãƒª[kyutai-labs/moshi](https://github.com/kyutai-labs/moshi)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ï¼


## License
ã“ã®ãƒªãƒã‚¸ãƒˆãƒªã¯ [Apache 2.0 License](LICENSE) ã®ä¸‹ã§æä¾›ã•ã‚Œã¦ã„ã¾ã™ï¼ãªãŠï¼Œ`data/spokenwoz_sample` ã«å«ã¾ã‚Œã‚‹ï¼ŒSpokenWOZ ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã¯ [CC BY-NC 4.0](https://creativecommons.org/licenses/by-nc/4.0/) ã®ä¸‹ã§æä¾›ã•ã‚Œã¦ã„ã¾ã™ï¼


## Citation
ã“ã®ãƒªãƒã‚¸ãƒˆãƒªã‚’ä½¿ç”¨ã—ãŸå ´åˆã¯ï¼Œä»¥ä¸‹ã®æ–‡çŒ®ã‚’å¼•ç”¨ã—ã¦ãã ã•ã„ï¼š
```bibtex
@article{ohashi2025towards,
    title={Towards a Japanese Full-duplex Spoken Dialogue System},
    author={Ohashi, Atsumoto and Iizuka, Shinya and Jiang, Jingjing and Higashinaka, Ryuichiro},
    journal={arXiv preprint arXiv:2506.02979},
    year={2025}
}
```
