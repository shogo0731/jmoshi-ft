{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "276dc24c-a03a-4f02-923b-4963f2307572",
   "metadata": {},
   "source": [
    "# FT用データ生成スクリプト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "683a8852-50c8-4ba2-a611-589ebd76bc9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !conda install -y -c conda-forge kalpy \\\n",
    "# kaldi \\\n",
    "# pynini\n",
    "\n",
    "# # パッケージインストール\n",
    "# !pip install -r requirements.sbv.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a816459b-765b-42cb-a7d1-10a1211a3fa3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "69a5268f-a2e3-48e3-ab2a-54e9e5e298e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # mfa\n",
    "# # 日本語辞書のダウンロード\n",
    "# !mfa model download dictionary japanese_mfa\n",
    "\n",
    "# # 日本語音響モデルのダウンロード\n",
    "# !mfa model download acoustic japanese_mfa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6e391c-3079-4a7f-8eeb-e122e07f6707",
   "metadata": {},
   "source": [
    "## テキスト対話データ生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1666ef7b-1012-47c3-9229-6082e2d04f8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from typing import Literal\n",
    "import ast\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_community.document_loaders import PDFMinerLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "\n",
    "# .envファイル読み込み\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "473c6dfd-9d4d-4bac-904a-df31dcd1a50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#config\n",
    "from os.path import join, expanduser\n",
    "\n",
    "OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]\n",
    "BASE_URL = \"https://api.openai.iniad.org/api/v1\"\n",
    "MODEL='gemini-2.5-flash'\n",
    "TEMPERATURE = 1.0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"\n",
    "\n",
    "# 生成する音声のサンプリングレート\n",
    "setting_sr = 16000\n",
    "\n",
    "#対話音声データの個数を指定\n",
    "gen_dial_num = 100\n",
    "\n",
    "# すでに作成した対話データを削除するかどうか\n",
    "IS_REMOVE_EXIST_FILE = True\n",
    "\n",
    "# ftに使うjsonとaudioの出力フォルダパス\n",
    "home_dir = expanduser(\"~\")\n",
    "json_dir_path = join(home_dir, \"Github/jmoshi-ft/gen_dialogue/data/sbv/transcription\")\n",
    "audio_dir_path = join(home_dir, \"Github/jmoshi-ft/gen_dialogue/data/sbv/audio\")\n",
    "\n",
    "# mfa関連のパス\n",
    "model_dir = join(home_dir, \"Documents/MFA/pretrained_models/acoustic/japanese_mfa.zip\")\n",
    "mfa_input_dir = join(home_dir, \"Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_input\")\n",
    "mfa_output_dir = join(home_dir, \"Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bf8bffea-83d1-44b8-9e3d-0bd71321ad36",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_paths = [\n",
    "    json_dir_path,\n",
    "    audio_dir_path,\n",
    "    mfa_input_dir,\n",
    "    mfa_output_dir,\n",
    "]\n",
    "\n",
    "for p in base_paths:\n",
    "    if not os.path.isdir(p):\n",
    "        os.makedirs(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bbc14b9b-7981-4975-a07b-16114cf7b3af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model定義\n",
    "model = ChatGoogleGenerativeAI(\n",
    "                 model=MODEL,\n",
    "                 temperature=TEMPERATURE)\n",
    "\n",
    "# 埋め込みモデル定義\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    openai_api_base=BASE_URL,\n",
    "    model=\"text-embedding-3-large\"\n",
    ")\n",
    "\n",
    "# データベース定義\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"collection\",\n",
    "    embedding_function=embeddings,\n",
    "    # persist_directory = \"/path/to/db_file\" # if necessary\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "97a271ae-62d4-44d1-b10d-8a8e38796ad8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                     | 0/3 [00:00<?, ?it/s]Cannot set gray non-stroke color because /'P0' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P1' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P0' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P1' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P0' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P1' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P0' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P1' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P0' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P1' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P0' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P1' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P0' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P1' is an invalid float value\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 3/3 [00:04<00:00,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "loader = DirectoryLoader(\n",
    "    \"../../mental_docs/\",\n",
    "    glob=\"*.pdf\",\n",
    "    show_progress=True,\n",
    "    loader_cls=PDFMinerLoader,\n",
    ")\n",
    "docs = loader.load()\n",
    "print(f\"Loaded {len(docs)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "669019e5-a634-40d8-b0f8-31abe0fb4877",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Debug\n",
    "# for doc in docs:\n",
    "#     print(\"-------------------------------------------------\")\n",
    "#     print(doc.metadata)\n",
    "#     print(len(doc.page_content))\n",
    "#     print(doc.page_content[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9ff84496-9aa7-45d9-89d3-3b8cde22fd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#読み込んだ文章データをオーバーラップ200文字で1000文字づつ分割\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    add_start_index=True, # 分割前の文章のインデックスを追跡\n",
    ")\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# データベースにデータを追加\n",
    "document_ids = vector_store.add_documents(documents=splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "757916d2-87c6-4033-8a23-02f0eaa6b074",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.middleware import dynamic_prompt, ModelRequest\n",
    "\n",
    "@dynamic_prompt\n",
    "def prompt_with_context(request: ModelRequest) -> str:\n",
    "    \"\"\"Inject context into state messages.\"\"\"\n",
    "    last_query = request.state[\"messages\"][-1].text\n",
    "    retrieved_docs = vector_store.similarity_search(last_query, k=2)\n",
    "\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "\n",
    "    system_message = (\n",
    "        \"You are a helpful assistant. Use the following context in your response:\"\n",
    "        f\"\\n\\n{docs_content}\"\n",
    "    )\n",
    "\n",
    "    return system_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d58e3775-2aad-4363-b893-f7d74f62d3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class Dialogue(BaseModel):\n",
    "    \"\"\"対話データを構成する対話クラス\"\"\"\n",
    "    speaker: Literal[\"A\", \"B\"] = Field(..., description=\"話者。Aはカウンセラー、Bはクライエントを表す。\")\n",
    "    text: str = Field(..., description=\"話者が話した内容。\")\n",
    "\n",
    "class Dialogues(BaseModel):\n",
    "    \"\"\"カウンセリングを目的としたカウンセリング対話データ\"\"\"\n",
    "    dialogues: list[Dialogue] = Field(..., description=\"対話データを構成する対話クラスのリスト。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c850f79b-c9ac-4722-b165-481de934e13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain.agents.structured_output import ToolStrategy\n",
    "\n",
    "agent = create_agent(\n",
    "    model, \n",
    "    tools=[],\n",
    "    middleware=[prompt_with_context],\n",
    "    response_format=ToolStrategy(\n",
    "        Dialogues,\n",
    "        handle_errors=\"フォーマットに合うように、もう一度対話データを生成してください。\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "261f3908-86f7-4ddb-9565-c7b56e25632b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#promptを作成\n",
    "import random\n",
    "\n",
    "\n",
    "sessions = [\n",
    "    \"【段階：初期】信頼関係を築きつつ、悩みの背景を深掘りするシーン\",\n",
    "    \"【段階：中期】クライエントの「すべき思考」に焦点を当て、認知の歪みを扱うシーン\",\n",
    "    \"【段階：終結期】これまでのセッションを振り返り、終結に向けて準備するシーン\",\n",
    "]\n",
    "\n",
    "def gen_prompt_txt():\n",
    "    choiced = random.randint(0, 2)\n",
    "    choiced_session = sessions[choiced]\n",
    "    prompt_txt = f\"\"\"メンタルヘルスケアカウンセリングのセッションをシミュレーションしてください。\n",
    "シミュレーションしたい「段階」と「テーマ」:\n",
    "{choiced_session}\n",
    "\n",
    "役割定義:\n",
    "A (カウンセラー): メンタルヘルスケアの専門知識を持つ経験豊富なカウンセラー。傾聴と共感の姿勢を基本とし、クライエントの言葉を促すように、優しく、自然な話し言葉（「〜ですね」「〜でしたか」など）を使います。\n",
    "B (クライエント): 仕事上の悩みだけでなく、日常生活全般に対して漠然とした不安や焦りを感じている人物。\n",
    "\n",
    "対話の要件:\n",
    "スタイル: 実際の会話の文字起こしのように、堅苦しくない自然な「話し言葉」を使用してください。\n",
    "相槌 (あいづち): カウンセラー（A）は、クライエント（B）の話を促し、共感を示すため、「ええ」「はい」「そうなんですね」「なるほど」といった細かな相槌を頻繁に、適切なタイミングで挿入してください。\n",
    "構成: 会話が途中で途切れるのではなく、初回のヒアリングとして「一区切り」がつき、自然に終了する流れにしてください（例：次回の約束、今回のまとめなど）。\n",
    "分量: 会話の往復は合計12〜20ターン程度、全体の文字数が合計500〜800文字程度になるように構成してください。\n",
    "\"\"\"\n",
    "    return prompt_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3d76aa80-c527-41a1-963a-5e60dc2307e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# テキスト対話生成関数\n",
    "def gen_txt_dialogue():\n",
    "    prompt = gen_prompt_txt()\n",
    "    resp = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": prompt}]})\n",
    "    dialogues_list = resp[\"structured_response\"].dialogues\n",
    "    return dialogues_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5813f865-a27c-473c-8acc-e8739dd606ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#DEBUG\n",
    "# txt_dialogue = gen_txt_dialogue()\n",
    "# print(txt_dialogue)\n",
    "# lst_dialogue = txt_to_lst(txt_dialogue)\n",
    "# print(lst_dialogue)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cd4da5-0ba6-4958-929d-c547e7c1f72c",
   "metadata": {},
   "source": [
    "## テキスト対話データを音声対話データに変換 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2caf61a3-b36a-48a1-8604-b8d720c5265d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jvnv-F2-jp/jvnv-F2_e166_s20000.safetensors\n",
      "jvnv-F2-jp/config.json\n",
      "jvnv-F2-jp/style_vectors.npy\n",
      "jvnv-M2-jp/jvnv-M2-jp_e159_s17000.safetensors\n",
      "jvnv-M2-jp/config.json\n",
      "jvnv-M2-jp/style_vectors.npy\n"
     ]
    }
   ],
   "source": [
    "from style_bert_vits2.nlp import bert_models\n",
    "from style_bert_vits2.constants import Languages\n",
    "from pathlib import Path\n",
    "from huggingface_hub import hf_hub_download\n",
    "from style_bert_vits2.tts_model import TTSModel\n",
    "\n",
    "bert_models.load_model(Languages.JP, \"ku-nlp/deberta-v2-large-japanese-char-wwm\")\n",
    "bert_models.load_tokenizer(Languages.JP, \"ku-nlp/deberta-v2-large-japanese-char-wwm\")\n",
    "assets_root = Path(\"model_assets\")\n",
    "\n",
    "# # 子春音あみ\n",
    "# model_file = \"koharune-ami/koharune-ami.safetensors\"\n",
    "# config_file = \"koharune-ami/config.json\"\n",
    "# style_file = \"koharune-ami/style_vectors.npy\"\n",
    "# hf_repo = \"litagin/sbv2_koharune_ami\"\n",
    "\n",
    "# # あみたろ\n",
    "# model_file = \"amitaro/amitaro.safetensors\"\n",
    "# config_file = \"amitaro/config.json\"\n",
    "# style_file = \"amitaro/style_vectors.npy\"\n",
    "# hf_repo = \"litagin/sbv2_amitaro\"\n",
    "\n",
    "\n",
    "# デフォルトの女性2\n",
    "model_file = \"jvnv-F2-jp/jvnv-F2_e166_s20000.safetensors\"\n",
    "config_file = \"jvnv-F2-jp/config.json\"\n",
    "style_file = \"jvnv-F2-jp/style_vectors.npy\"\n",
    "hf_repo = \"litagin/style_bert_vits2_jvnv\"\n",
    "\n",
    "for file in [model_file, config_file, style_file]:\n",
    "    print(file)\n",
    "    hf_hub_download(hf_repo, file, local_dir=\"model_assets\")\n",
    "\n",
    "A_model = TTSModel(\n",
    "    model_path=assets_root / model_file,\n",
    "    config_path=assets_root / config_file,\n",
    "    style_vec_path=assets_root / style_file,\n",
    "    device=\"cuda\",\n",
    ")\n",
    "\n",
    "# デフォルトの男性2\n",
    "model_file = \"jvnv-M2-jp/jvnv-M2-jp_e159_s17000.safetensors\"\n",
    "config_file = \"jvnv-M2-jp/config.json\"\n",
    "style_file = \"jvnv-M2-jp/style_vectors.npy\"\n",
    "\n",
    "for file in [model_file, config_file, style_file]:\n",
    "    print(file)\n",
    "    hf_hub_download(hf_repo, file, local_dir=\"model_assets\")\n",
    "\n",
    "B_model = TTSModel(\n",
    "    model_path=assets_root / model_file,\n",
    "    config_path=assets_root / config_file,\n",
    "    style_vec_path=assets_root / style_file,\n",
    "    device=\"cuda\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "568879f5-82d9-488f-a281-d3ad816f248a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "def sbv_tts(text: str, speaker: Literal[\"A\", \"B\"], assist_text=None):\n",
    "    if speaker == \"A\":\n",
    "        sr, audio = A_model.infer(\n",
    "            text = text,\n",
    "            style='Happy',\n",
    "            style_weight=1,\n",
    "            split_interval = 0.3,\n",
    "            use_assist_text = True if assist_text is not None else None,\n",
    "            assist_text = assist_text\n",
    "        )\n",
    "    else:\n",
    "        sr, audio = B_model.infer(\n",
    "            text = text,\n",
    "            style='Sad',\n",
    "            style_weight=1,\n",
    "            split_interval = 0.3,\n",
    "            use_assist_text = True if assist_text is not None else None,\n",
    "            assist_text = assist_text\n",
    "        )\n",
    "    \n",
    "    return sr, audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d9df1471-cccf-40fb-bb30-23ba6fdeba25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "def gen_audio_dialogue(text_dialogue_list):\n",
    "    # 音声ファイルを順番に生成（ファイルは不要なのでwave配列で持つ）\n",
    "    wav_data = []\n",
    "    for dial in text_dialogue_list:\n",
    "        speaker = dial.speaker\n",
    "        sr, wav = sbv_tts(dial.text, speaker)\n",
    "        print(wav.shape)\n",
    "\n",
    "        # サンプリングレートを変換\n",
    "        if sr != setting_sr:\n",
    "            # 16ビット整数のデータを、-1.0から1.0の範囲に収まる浮動小数点数に正規化\n",
    "            wav = wav.astype(np.float32) / 32768.0\n",
    "            wav = librosa.resample(wav, orig_sr=sr, target_sr=setting_sr)\n",
    "\n",
    "        # 0.3秒間の無音時間を追加\n",
    "        duration_sec = 0.3\n",
    "        num_silent_samples = int(setting_sr*duration_sec)\n",
    "        silence = np.zeros(num_silent_samples, dtype=wav.dtype)\n",
    "        wav_with_silence = np.concatenate((wav, silence))\n",
    "        wav_data.append(wav_with_silence)\n",
    "    \n",
    "    # 最終的な音声長を決定\n",
    "    max_len = sum([len(w) for w in wav_data])\n",
    "    \n",
    "    # ステレオ音声用（2チャンネル×最大長）の空配列をゼロ初期化で作成\n",
    "    stereo = np.zeros((2, max_len), dtype=np.float32)\n",
    "    \n",
    "    pos = 0\n",
    "    for i, wav in enumerate(wav_data):\n",
    "        ch = i%2  # 0:左(A), 1:右(B)\n",
    "        stereo[ch, pos:pos+len(wav)] += wav\n",
    "        pos += len(wav)\n",
    "    \n",
    "    # 転置(-1,2)する\n",
    "    stereo = stereo.T\n",
    "    return stereo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa95302-64ba-4175-ab42-4939b02ee1be",
   "metadata": {},
   "source": [
    "## mfa(montreal force alignment)による音声アラインメント"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4cb4f6bb-5c7e-4a70-8a8a-34276924a55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab\n",
    "import re\n",
    "\n",
    "# 句読点のパターン\n",
    "PUNCT_RE = re.compile(r'^[。、,.!?！？…]+$')\n",
    "\n",
    "def tokenize_text(text, is_punct_isolated=False):\n",
    "    tokens = []\n",
    "    punct_dict = {}\n",
    "    checked_punct_pos = 0\n",
    "    try:\n",
    "        # MeCabのタガーを初期化\n",
    "        tagger = MeCab.Tagger()\n",
    "\n",
    "        # MeCabは内部でShift-JISやEUC-JPを期待することがあるため、\n",
    "        # UnicodeDecodeErrorを避けるために明示的にUTF-8でエンコード・デコードする\n",
    "        # parseToNodeは、より詳細な情報をノードオブジェクトとして取得できるメソッド\n",
    "        node = tagger.parseToNode(text)\n",
    "        while node:\n",
    "            if not node.surface:\n",
    "                pass\n",
    "                \n",
    "            elif not is_punct_isolated and PUNCT_RE.match(node.surface) and tokens:\n",
    "                punct_dict[checked_punct_pos] = node.surface\n",
    "                \n",
    "                checked_punct_pos += len(node.surface)\n",
    "                # 句読点なら直前トークンに連結\n",
    "                tokens[-1] += node.surface\n",
    "            else:\n",
    "                checked_punct_pos += len(node.surface)\n",
    "                \n",
    "                # 通常トークンはそのまま追加\n",
    "                tokens.append(node.surface)\n",
    "            node = node.next\n",
    "    except RuntimeError as e:\n",
    "        print(f\"MeCabの実行中にエラーが発生しました: {e}\", file=sys.stderr)\n",
    "        \n",
    "    return tokens, punct_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7d7c04cc-e177-4fa4-a03d-80d7f29a80b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_txt_file_using_mecab(input_txt, path):\n",
    "    tokens, punct_dict = tokenize_text(input_txt)\n",
    "    output = \"\"\n",
    "    for token in tokens:\n",
    "        output += token + \"\\n\"\n",
    "        \n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(output)\n",
    "    return tokens, punct_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "09aaef40-fca0-46e3-96b2-0c04a1ff984a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join, expanduser\n",
    "import subprocess\n",
    "import json\n",
    "\n",
    "def alignment_channel(channel, txt, target_dir_name):\n",
    "    input_dir_path = join(mfa_input_dir, target_dir_name)\n",
    "    output_dir_path = join(mfa_output_dir, target_dir_name)\n",
    "    os.makedirs(input_dir_path, exist_ok=True)\n",
    "    os.makedirs(output_dir_path, exist_ok=True)\n",
    "    \n",
    "    for_align_audio_path = join(input_dir_path, f\"{target_dir_name}.wav\")\n",
    "    for_align_txt_path = join(input_dir_path, f\"{target_dir_name}.txt\")\n",
    "\n",
    "    sf.write(for_align_audio_path, channel, setting_sr)\n",
    "    _, punct_dict = generate_txt_file_using_mecab(txt, for_align_txt_path)\n",
    "    subprocess.run([\n",
    "        \"mfa\",\n",
    "        \"align\",\n",
    "        input_dir_path,\n",
    "        \"japanese_mfa\",\n",
    "        model_dir,\n",
    "        output_dir_path,\n",
    "        \"--\",\n",
    "        \"--overwrite\",\n",
    "        \"--clean\",\n",
    "        \"----final_clean\",\n",
    "        \"--output_format\", \"json\",\n",
    "        \"--beam\", \"1000\",\n",
    "        \"--retry_beam\", \"4000\",\n",
    "        \"--punctuation\", \"…\",\n",
    "    ])\n",
    "    return punct_dict\n",
    "\n",
    "def json_formatter_for_ft(align_json_A, align_json_B):\n",
    "    json = []\n",
    "\n",
    "    segments_A = align_json_A[\"tiers\"][\"words\"][\"entries\"]\n",
    "    segments_B = align_json_B[\"tiers\"][\"words\"][\"entries\"]\n",
    "    for segment in segments_A:\n",
    "        json.append({\n",
    "            \"speaker\": \"A\",\n",
    "            \"word\": segment[2],\n",
    "            \"start\": segment[0],\n",
    "            \"end\": segment[1],\n",
    "        })\n",
    "    for segment in segments_B:\n",
    "        json.append({\n",
    "            \"speaker\": \"B\",\n",
    "            \"word\": segment[2],\n",
    "            \"start\": segment[0],\n",
    "            \"end\": segment[1],\n",
    "        })\n",
    "    sorted_json = sorted(json, key=lambda seg: seg[\"start\"])\n",
    "    return sorted_json\n",
    "\n",
    "def lst_to_line_str(lst):\n",
    "    result = \"\"\n",
    "    for s in lst:\n",
    "        result += s\n",
    "    return result\n",
    "    \n",
    "def alignment_audio_dialogue(text_dialogue_list, audio_path, idx):\n",
    "    # ステレオ分離: speaker A=左(0), B=右(1)と仮定\n",
    "    audio, sr = sf.read(audio_path)    # (samples, channels)\n",
    "    channel_A = audio[:,0]\n",
    "    channel_B = audio[:,1]\n",
    "    txt_lst_A = []\n",
    "    txt_lst_B = []\n",
    "    for txt_dial in text_dialogue_list:\n",
    "        if txt_dial.speaker == \"A\":\n",
    "            txt_lst_A.append(txt_dial.text)\n",
    "        else:\n",
    "            txt_lst_B.append(txt_dial.text)\n",
    "    A_full_txt = lst_to_line_str(txt_lst_A)\n",
    "    B_full_txt = lst_to_line_str(txt_lst_B)\n",
    "    \n",
    "    target_dir_name_A = f\"A_{idx}\"\n",
    "    target_dir_name_B = f\"B_{idx}\"\n",
    "    punct_dict_A = alignment_channel(channel_A, A_full_txt, target_dir_name_A)\n",
    "    punct_dict_B = alignment_channel(channel_B, B_full_txt, target_dir_name_B)\n",
    "    json_path_A = join(mfa_output_dir, target_dir_name_A, f\"{target_dir_name_A}.json\")\n",
    "    json_path_B = join(mfa_output_dir, target_dir_name_B, f\"{target_dir_name_B}.json\")\n",
    "    with open(json_path_A, \"r\") as f:\n",
    "        json_A = json.load(f)\n",
    "    with open(json_path_B, \"r\") as f:\n",
    "        json_B = json.load(f)\n",
    "    \n",
    "    ft_json = json_formatter_for_ft(json_A, json_B)\n",
    "\n",
    "    return ft_json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b917fe-46ac-43a6-a5e7-c9903095616a",
   "metadata": {},
   "source": [
    "## フォルダ初期化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "17a8d44f-adba-483e-9d5d-65dae116d409",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_file_name():\n",
    "    wav_file_pattern = r\"^(\\d+)\\.wav$\"\n",
    "    num = -1\n",
    "    for file in os.listdir(audio_dir_path):\n",
    "        if not os.path.exists(os.path.join(audio_dir_path, file)):\n",
    "            continue\n",
    "        if not re.match(wav_file_pattern, file):\n",
    "            continue\n",
    "    \n",
    "        match_obj = re.match(wav_file_pattern, file)\n",
    "        get_number = int(match_obj.groups()[0])\n",
    "    \n",
    "        if num < get_number:\n",
    "            num = get_number\n",
    "    return num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4895d7b4-d9e6-43c3-b4b6-d21b0d4eac50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import shutil\n",
    "\n",
    "def delete_files(dir_path):\n",
    "    shutil.rmtree(dir_path)\n",
    "    os.makedirs(dir_path)\n",
    "\n",
    "if IS_REMOVE_EXIST_FILE:\n",
    "    file_name_num = -1\n",
    "    for dir_path in base_paths:\n",
    "        delete_files(dir_path)\n",
    "else:\n",
    "    file_name_num = get_file_name()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e535afd9-ad91-43af-a583-32cf4525864e",
   "metadata": {},
   "source": [
    "## メイン処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "92baf05a-d14e-4a4b-8f3d-98bdc5043914",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-25 19:44:40\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "今日は、お越しくださってありがとうございます。何か、今お話ししたいこと、ありますか？\n",
      "\u001b[32m10-25 19:44:40\u001b[0m |\u001b[1m  INFO  \u001b[0m| infer.py:24 | Using JP-Extra model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/s1f102201582/anaconda3/envs/sbv-tts/lib/python3.12/site-packages/torch/nn/utils/weight_norm.py:144: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-25 19:44:41\u001b[0m |\u001b[1m  INFO  \u001b[0m| safetensors.py:50 | Loaded 'model_assets/jvnv-F2-jp/jvnv-F2_e166_s20000.safetensors' (iteration 166)\n",
      "\u001b[32m10-25 19:44:41\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(275968,)\n",
      "\u001b[32m10-25 19:44:41\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、ありがとうございます。最近、仕事のことや、日常生活全体に漠然とした不安や焦りを感じることが多くて…。\n",
      "\u001b[32m10-25 19:44:41\u001b[0m |\u001b[1m  INFO  \u001b[0m| infer.py:24 | Using JP-Extra model\n",
      "\u001b[32m10-25 19:44:42\u001b[0m |\u001b[1m  INFO  \u001b[0m| safetensors.py:50 | Loaded 'model_assets/jvnv-M2-jp/jvnv-M2-jp_e159_s17000.safetensors' (iteration 159)\n",
      "\u001b[32m10-25 19:44:42\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(401408,)\n",
      "\u001b[32m10-25 19:44:42\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうなんですね。漠然とした不安や焦り、ですか。ええ。どんな時に強く感じられますか？\n",
      "\u001b[32m10-25 19:44:42\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(324096,)\n",
      "\u001b[32m10-25 19:44:42\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "うーん、仕事中はもちろんですが、家に帰っても落ち着かなくて。夜も寝付けず、朝起きるのもつらい、みたいな感じです。\n",
      "\u001b[32m10-25 19:44:43\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(391168,)\n",
      "\u001b[32m10-25 19:44:43\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど。お仕事中だけでなく、お家でも不安が続いているんですね。睡眠にも影響が。それはお辛いですね。いつ頃から続いているんですか？\n",
      "\u001b[32m10-25 19:44:43\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(514048,)\n",
      "\u001b[32m10-25 19:44:43\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "半年くらい前から、でしょうか。最初は疲れているだけかと。だんだんひどくなってきて…。きっかけというよりは、気づいたらこうなっていました。\n",
      "\u001b[32m10-25 19:44:43\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(447488,)\n",
      "\u001b[32m10-25 19:44:43\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、そうでしたか。だんだんと、こういった状態になっていったのですね。ご自身で、何か試されたことはありましたか？\n",
      "\u001b[32m10-25 19:44:43\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(375296,)\n",
      "\u001b[32m10-25 19:44:43\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、休日に気分転換しようとしたり、趣味に打ち込んだりもしたんですけど、結局、また月曜日が来ると思うと、憂鬱になってしまって…。\n",
      "\u001b[32m10-25 19:44:43\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(454144,)\n",
      "\u001b[32m10-25 19:44:43\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうなんですね。色々と工夫されていたのですね。ええ。それでも気分が晴れないと。今日、こうしてお話しくださって、少しでもお気持ちが軽くなったなら嬉しいです。\n",
      "\u001b[32m10-25 19:44:43\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(557568,)\n",
      "\u001b[32m10-25 19:44:43\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、少し話せて、落ち着きました。ありがとうございます。\n",
      "\u001b[32m10-25 19:44:43\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(208384,)\n",
      "\u001b[32m10-25 19:44:43\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "よかったです。ええ。今日はBさんの不安や、それが日常生活にどう影響しているかお聞かせいただきました。次回は、もう少し具体的に、どんな時に不安が強くなるのか、また、どうすれば楽になるか、一緒に見ていけたらと思いますが、いかがでしょうか？\n",
      "\u001b[32m10-25 19:44:43\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(857088,)\n",
      "\u001b[32m10-25 19:44:43\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、お願いします。\n",
      "\u001b[32m10-25 19:44:44\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(78848,)\n",
      "\u001b[32m10-25 19:44:44\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、ありがとうございます。では、次回のカウンセリングは来週の同じ時間でよろしいでしょうか？\n",
      "\u001b[32m10-25 19:44:44\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(288768,)\n",
      "\u001b[32m10-25 19:44:44\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、大丈夫です。\n",
      "\u001b[32m10-25 19:44:44\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(77312,)\n",
      "\u001b[32m10-25 19:44:44\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、承知いたしました。今日はこれで終わりにさせていただきますね。また来週、お待ちしております。お気をつけてお帰りください。\n",
      "\u001b[32m10-25 19:44:44\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(420864,)\n",
      "\u001b[32m10-25 19:44:44\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ありがとうございました。\n",
      "\u001b[32m10-25 19:44:44\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(61440,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:31\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:04\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_0...\u001b[0m                                                                \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_0\u001b[0m!                                                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m141.009\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:27\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_0...\u001b[0m                                                                \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_0\u001b[0m!                                                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m139.006\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-25 19:50:22\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "Bさん、今日はこれまでのセッションを振り返り、終結に向けて準備していきましょうか。\n",
      "\u001b[32m10-25 19:50:22\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(258560,)\n",
      "\u001b[32m10-25 19:50:22\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、もうそんな時期なんですね。少し寂しい気持ちもあります。\n",
      "\u001b[32m10-25 19:50:22\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(223232,)\n",
      "\u001b[32m10-25 19:50:22\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、そうなんですね。最初、漠然とした不安でしたね。この数ヶ月、Bさんにとってどんな時間でしたか？\n",
      "\u001b[32m10-25 19:50:22\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(369664,)\n",
      "\u001b[32m10-25 19:50:22\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。本当に何から手をつけていいか分からず、悩みが漠然としていました。\n",
      "\u001b[32m10-25 19:50:23\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(244224,)\n",
      "\u001b[32m10-25 19:50:23\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど。そこから、思考や感情のパターンを整理していきましたね。特に印象的なことは？\n",
      "\u001b[32m10-25 19:50:23\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(297984,)\n",
      "\u001b[32m10-25 19:50:23\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ。「集団面接で話せない」問題で、自分の考えや感情、行動のパターンに気づけたのが大きかったです。\n",
      "\u001b[32m10-25 19:50:23\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(351232,)\n",
      "\u001b[32m10-25 19:50:23\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、そうでしたね。客観的に見られた時、どんなお気持ちでしたか？\n",
      "\u001b[32m10-25 19:50:23\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(235008,)\n",
      "\u001b[32m10-25 19:50:23\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ああ、これだったのか、と。気持ちが楽になり、ホームワークで試して、自信も少しつきました。\n",
      "\u001b[32m10-25 19:50:23\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(318464,)\n",
      "\u001b[32m10-25 19:50:23\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "素晴らしいですね！ご自身で対策を見つけ、行動されたこと、大きな進歩です。\n",
      "\u001b[32m10-25 19:50:24\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(287232,)\n",
      "\u001b[32m10-25 19:50:24\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ありがとうございます。まだ不安もありますが、前よりずっと、どうすれば良いか考えられます。\n",
      "\u001b[32m10-25 19:50:24\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(286208,)\n",
      "\u001b[32m10-25 19:50:24\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、そうなんですね。ご自身で問題を乗り越える力が備わってきた証拠ですよ。\n",
      "\u001b[32m10-25 19:50:24\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(271872,)\n",
      "\u001b[32m10-25 19:50:24\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。これからも、学んだことを忘れずに過ごしたいです。\n",
      "\u001b[32m10-25 19:50:24\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(175616,)\n",
      "\u001b[32m10-25 19:50:24\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、もちろんです。また何かあればいつでもご相談ください。次回の面談で、まとめと今後についてお話ししましょう。\n",
      "\u001b[32m10-25 19:50:24\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(390656,)\n",
      "\u001b[32m10-25 19:50:24\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、よろしくお願いします。\n",
      "\u001b[32m10-25 19:50:25\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(97792,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:16\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:04\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_1...\u001b[0m                                                                \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_1\u001b[0m!                                                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m123.378\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:15\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:04\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_1...\u001b[0m                                                                \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_1\u001b[0m!                                                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m125.590\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-25 19:55:11\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "今日は、これまでのセッションを少し振り返りながら、今後のお話をしていきましょうか。〇〇様、いかがですか？これまでの時間で、何かご自身の中で変化したな、と感じることはありますか？\n",
      "\u001b[32m10-25 19:55:11\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(589312,)\n",
      "\u001b[32m10-25 19:55:11\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい...そうですね。最初は漠然と「性格を直したい」って思ってたんですけど、今はもう少し具体的に、どういう時に不安になるのか、どう考えちゃうのか、少しずつわかるようになってきた気がします。\n",
      "\u001b[32m10-25 19:55:11\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(649216,)\n",
      "\u001b[32m10-25 19:55:11\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうなんですね。ええ、なるほど。漠然とした不安が、より具体的に見えてきた、ということですね。素晴らしい気づきですね。\n",
      "\u001b[32m10-25 19:55:12\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(431104,)\n",
      "\u001b[32m10-25 19:55:12\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。特に、以前お話しした、集団面接でのディスカッションの時、「私は口べたでうまく伝えられない」って考えて、失敗しそうで不安になって、結局何も言えなかったことがあったじゃないですか。\n",
      "\u001b[32m10-25 19:55:12\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(559104,)\n",
      "\u001b[32m10-25 19:55:12\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、ありましたね。あの時のことですね。\n",
      "\u001b[32m10-25 19:55:12\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(158720,)\n",
      "\u001b[32m10-25 19:55:12\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "あれが、自分の「認知」と「感情」と「行動」が悪循環になってたんだって、図に書き出して整理できたのが、すごく大きかったです。\n",
      "\u001b[32m10-25 19:55:12\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(386560,)\n",
      "\u001b[32m10-25 19:55:12\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうでしたね。ご自身の思考のパターンを客観的に見つめ直すことができた、ということですね。よく頑張られましたね。はい。\n",
      "\u001b[32m10-25 19:55:12\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(399872,)\n",
      "\u001b[32m10-25 19:55:12\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。それで、ホームワークで、まずは簡単な意見でもいいから、積極的に話す練習をしてみようって...。少しずつですけど、実践してみることができています。\n",
      "\u001b[32m10-25 19:55:12\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(486912,)\n",
      "\u001b[32m10-25 19:55:13\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうなんですね！それは本当に大きな一歩ですね。ご自身で悪循環を断ち切るための具体的な行動に繋げられたということですね。素晴らしいです。\n",
      "\u001b[32m10-25 19:55:13\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(497664,)\n",
      "\u001b[32m10-25 19:55:13\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "でも、やっぱりまだ、完全に不安がなくなったわけじゃないですし、今後一人でやっていけるかなって、少し不安もあります。\n",
      "\u001b[32m10-25 19:55:13\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(358400,)\n",
      "\u001b[32m10-25 19:55:13\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、そうですね。不安に感じるお気持ちもよくわかります。でも、〇〇様はこれまでのセッションで、ご自身で問題を客観的に捉える力や、ご自身の認知や感情、行動を整理する力を、もう十分に身につけられましたよね。はい。\n",
      "\u001b[32m10-25 19:55:13\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(796672,)\n",
      "\u001b[32m10-25 19:55:13\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "...そう、ですね。\n",
      "\u001b[32m10-25 19:55:13\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(61440,)\n",
      "\u001b[32m10-25 19:55:13\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ。これからもきっと、ご自身で乗り越えていける力が備わっていると思いますよ。今日はここまでにしましょうか。次回は、今回お話しいただいたことを踏まえて、今後どうしていきたいか、もう少し具体的に終結に向けてのお話ができたらと思います。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: JPCommonLabel_insert_pause() in jpcommon_label.c: First mora should not be short pause.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-25 19:55:14\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(824320,)\n",
      "\u001b[32m10-25 19:55:14\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、ありがとうございます。よろしくお願いします。\n",
      "\u001b[32m10-25 19:55:14\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(183296,)\n",
      "\u001b[32m10-25 19:55:14\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、こちらこそ、ありがとうございました。\n",
      "\u001b[32m10-25 19:55:14\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(128512,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:36\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:06\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_2...\u001b[0m                                                                \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_2\u001b[0m!                                                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m147.451\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:33\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_2...\u001b[0m                                                                \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_2\u001b[0m!                                                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m141.964\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-25 20:00:35\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こんにちは、Bさん。今日はどんなことをお話ししましょうか。\n",
      "\u001b[32m10-25 20:00:35\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(169984,)\n",
      "\u001b[32m10-25 20:00:35\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こんにちは。なんだか、最近ずっと焦りを感じていて…仕事もプライベートも、もっと「こうあるべき」っていう気持ちが強いんです。\n",
      "\u001b[32m10-25 20:00:35\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(416256,)\n",
      "\u001b[32m10-25 20:00:35\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうですか。「こうあるべき」という気持ちが、はい。具体的には、どんな時にそう感じることが多いですか？\n",
      "\u001b[32m10-25 20:00:36\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(337408,)\n",
      "\u001b[32m10-25 20:00:36\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ。例えば、仕事だと「もっと完璧な資料を作らなければならない」とか、家では「もっと家事をきちんとすべきだ」とか…いつも頭の中で、そう思ってしまうんです。\n",
      "\u001b[32m10-25 20:00:36\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(527360,)\n",
      "\u001b[32m10-25 20:00:36\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど、「〜しなければならない」「〜すべきだ」というお気持ちなんですね。ええ。そういった考えが浮かんだ時、Bさんはどんな風に感じられますか？\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: JPCommonLabel_push_word() in jpcommon_label.c: First mora should not be long vowel symbol.\n",
      "WARNING: JPCommonLabel_push_word() in jpcommon_label.c: First mora should not be long vowel symbol.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-25 20:00:36\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(472064,)\n",
      "\u001b[32m10-25 20:00:36\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうですね…すごくプレッシャーを感じて、息苦しくなります。結局、どれも中途半端に終わってしまって、また「だめだ」って自分を責めてしまうんです。\n",
      "\u001b[32m10-25 20:00:36\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(487424,)\n",
      "\u001b[32m10-25 20:00:36\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "それは、とてもお辛いですね。はい。完璧を求めようと頑張っていらっしゃるのに、かえってご自身を責めてしまう、と。\n",
      "\u001b[32m10-25 20:00:36\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(402944,)\n",
      "\u001b[32m10-25 20:00:36\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、まさにそんな感じです。どうしたらいいのか、分からなくて。\n",
      "\u001b[32m10-25 20:00:37\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(215552,)\n",
      "\u001b[32m10-25 20:00:37\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうなんですね。Bさんの中で、「こうあるべき」という考えがとても強くあるんですね。ええ。この「〜すべき」という考え方について、少し一緒に考えてみることはできますか？もしかしたら、その考え方がBさんを少し苦しめているのかもしれません。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: JPCommonLabel_push_word() in jpcommon_label.c: First mora should not be long vowel symbol.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-25 20:00:37\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(809472,)\n",
      "\u001b[32m10-25 20:00:37\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "…はい、そうかもしれません。考えてみたいです。\n",
      "\u001b[32m10-25 20:00:37\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(173056,)\n",
      "\u001b[32m10-25 20:00:37\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ありがとうございます。では、今日のところは、Bさんがおっしゃった「〜すべき」というお気持ちについて、それがどんな時に、どんな風にBさんに影響を与えているのか、少しお話を聞かせていただきました。次回は、もう少し掘り下げて、その「〜すべき」の裏側にあるBさんの大切にされていることなども、一緒に見ていけたらと思いますが、いかがでしょうか。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: JPCommonLabel_insert_pause() in jpcommon_label.c: First mora should not be short pause.\n",
      "WARNING: JPCommonLabel_push_word() in jpcommon_label.c: First mora should not be long vowel symbol.\n",
      "WARNING: JPCommonLabel_push_word() in jpcommon_label.c: First mora should not be long vowel symbol.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-25 20:00:37\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(1113600,)\n",
      "\u001b[32m10-25 20:00:37\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、お願いします。少し、気持ちが楽になりました。\n",
      "\u001b[32m10-25 20:00:37\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(187392,)\n",
      "\u001b[32m10-25 20:00:37\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "よかったです。ええ。では、次回は来週の同じ時間でよろしいでしょうか。\n",
      "\u001b[32m10-25 20:00:38\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(222208,)\n",
      "\u001b[32m10-25 20:00:38\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、大丈夫です。\n",
      "\u001b[32m10-25 20:00:38\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(75776,)\n",
      "\u001b[32m10-25 20:00:38\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "承知いたしました。では、今日はこれで終わりにしましょう。お気をつけてお帰りくださいね。\n",
      "\u001b[32m10-25 20:00:38\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(282624,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:28\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:04\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_3...\u001b[0m                                                                \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_3\u001b[0m!                                                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m139.922\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:31\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_3...\u001b[0m                                                                \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_3\u001b[0m!                                                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m140.959\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-25 20:05:53\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こんにちは、Bさん。前回の続きになりますが、最近何か気になっていることや、お話ししたいことはありますか？\n",
      "\u001b[32m10-25 20:05:53\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(357376,)\n",
      "\u001b[32m10-25 20:05:53\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、先生。最近、なんだか仕事でもプライベートでも、『もっとこうあるべきだ』と考えてしまって、すごく疲れるんです。\n",
      "\u001b[32m10-25 20:05:53\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(373760,)\n",
      "\u001b[32m10-25 20:05:53\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど。『もっとこうあるべきだ』、ですか。ええ、具体的にどのような時にそう感じることが多いですか？\n",
      "\u001b[32m10-25 20:05:53\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(342528,)\n",
      "\u001b[32m10-25 20:05:53\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうですね…。例えば、仕事で新しいプロジェクトを任された時も、『完璧にこなさなければならない』って強く思ってしまって。少しでもうまくいかないと、『自分はダメだ』って責めてしまうんです。\n",
      "\u001b[32m10-25 20:05:54\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(592896,)\n",
      "\u001b[32m10-25 20:05:54\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、はい、そうなんですね。『完璧にこなさなければならない』。ええ、それはとても重荷に感じられるでしょうね。そうした時、どんなお気持ちになりますか？\n",
      "\u001b[32m10-25 20:05:54\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(507904,)\n",
      "\u001b[32m10-25 20:05:54\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "不安になりますし、イライラすることもあります。あと、『もっと早くできるはずだ』とか、『もっと頑張らなければならない』っていう気持ちが常に頭にあって、なかなかリラックスできないんです。\n",
      "\u001b[32m10-25 20:05:54\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(509440,)\n",
      "\u001b[32m10-25 20:05:54\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど、常に『〜するべきだ』『〜しなければならない』というお気持ちが頭の中を巡っているのですね。そうすると、確かに心が休まらないでしょうね。\n",
      "\u001b[32m10-25 20:05:54\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(464384,)\n",
      "\u001b[32m10-25 20:05:54\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、本当に。休むときも『もっと生産的なことをすべきだ』とか考えてしまって…\n",
      "\u001b[32m10-25 20:05:55\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(261632,)\n",
      "\u001b[32m10-25 20:05:55\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、そうなんですね。普段の生活の中でも、ご自身に『〜すべき』というルールを課してしまっている、ということでしょうか。\n",
      "\u001b[32m10-25 20:05:55\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(377856,)\n",
      "\u001b[32m10-25 20:05:55\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうかもしれません。周りの人もみんな頑張っているのに、自分だけができていないんじゃないか、って焦りも感じます。\n",
      "\u001b[32m10-25 20:05:55\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(345600,)\n",
      "\u001b[32m10-25 20:05:55\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど。他の人と比べてしまう気持ち、ええ、よくわかります。その『〜すべき』という考え方があることで、Bさんご自身にとって、何か良いことはありますか？それとも、逆に苦しく感じることの方が多いでしょうか。\n",
      "\u001b[32m10-25 20:05:55\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(757248,)\n",
      "\u001b[32m10-25 20:05:55\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "うーん…良いこと、ですか。あまりないかもしれません。むしろ、ずっと追われているような感覚で、正直しんどいです。\n",
      "\u001b[32m10-25 20:05:55\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(379392,)\n",
      "\u001b[32m10-25 20:05:55\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、そうなんですね。ありがとうございます。そうお話しいただけて、私としても、Bさんがご自身の状況を客観的に見ていらっしゃる証拠だと感じます。今日はこの『〜すべき』という考え方が、Bさんをどれだけ苦しめているのか、少し具体的に見えてきましたね。次回は、この『〜すべき』という考え方が、本当にBさんにとって必要なものなのか、もう少し掘り下げて考えてみましょうか。\n",
      "\u001b[32m10-25 20:05:56\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(1304576,)\n",
      "\u001b[32m10-25 20:05:56\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、お願いします。なんだか、話しているうちに少し楽になった気がします。\n",
      "\u001b[32m10-25 20:05:56\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(253440,)\n",
      "\u001b[32m10-25 20:05:56\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "それは良かったです。ええ、また来週、同じ時間にお待ちしておりますね。\n",
      "\u001b[32m10-25 20:05:56\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(248320,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:38\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_4...\u001b[0m                                                                \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_4\u001b[0m!                                                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m147.910\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:33\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:04\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_4...\u001b[0m                                                                \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_4\u001b[0m!                                                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m144.223\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-25 20:11:24\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こんにちは、〇〇さん。本日は来てくださってありがとうございます。今日はどんなことでお困りですか？\n",
      "\u001b[32m10-25 20:11:24\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(280576,)\n",
      "\u001b[32m10-25 20:11:24\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こんにちは。なんだか、うまく話せるか不安ですが…。最近、仕事のことで漠然とした不安や焦りを感じていて。日常生活全般にもモヤモヤするんです。\n",
      "\u001b[32m10-25 20:11:24\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(495104,)\n",
      "\u001b[32m10-25 20:11:24\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうなんですね。お仕事での不安や焦り、そして日常生活全体へのモヤモヤ、ですか。それはお辛いですね。具体的に、どんな時にそういった気持ちになりますか？\n",
      "\u001b[32m10-25 20:11:24\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(575488,)\n",
      "\u001b[32m10-25 20:11:24\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "うーん…新しいプロジェクトが始まるときなんかは、失敗したらどうしようって。休日も、何かしないとって焦るばかりで、結局何も手につかなくて。\n",
      "\u001b[32m10-25 20:11:24\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(489472,)\n",
      "\u001b[32m10-25 20:11:24\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど。新しいことへのプレッシャーや、休日もリラックスしきれない感じ、ですね。お話を伺っていると、常に何かを気にされているように感じられます。\n",
      "\u001b[32m10-25 20:11:25\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(532992,)\n",
      "\u001b[32m10-25 20:11:25\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、まさにそんな感じです。いつも頭のどこかで、あれこれと考えてしまって、落ち着かないんです。\n",
      "\u001b[32m10-25 20:11:25\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(318976,)\n",
      "\u001b[32m10-25 20:11:25\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、そうなんですね。漠然とした不安や焦りというお気持ち、とてもよく分かります。そういった気持ちは、いつ頃から続くようになりましたか？\n",
      "\u001b[32m10-25 20:11:25\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(482304,)\n",
      "\u001b[32m10-25 20:11:25\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "きっかけははっきりしないんですが、ここ半年くらいでしょうか。だんだんひどくなってきたような気がします。\n",
      "\u001b[32m10-25 20:11:25\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(285696,)\n",
      "\u001b[32m10-25 20:11:25\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうでしたか、半年ほど前から…。今日はお話しいただき、ありがとうございます。漠然とした不安や焦り、それが日常生活にも影響していること、よく分かりました。次回は、そういったお気持ちがどのように現れるのか、もう少し詳しく伺わせていただきたいと思います。\n",
      "\u001b[32m10-25 20:11:25\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(948736,)\n",
      "\u001b[32m10-25 20:11:25\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、ぜひお願いします。少し話しただけでも、なんだかほっとしました。\n",
      "\u001b[32m10-25 20:11:26\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(256512,)\n",
      "\u001b[32m10-25 20:11:26\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "それは良かったです。また来週、お話しできるのを楽しみにしていますね。\n",
      "\u001b[32m10-25 20:11:26\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(240128,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:21\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:04\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_5...\u001b[0m                                                                \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_5\u001b[0m!                                                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m130.219\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:23\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:04\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_5...\u001b[0m                                                                \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_5\u001b[0m!                                                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m133.798\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-25 20:16:38\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こんにちは。今日はよくいらっしゃいましたね。どうぞ楽にしてください。\n",
      "\u001b[32m10-25 20:16:38\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(221696,)\n",
      "\u001b[32m10-25 20:16:38\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ありがとうございます。少し緊張して…仕事のことが一番で、最近、全部うまくいかない気がします。\n",
      "\u001b[32m10-25 20:16:38\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(346112,)\n",
      "\u001b[32m10-25 20:16:38\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうなんですね。ええ。お仕事が一番の悩みなんですね。\n",
      "\u001b[32m10-25 20:16:39\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(200192,)\n",
      "\u001b[32m10-25 20:16:39\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "新しいプロジェクトで常に追われています。家でも仕事が頭から離れず、夜も眠れなくて。\n",
      "\u001b[32m10-25 20:16:39\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(306688,)\n",
      "\u001b[32m10-25 20:16:39\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど。心身ともに休まらない感じですね。眠れないほど、ですか。\n",
      "\u001b[32m10-25 20:16:39\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(237056,)\n",
      "\u001b[32m10-25 20:16:39\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。趣味もやる気が出ず、友人にも愚痴ばかりで、嫌になります。\n",
      "\u001b[32m10-25 20:16:39\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(215040,)\n",
      "\u001b[32m10-25 20:16:39\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ。趣味にも手がつかず、友人との時間も辛いんですね。いつ頃からでしょう？\n",
      "\u001b[32m10-25 20:16:39\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(280064,)\n",
      "\u001b[32m10-25 20:16:39\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "半年くらい前からかと。漠然とした不安や焦りが、いつも心にあります。\n",
      "\u001b[32m10-25 20:16:40\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(257024,)\n",
      "\u001b[32m10-25 20:16:40\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "半年ほど前から、不安や焦りが。お辛いですね。\n",
      "\u001b[32m10-25 20:16:40\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(176128,)\n",
      "\u001b[32m10-25 20:16:40\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "どうにかしたくて、ここに来ました。\n",
      "\u001b[32m10-25 20:16:40\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(113152,)\n",
      "\u001b[32m10-25 20:16:40\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "今日、話してくださりありがとうございます。大きな一歩ですよ。これから一緒に、楽になる方法を考えていきましょう。\n",
      "\u001b[32m10-25 20:16:40\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(379904,)\n",
      "\u001b[32m10-25 20:16:40\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、ありがとうございます。少し安心しました。\n",
      "\u001b[32m10-25 20:16:40\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(181248,)\n",
      "\u001b[32m10-25 20:16:40\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "よかったです。今日はまずお気持ちを伺いました。次回はもう少し詳しく。来週の同じ時間でいかがですか？\n",
      "\u001b[32m10-25 20:16:41\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(353792,)\n",
      "\u001b[32m10-25 20:16:41\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、大丈夫です。よろしくお願いします。\n",
      "\u001b[32m10-25 20:16:41\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(145408,)\n",
      "\u001b[32m10-25 20:16:41\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、では今日はこれで終わりにしましょう。お疲れ様でした。\n",
      "\u001b[32m10-25 20:16:41\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(190464,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:18\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:04\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_6...\u001b[0m                                                                \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_6\u001b[0m!                                                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m129.702\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:13\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:04\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_6...\u001b[0m                                                                \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_6\u001b[0m!                                                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m121.544\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-25 20:22:09\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "〇〇さん、こんにちは。今日はこれまでのセッションを振り返りながら、終結に向けてのお話をできたらと思っていますが、いかがでしょうか。\n",
      "\u001b[32m10-25 20:22:09\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(412672,)\n",
      "\u001b[32m10-25 20:22:09\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、もうそんな時期なんですね。あっという間でしたが、本当に色々なことが変わったように感じます。\n",
      "\u001b[32m10-25 20:22:09\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(328192,)\n",
      "\u001b[32m10-25 20:22:09\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、そうですね。はい。最初にお越しになった頃は、漠然とした不安を抱えていらっしゃいましたね。今、特にどんな変化を感じていらっしゃいますか。\n",
      "\u001b[32m10-25 20:22:09\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(478208,)\n",
      "\u001b[32m10-25 20:22:09\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "一番は、自分の気持ちに気づけるようになったこと、でしょうか。感情の理由が少し冷静に捉えられるようになりました。\n",
      "\u001b[32m10-25 20:22:10\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(361472,)\n",
      "\u001b[32m10-25 20:22:10\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど。ご自身の感情を客観的に理解できるようになったのですね。それは大きな進歩ですね。ええ。\n",
      "\u001b[32m10-25 20:22:10\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(351744,)\n",
      "\u001b[32m10-25 20:22:10\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。あと、以前は苦手だった人前での発言も、この間の会議では、ちゃんと自分の意見を伝えられました。緊張はしましたが、黙らずに済みました。\n",
      "\u001b[32m10-25 20:22:10\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(508928,)\n",
      "\u001b[32m10-25 20:22:10\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "素晴らしいですね！ご自身で一歩踏み出せた。はい、〇〇さんがご自身の内面と向き合い、努力されてきたことが伝わってきます。そうなんですね。\n",
      "\u001b[32m10-25 20:22:10\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(459264,)\n",
      "\u001b[32m10-25 20:22:10\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ありがとうございます。でも、この先、また不安になったり、うまくいかなくなったりしないかと思うと、少し心配で…。\n",
      "\u001b[32m10-25 20:22:10\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(369152,)\n",
      "\u001b[32m10-25 20:22:10\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、そうですね。それは誰にでも起こりうることです。はい。でも、〇〇さんには、もうそのための力が備わっていますよ。\n",
      "\u001b[32m10-25 20:22:11\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(390656,)\n",
      "\u001b[32m10-25 20:22:11\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "私に、ですか？\n",
      "\u001b[32m10-25 20:22:11\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(48640,)\n",
      "\u001b[32m10-25 20:22:11\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。セッションで学んだ、ご自身の認知や感情、行動を見つめ、悪循環を断ち切るための具体的な方法ですね。それを思い出して活用してください。\n",
      "\u001b[32m10-25 20:22:11\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(521728,)\n",
      "\u001b[32m10-25 20:22:11\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうですね…一人で抱え込まずに、試してみます。なんだか、少し自信が持てました。次回の最終セッションもよろしくお願いします。\n",
      "\u001b[32m10-25 20:22:11\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(424448,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:22\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_7...\u001b[0m                                                                \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_7\u001b[0m!                                                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m131.632\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:20\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:04\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_7...\u001b[0m                                                                \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_7\u001b[0m!                                                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m130.822\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-25 20:27:27\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こんにちは。今日はよく来てくださいましたね。どうぞ楽にしてください。\n",
      "\u001b[32m10-25 20:27:27\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(227840,)\n",
      "\u001b[32m10-25 20:27:27\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、緊張しています。\n",
      "\u001b[32m10-25 20:27:27\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(87040,)\n",
      "\u001b[32m10-25 20:27:27\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、そうですよね。初めての場所は緊張しますものね。今日はどんなことをお話ししたい気分ですか？\n",
      "\u001b[32m10-25 20:27:27\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(329728,)\n",
      "\u001b[32m10-25 20:27:27\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "最近、なんだか漠然とした不安があって、仕事もプライベートも、このままでいいのかなって焦るんです。\n",
      "\u001b[32m10-25 20:27:27\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(334336,)\n",
      "\u001b[32m10-25 20:27:27\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうなんですね。漠然とした不安や焦りですね。いつ頃からでしょう？\n",
      "\u001b[32m10-25 20:27:28\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(244736,)\n",
      "\u001b[32m10-25 20:27:28\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "きっかけは難しいですが、半年くらいでしょうか。特に変化もないのに、いつもモヤモヤしていて。\n",
      "\u001b[32m10-25 20:27:28\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(295424,)\n",
      "\u001b[32m10-25 20:27:28\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど。半年くらいモヤモヤが。お仕事で特にそう感じることは？\n",
      "\u001b[32m10-25 20:27:28\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(246784,)\n",
      "\u001b[32m10-25 20:27:28\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、周りと比べて自分だけ遅れてる気がして、自信が持てません。意見も言えなくて…。\n",
      "\u001b[32m10-25 20:27:28\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(317440,)\n",
      "\u001b[32m10-25 20:27:28\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、そうなんですね。比較したり、意見を言えなかったり。お辛いですね。\n",
      "\u001b[32m10-25 20:27:28\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(262656,)\n",
      "\u001b[32m10-25 20:27:28\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。もっとしっかりしなきゃと思うのに、どうしたらいいか分からなくて。\n",
      "\u001b[32m10-25 20:27:29\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(204288,)\n",
      "\u001b[32m10-25 20:27:29\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど。しっかりしたいけど、分からないと。日常生活への影響は？\n",
      "\u001b[32m10-25 20:27:29\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(244224,)\n",
      "\u001b[32m10-25 20:27:29\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、家でも気分が晴れず、休日も何もする気が起きません。\n",
      "\u001b[32m10-25 20:27:29\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(211968,)\n",
      "\u001b[32m10-25 20:27:29\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうなんですね。お仕事の自信のなさや「しっかりしなきゃ」という気持ちが、休日にも影響しているのですね。今日は、漠然とした不安の背景を少し伺えました。次回、もう少し詳しくお聞かせくださいね。\n",
      "\u001b[32m10-25 20:27:29\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(671232,)\n",
      "\u001b[32m10-25 20:27:29\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、話せて楽になりました。来週もよろしくお願いします。\n",
      "\u001b[32m10-25 20:27:29\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(219648,)\n",
      "\u001b[32m10-25 20:27:29\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、こちらこそ。来週もまた、お話しできるのを楽しみにしていますね。\n",
      "\u001b[32m10-25 20:27:29\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(246784,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:20\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:04\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_8...\u001b[0m                                                                \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_8\u001b[0m!                                                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m130.633\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:17\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:06\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_8...\u001b[0m                                                                \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_8\u001b[0m!                                                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m128.747\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-25 20:32:24\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "Bさん、こんにちは。今日はこれまでのセッションを少し振り返ってみませんか。ここまで、色々なことをお話しくださいましたものね。\n",
      "\u001b[32m10-25 20:32:24\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(408064,)\n",
      "\u001b[32m10-25 20:32:24\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、そうですね。ええ。最初は漠然とした不安でいっぱいでした。仕事のことも、プライベートのことも、なんだか全部がうまくいかないような気がして…。\n",
      "\u001b[32m10-25 20:32:24\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(527872,)\n",
      "\u001b[32m10-25 20:32:24\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、そうでしたね。漠然とした不安、そしてお仕事のことや、日常生活での焦り…「どうしたらいいんだろう」というお気持ちをたくさんお話ししてくださいましたね。はい。\n",
      "\u001b[32m10-25 20:32:24\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(575488,)\n",
      "\u001b[32m10-25 20:32:24\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。でも、ここにきて先生とお話しするうちに、少しずつ「何が不安の原因なのか」とか、「どうして焦ってしまうのか」といった、より具体的な問題として捉えられるようになってきた気がします。\n",
      "\u001b[32m10-25 20:32:25\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(593408,)\n",
      "\u001b[32m10-25 20:32:25\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうなんですね。漠然としたものが、具体的な形として見えてきた。それは大きな変化ですね。ご自身でも、そう感じていらっしゃるんですね。\n",
      "\u001b[32m10-25 20:32:25\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(487424,)\n",
      "\u001b[32m10-25 20:32:25\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "特に、「こうあるべきだ」って自分で自分を縛っていたことに気づけたのが大きかったです。完璧にこなさなきゃ、とか、もっと頑張らなきゃ、とか…。\n",
      "\u001b[32m10-25 20:32:25\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(424448,)\n",
      "\u001b[32m10-25 20:32:25\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど。「こうあるべき」という、ご自身の考え方、認知に気づけた。はい。それは、本当に大きな発見でしたよね。\n",
      "\u001b[32m10-25 20:32:25\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(425984,)\n",
      "\u001b[32m10-25 20:32:25\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。それがわかってからは、少し気持ちが楽になって、前向きに行動できるようになってきたかなって思います。以前ならためらっていたことにも、挑戦してみようかなって。\n",
      "\u001b[32m10-25 20:32:25\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(512512,)\n",
      "\u001b[32m10-25 20:32:25\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、素晴らしいですね。ご自身で悪循環に気づき、それを断ち切るための行動も、一つ一つ丁寧に実践されてきた。本当に、よく頑張られましたね。\n",
      "\u001b[32m10-25 20:32:25\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(548864,)\n",
      "\u001b[32m10-25 20:32:25\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ホームワークで小さな目標を立てて試してみるのも、最初は正直不安でした。でも、やってみたら「意外と大丈夫だった」とか、「思ったよりうまくいった」っていう経験が増えて。\n",
      "\u001b[32m10-25 20:32:26\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(523776,)\n",
      "\u001b[32m10-25 20:32:26\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうでしたね。一歩踏み出すこと、とても勇気が要ることだったと思います。そうした小さな成功体験が、今のBさんの自信に繋がっているんですね。ええ、とてもよく分かります。\n",
      "\u001b[32m10-25 20:32:26\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(599040,)\n",
      "\u001b[32m10-25 20:32:26\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、本当にそう思います。先生とお話しできて、本当に良かったです。私一人では、ここまでたどり着けなかったと思います。\n",
      "\u001b[32m10-25 20:32:26\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(423424,)\n",
      "\u001b[32m10-25 20:32:26\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、ありがとうございます。Bさんがご自身の力で、ここまでたくさんの変化を起こされてきたこと、私も大変嬉しく思います。はい。さて、ここまで来る中で、Bさんご自身も随分と安定されてきたように見受けられますが、そろそろ今後のセッションについて、少しずつ考えていく時期かもしれませんね。\n",
      "\u001b[32m10-25 20:32:26\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(1012224,)\n",
      "\u001b[32m10-25 20:32:26\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうですね…。少し寂しい気もしますが、もう大丈夫かな、っていう気持ちもあります。正直なところ、以前ほど頻繁にお話ししなくても良いかな、とも感じています。\n",
      "\u001b[32m10-25 20:32:27\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(509952,)\n",
      "\u001b[32m10-25 20:32:27\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうですね。ええ。もちろん、いつでもまたいらしていただいても構いません。その上で、一度、今後のペースなども含めて、次回のセッションで具体的に相談させていただけますか？ Bさんのペースで、無理なく進めていきましょう。\n",
      "\u001b[32m10-25 20:32:27\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(697856,)\n",
      "\u001b[32m10-25 20:32:27\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、わかりました。ありがとうございます。とても安心しました。\n",
      "\u001b[32m10-25 20:32:27\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(244736,)\n",
      "\u001b[32m10-25 20:32:27\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ。それでは、本日はここまでにしましょうか。\n",
      "\u001b[32m10-25 20:32:27\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(148480,)\n",
      "\u001b[32m10-25 20:32:27\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。今日もありがとうございました。\n",
      "\u001b[32m10-25 20:32:27\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(115200,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:45\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_9...\u001b[0m                                                                \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_9\u001b[0m!                                                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m155.832\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:43\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:04\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_9...\u001b[0m                                                                \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_9\u001b[0m!                                                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m150.869\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-25 20:38:32\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "Bさん、こんにちは。前回の後、何か気づかれたことはありましたか？\n",
      "\u001b[32m10-25 20:38:32\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(243200,)\n",
      "\u001b[32m10-25 20:38:32\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、常に「もっとこうしなきゃ」とか「もっと頑張らなきゃ」って焦っている自分がいて、それがしんどいです。\n",
      "\u001b[32m10-25 20:38:32\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(293888,)\n",
      "\u001b[32m10-25 20:38:32\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "「もっとこうしなきゃ、頑張らなきゃ」というお気持ちですね。ええ。どんな時に特に強く感じられますか？\n",
      "\u001b[32m10-25 20:38:33\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(317440,)\n",
      "\u001b[32m10-25 20:38:33\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "仕事中も、家でも「部屋をきれいにすべきだ」とか「もっと趣味に時間を使うべきなのに」って、考えてしまいます。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: JPCommonLabel_insert_pause() in jpcommon_label.c: First mora should not be short pause.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-25 20:38:33\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(337920,)\n",
      "\u001b[32m10-25 20:38:33\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど。仕事でも自宅でも、「〜すべきだ」という考えが巡るのですね。そういった時、どんな風に感じますか？\n",
      "\u001b[32m10-25 20:38:33\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(379904,)\n",
      "\u001b[32m10-25 20:38:33\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "焦りや、「自分はダメだ」という不安な気持ちになります。全然できてないって。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: JPCommonLabel_push_word() in jpcommon_label.c: First mora should not be long vowel symbol.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-25 20:38:33\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(254464,)\n",
      "\u001b[32m10-25 20:38:33\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "焦りや不安、ご自身を責める気持ちに繋がるのですね。その「すべき」という考えが、Bさんを苦しめていると感じることは？\n",
      "\u001b[32m10-25 20:38:33\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(403968,)\n",
      "\u001b[32m10-25 20:38:33\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "まさにそうです。わかっていても、止められなくて……。\n",
      "\u001b[32m10-25 20:38:33\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(164864,)\n",
      "\u001b[32m10-25 20:38:33\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、そうなんですね。この「〜すべきだ」という考え、Bさんの頑張り屋さんの証拠でもありますね。でも、それが時にBさんを追い詰めているのかもしれません。今日は、この「すべき思考」の影響についてお話しできました。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: JPCommonLabel_push_word() in jpcommon_label.c: First mora should not be long vowel symbol.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-25 20:38:34\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(712192,)\n",
      "\u001b[32m10-25 20:38:34\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。少し客観的に見られた気がします。\n",
      "\u001b[32m10-25 20:38:34\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(143360,)\n",
      "\u001b[32m10-25 20:38:34\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "大きな気づきですね。次回は、この「すべき思考」と少し距離を置くための方法を、一緒に考えてみましょう。\n",
      "\u001b[32m10-25 20:38:34\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(354304,)\n",
      "\u001b[32m10-25 20:38:34\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、ぜひお願いします。\n",
      "\u001b[32m10-25 20:38:34\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(88064,)\n",
      "\u001b[32m10-25 20:38:34\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ありがとうございます。では、今日のセッションはここまでとさせていただきます。お疲れ様でした。\n",
      "\u001b[32m10-25 20:38:35\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(299520,)\n",
      "\u001b[32m10-25 20:38:35\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ありがとうございました。\n",
      "\u001b[32m10-25 20:38:35\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(62464,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:16\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:04\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_10...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_10\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m124.800\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:17\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_10...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_10\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m128.002\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-25 20:43:17\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こんにちは。本日はお越しいただきありがとうございます。今日はどんなことをお話ししたい気分ですか？\n",
      "\u001b[32m10-25 20:43:17\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(313344,)\n",
      "\u001b[32m10-25 20:43:17\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こんにちは。あの、なんだか最近、色々と漠然とした不安や焦りを感じていて…何から話したらいいのか、自分でもよくわからないんです。\n",
      "\u001b[32m10-25 20:43:17\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(459264,)\n",
      "\u001b[32m10-25 20:43:17\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうなんですね。漠然とした不安や、焦りを感じていらっしゃるんですね。ええ。自分でも何から話したらいいか分からない、というお気持ち、よく分かりますよ。もしよろしければ、今一番、頭に浮かんでいることや、最近気になっていることから、お話しいただけますか？\n",
      "\u001b[32m10-25 20:43:18\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(868352,)\n",
      "\u001b[32m10-25 20:43:18\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい…そうですね。仕事のこともそうなんですけど、なんかこう、家でも落ち着かなくて。常に何かしていないといけない、って思っちゃうのに、結局何も手につかなくて自己嫌悪に陥る、みたいな。\n",
      "\u001b[32m10-25 20:43:18\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(640512,)\n",
      "\u001b[32m10-25 20:43:18\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど。お仕事だけでなく、お家でも落ち着かないと感じていらっしゃるんですね。はい。何も手につかないのに、「何かしないといけない」と思ってしまう…そうなんですね。そういった状況は、いつ頃から感じることが多いですか？\n",
      "\u001b[32m10-25 20:43:18\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(745984,)\n",
      "\u001b[32m10-25 20:43:18\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "多分、半年前くらいからでしょうか。新しいプロジェクトが始まってから、特にひどくなった気がします。でも、仕事だけが原因じゃないような気もするんです。\n",
      "\u001b[32m10-25 20:43:18\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(458752,)\n",
      "\u001b[32m10-25 20:43:18\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、そうなんですね。新しいプロジェクトがきっかけで、より強く感じるようになったのですね。はい。仕事だけでなく、日常全体にも影響が出ているということでしたね。もしよろしければ、日常生活で特に「困るな」と感じる瞬間はありますか？\n",
      "\u001b[32m10-25 20:43:19\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(836096,)\n",
      "\u001b[32m10-25 20:43:19\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "うーん…例えば、週末に友達と会う約束をしていても、前日から「ちゃんと楽しめるかな」とか「つまらないって思われたらどうしよう」とか考えてしまって、全然リラックスできないこと、とかでしょうか。\n",
      "\u001b[32m10-25 20:43:19\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(596480,)\n",
      "\u001b[32m10-25 20:43:19\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど。お友達との時間も、心から楽しめないと感じていらっしゃるんですね。ええ。それは、お辛いですね。そういう時、どんな気持ちになりますか？\n",
      "\u001b[32m10-25 20:43:19\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(531456,)\n",
      "\u001b[32m10-25 20:43:19\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうですね…「またか」って、がっかりする気持ちと、「どうしてこんな風にしか考えられないんだろう」って、自分を責めてしまう気持ちが強いです。\n",
      "\u001b[32m10-25 20:43:19\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(429568,)\n",
      "\u001b[32m10-25 20:43:19\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "自分を責めてしまう気持ちが強いんですね。ええ。今日はお仕事のこと、そして日常生活での漠然とした不安について、お話しくださってありがとうございます。少しずつ、ご自身の状況を整理するきっかけになったでしょうか。\n",
      "\u001b[32m10-25 20:43:19\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(731648,)\n",
      "\u001b[32m10-25 20:43:20\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、少しスッキリしました。ありがとうございます。\n",
      "\u001b[32m10-25 20:43:20\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(185856,)\n",
      "\u001b[32m10-25 20:43:20\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "よかったです。今日お話しいただいた中で、特に気になったのは、新しいプロジェクトが始まってからの仕事のプレッシャーと、それが日常生活にも影響している部分ですね。次回は、その「仕事のプレッシャー」について、もう少し詳しくお伺いしてもよろしいでしょうか？\n",
      "\u001b[32m10-25 20:43:20\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(821760,)\n",
      "\u001b[32m10-25 20:43:20\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、ぜひお願いします。\n",
      "\u001b[32m10-25 20:43:20\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(89088,)\n",
      "\u001b[32m10-25 20:43:20\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ありがとうございます。では、来週の同じ時間で、またお会いできますでしょうか？\n",
      "\u001b[32m10-25 20:43:20\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(240640,)\n",
      "\u001b[32m10-25 20:43:20\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、大丈夫です。\n",
      "\u001b[32m10-25 20:43:21\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(74752,)\n",
      "\u001b[32m10-25 20:43:21\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、承知いたしました。今日はありがとうございました。\n",
      "\u001b[32m10-25 20:43:21\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(175616,)\n",
      "\u001b[32m10-25 20:43:21\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ありがとうございました。\n",
      "\u001b[32m10-25 20:43:21\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(60416,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:39\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:04\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_11...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_11\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m150.466\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:44\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:04\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_11...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_11\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m152.175\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-25 20:48:51\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こんにちは、今日は来てくださってありがとうございます。今日はどんなことをお話ししたいですか？ゆっくりお話しくださいね。\n",
      "\u001b[32m10-25 20:48:51\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(372224,)\n",
      "\u001b[32m10-25 20:48:51\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こんにちは。あの、どこから話したらいいのか、ちょっと漠然としすぎているんですけど…最近、なんだかずっと不安で、焦っているような気がして。仕事もプライベートも、全部うまくいっていない気がするんです。\n",
      "\u001b[32m10-25 20:48:51\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(671744,)\n",
      "\u001b[32m10-25 20:48:51\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、そうなんですね。漠然とした不安や焦りを感じていらっしゃるんですね。ええ、大丈夫ですよ。焦らなくても、ゆっくりで構いませんからね。具体的に、どんな時にそのように感じることが多いですか？\n",
      "\u001b[32m10-25 20:48:52\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(675840,)\n",
      "\u001b[32m10-25 20:48:52\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうですね…仕事中はもちろん、家に帰ってからも、ふと「このままでいいのかな」って考えてしまって。漠然と、将来が不安になるというか。週末も、何をしても気分が晴れないことが増えました。\n",
      "\u001b[32m10-25 20:48:52\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(650240,)\n",
      "\u001b[32m10-25 20:48:52\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど。仕事中だけでなく、お休みの日も気分が晴れない、ということが続いているんですね。ええ、それはお辛いですね。特に、仕事ではどんなことで悩まれることが多いですか？\n",
      "\u001b[32m10-25 20:48:52\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(613888,)\n",
      "\u001b[32m10-25 20:48:52\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "仕事は、まあ、人間関係もそうですけど、自分の成果が出せているのか不安で。周りの人はもっと頑張っているのに、自分だけ置いていかれているような気持ちになるんです。それで、余計に焦ってしまって。\n",
      "\u001b[32m10-25 20:48:52\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(635904,)\n",
      "\u001b[32m10-25 20:48:52\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうなんですね。周りの方と比べてしまって、ご自身の成果に不安を感じることも多いんですね。はい。その焦りが、日常生活にも影響していると感じることはありますか？例えば、食欲が落ちたり、眠れなくなったりとか。\n",
      "\u001b[32m10-25 20:48:53\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(765952,)\n",
      "\u001b[32m10-25 20:48:53\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、まさに。食欲は、なんだかあまりない日もあって。夜も、寝つきが悪くなったり、途中で目が覚めたりすることが増えました。朝起きるのも、だるくて…。\n",
      "\u001b[32m10-25 20:48:53\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(504320,)\n",
      "\u001b[32m10-25 20:48:53\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、食欲や睡眠にも影響が出ているんですね。お辛い状況ですね。今は「漠然とした不安」というお話でしたけれど、もし仮に、その漠然とした不安が、もし形になるとしたら、どんな形をしていると思いますか？\n",
      "\u001b[32m10-25 20:48:53\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(759296,)\n",
      "\u001b[32m10-25 20:48:53\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "形…ですか。うーん、そうですね。なんだろう。はっきりしない、モヤモヤした雲みたいな感じでしょうか。掴もうとしても、すり抜けていってしまうような。何が原因なのかも、よくわからなくて。\n",
      "\u001b[32m10-25 20:48:53\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(660480,)\n",
      "\u001b[32m10-25 20:48:53\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、モヤモヤした雲のよう、掴もうとしてもすり抜けてしまう、という感覚なんですね。ええ、そういったお気持ち、よくわかりますよ。原因がはっきりしないと、余計にどうしたらいいか分からなくなってしまいますよね。今日は、ご自身の感じていることをたくさんお話してくださって、ありがとうございます。漠然とした不安が、どんな時に、どんなふうに現れるのか、少しずつ見えてきたように思います。次回は、この「モヤモヤ」が、どんな場面で特に強く感じられるのか、もう少し詳しくお話しできたら嬉しいのですが、いかがでしょうか？\n",
      "\u001b[32m10-25 20:48:54\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(1753088,)\n",
      "\u001b[32m10-25 20:48:54\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、わかりました。今日は少し話せて、少しだけ気持ちが軽くなった気がします。次回もよろしくお願いします。\n",
      "\u001b[32m10-25 20:48:54\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(388608,)\n",
      "\u001b[32m10-25 20:48:54\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、よかったです。来週の同じ時間でよろしいでしょうか。それでは、今日はこれで終わりにしましょう。お疲れ様でした。\n",
      "\u001b[32m10-25 20:48:54\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(382976,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:45\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:04\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_12...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_12\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m156.014\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h\u001b[?25l"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:50\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_12...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_12\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m160.648\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-25 20:54:56\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こんにちは、〇〇さん。今日はどのようなことについてお話ししたいですか？\n",
      "\u001b[32m10-25 20:54:56\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(190976,)\n",
      "\u001b[32m10-25 20:54:56\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こんにちは。最近、漠然とした不安と焦りがあって。仕事も家でも落ち着かないんです。\n",
      "\u001b[32m10-25 20:54:56\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(286208,)\n",
      "\u001b[32m10-25 20:54:56\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうですか。漠然とした不安や焦りですね。仕事でも家でも、ですか。ええ、お辛いですね。\n",
      "\u001b[32m10-25 20:54:57\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(335872,)\n",
      "\u001b[32m10-25 20:54:57\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。常に『何かしないと』って、頭のどこかで思ってる気がして。\n",
      "\u001b[32m10-25 20:54:57\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(218112,)\n",
      "\u001b[32m10-25 20:54:57\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど。『何かしないと』ですか。どんな時に強く感じますか？\n",
      "\u001b[32m10-25 20:54:57\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(220160,)\n",
      "\u001b[32m10-25 20:54:57\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "朝からタスクが浮かんで集中できず、夜も寝付けません。すぐ目が覚めてしまって。\n",
      "\u001b[32m10-25 20:54:57\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(267264,)\n",
      "\u001b[32m10-25 20:54:57\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、朝から辛く、夜もゆっくり休めないのですね。それはお疲れでしょう。\n",
      "\u001b[32m10-25 20:54:57\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(281088,)\n",
      "\u001b[32m10-25 20:54:57\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "疲れてるのに、休むのも申し訳ないような気がして。昔はこんなことなかったんですけど。\n",
      "\u001b[32m10-25 20:54:58\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(276480,)\n",
      "\u001b[32m10-25 20:54:58\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうなんですね。休むことに罪悪感…以前とは違ったのですね。何かきっかけはありましたか？\n",
      "\u001b[32m10-25 20:54:58\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(336384,)\n",
      "\u001b[32m10-25 20:54:58\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "明確には…でも、この1年くらいで仕事の責任が増えたあたりから、かもしれません。\n",
      "\u001b[32m10-25 20:54:58\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(270848,)\n",
      "\u001b[32m10-25 20:54:58\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど、仕事の責任が増えたあたりから不安や焦りを感じ始めた、と。ええ、変化があったのかもしれませんね。\n",
      "\u001b[32m10-25 20:54:58\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(378880,)\n",
      "\u001b[32m10-25 20:54:58\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "今日は、漠然とした不安や、それが仕事や睡眠に影響していることについて伺いました。次回、もう少し詳しくお話しいただけますか？\n",
      "\u001b[32m10-25 20:54:58\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(451584,)\n",
      "\u001b[32m10-25 20:54:58\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、ありがとうございます。\n",
      "\u001b[32m10-25 20:54:59\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(96768,)\n",
      "\u001b[32m10-25 20:54:59\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "では、次回の予約は後ほどご連絡しますね。今日は本当にお疲れ様でした。\n",
      "\u001b[32m10-25 20:54:59\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(270848,)\n",
      "\u001b[32m10-25 20:54:59\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、ありがとうございました。\n",
      "\u001b[32m10-25 20:54:59\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(102912,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:22\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:04\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_13...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_13\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m131.391\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:15\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:06\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_13...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_13\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m127.027\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-25 21:00:26\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こんにちは。今日はよくお越しくださいましたね。何か、お話ししたいことなどありますか？\n",
      "\u001b[32m10-25 21:00:27\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(276992,)\n",
      "\u001b[32m10-25 21:00:27\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こんにちは。漠然とした不安がずっとあって。仕事もプライベートも、落ち着かないんです。\n",
      "\u001b[32m10-25 21:00:27\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(290304,)\n",
      "\u001b[32m10-25 21:00:27\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうなんですね。全体的に落ち着かない、漠然とした不安…ええ、そうでしたか。\n",
      "\u001b[32m10-25 21:00:27\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(308224,)\n",
      "\u001b[32m10-25 21:00:27\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。仕事でミスが増えたり、集中できなかったり。休みの日も「何かやらなきゃ」って焦るばかりで。\n",
      "\u001b[32m10-25 21:00:27\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(328704,)\n",
      "\u001b[32m10-25 21:00:27\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど。仕事や休日でも焦りを感じ、手につかないのですね。お辛いでしょう。\n",
      "\u001b[32m10-25 21:00:27\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(285184,)\n",
      "\u001b[32m10-25 21:00:27\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい…。このままでいいのか、周りと比べて自分だけ置いていかれる気がして、それが不安で。\n",
      "\u001b[32m10-25 21:00:28\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(301056,)\n",
      "\u001b[32m10-25 21:00:28\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "「このままでいいのか」と。周りとの比較で焦りも感じるのですね。よく分かります。\n",
      "\u001b[32m10-25 21:00:28\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(283648,)\n",
      "\u001b[32m10-25 21:00:28\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "昔はもっとやる気に満ちてたのに、今はどうして、って自分を責めることもあって。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: JPCommonLabel_insert_pause() in jpcommon_label.c: First mora should not be short pause.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-25 21:00:28\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(227840,)\n",
      "\u001b[32m10-25 21:00:28\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、ご自身の変化を感じ、責める気持ちもあるのですね。今日は、ここまでお話しいただけて良かったです。\n",
      "\u001b[32m10-25 21:00:28\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(356864,)\n",
      "\u001b[32m10-25 21:00:28\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "次回、もう少し詳しく伺えたらと思いますが、いかがでしょうか。\n",
      "\u001b[32m10-25 21:00:28\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(212480,)\n",
      "\u001b[32m10-25 21:00:28\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、ぜひお願いします。少し話しただけでも、気持ちが楽になりました。\n",
      "\u001b[32m10-25 21:00:28\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(255488,)\n",
      "\u001b[32m10-25 21:00:28\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "それは良かったです。では、次回の予約についてですが…。\n",
      "\u001b[32m10-25 21:00:29\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(183296,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:14\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:04\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_14...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_14\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m125.217\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h\u001b[?25l"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:13\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:04\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_14...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_14\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m123.594\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-25 21:05:42\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こんにちは、Bさん。今日はどんな感じですか？\n",
      "\u001b[32m10-25 21:05:43\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(132608,)\n",
      "\u001b[32m10-25 21:05:43\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こんにちは、Aさん。毎日「〜しなきゃ」って焦る気持ちが強くて、疲れます。\n",
      "\u001b[32m10-25 21:05:43\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: JPCommonLabel_push_word() in jpcommon_label.c: First mora should not be long vowel symbol.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(242176,)\n",
      "\u001b[32m10-25 21:05:43\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど、ええ。「〜しなきゃ」というお気持ちですね。どんな時にそう感じますか？\n",
      "\u001b[32m10-25 21:05:43\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: JPCommonLabel_push_word() in jpcommon_label.c: First mora should not be long vowel symbol.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(259072,)\n",
      "\u001b[32m10-25 21:05:43\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、仕事中です。「完璧に」「すぐ返信」とか、全部「〜すべき」って考えちゃって。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: JPCommonLabel_push_word() in jpcommon_label.c: First mora should not be long vowel symbol.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-25 21:05:43\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(297472,)\n",
      "\u001b[32m10-25 21:05:43\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうなんですね。その「〜すべき」がBさんを追い詰めているように聞こえます。どんなお気持ちですか？\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: JPCommonLabel_push_word() in jpcommon_label.c: First mora should not be long vowel symbol.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-25 21:05:43\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(302080,)\n",
      "\u001b[32m10-25 21:05:43\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、息苦しくて、できないと「ダメだ」って、落ち込んじゃうんです。\n",
      "\u001b[32m10-25 21:05:44\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(186368,)\n",
      "\u001b[32m10-25 21:05:44\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど、お辛いですね。ご自身を苦しめている感覚でしょうか。\n",
      "\u001b[32m10-25 21:05:44\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(230400,)\n",
      "\u001b[32m10-25 21:05:44\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、まさに。完璧じゃないと、評価されないんじゃないかって不安で。\n",
      "\u001b[32m10-25 21:05:44\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(208896,)\n",
      "\u001b[32m10-25 21:05:44\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ。「評価されない」不安から、「〜すべき」が強くなるのかもしれませんね。完璧でなくても大丈夫、と思えたら、少し楽になれそうですか？\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: JPCommonLabel_push_word() in jpcommon_label.c: First mora should not be long vowel symbol.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-25 21:05:44\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(463872,)\n",
      "\u001b[32m10-25 21:05:44\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "うーん……そう言われると、そうですね。でも、難しいです。\n",
      "\u001b[32m10-25 21:05:44\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(210432,)\n",
      "\u001b[32m10-25 21:05:44\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、すぐには難しいですよね。でも、その「〜すべき」に気づくのが大切なんです。次回までに、「本当にそうかな？」と、少し立ち止まって考えてみませんか？\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: JPCommonLabel_push_word() in jpcommon_label.c: First mora should not be long vowel symbol.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-25 21:05:45\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(535552,)\n",
      "\u001b[32m10-25 21:05:45\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、やってみます。\n",
      "\u001b[32m10-25 21:05:45\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(78336,)\n",
      "\u001b[32m10-25 21:05:45\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ。今日のセッションで、ご自身の考えに気づけたのは大きな一歩です。では、次回の予約ですが……。\n",
      "\u001b[32m10-25 21:05:45\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(338944,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:15\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:04\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_15...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_15\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m123.296\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:13\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_15...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_15\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m122.651\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-25 21:10:22\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こんにちは、〇〇さん。今日は来てくださってありがとうございます。今日はどんなことをお話しいただけますか？\n",
      "\u001b[32m10-25 21:10:23\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(288768,)\n",
      "\u001b[32m10-25 21:10:23\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こんにちは。あの、最近、なんだか漠然と不安な気持ちが続いていて、何から話したらいいのか…正直、よくわからなくて。\n",
      "\u001b[32m10-25 21:10:23\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(458752,)\n",
      "\u001b[32m10-25 21:10:23\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうなんですね。漠然とした不安な気持ちが続いているんですね。ええ、大丈夫ですよ。話したいことから、ゆっくりお話しくださいね。私も一緒に考えさせていただきますから。\n",
      "\u001b[32m10-25 21:10:23\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(601600,)\n",
      "\u001b[32m10-25 21:10:23\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい…。仕事のことももちろんあるんですが、家での時間もなんとなく落ち着かないというか、常に焦っているような感じがして…休むべき時も、休めている気がしないんです。\n",
      "\u001b[32m10-25 21:10:23\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(548352,)\n",
      "\u001b[32m10-25 21:10:23\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど。仕事だけでなく、お家でも落ち着かない感じがするんですね。常に焦っているような、と。それは、いつ頃からそう感じるようになりましたか？\n",
      "\u001b[32m10-25 21:10:23\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(485888,)\n",
      "\u001b[32m10-25 21:10:23\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうですね…、半年くらい前からでしょうか。大きな出来事があったわけではないんですけど、小さなことが積み重なって、気づいたらこんな感じに…なってしまいました。\n",
      "\u001b[32m10-25 21:10:24\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(541696,)\n",
      "\u001b[32m10-25 21:10:24\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、半年くらい前から。大きな出来事があったわけではないけれど、小さなことが積み重なって、ですね。そうなんですね…。それは大変でしたね。\n",
      "\u001b[32m10-25 21:10:24\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(501760,)\n",
      "\u001b[32m10-25 21:10:24\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。何をしてても『これでいいのかな』とか、『もっと何かしないと』って考えてしまって、なかなか目の前のことに集中できないし、気分転換もできなくて。\n",
      "\u001b[32m10-25 21:10:24\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(451072,)\n",
      "\u001b[32m10-25 21:10:24\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど。『これでいいのかな』とか、『もっと何かしないと』というお気持ちが、常に頭の中にあるんですね。休まらないというのは、本当におつらいですね。\n",
      "\u001b[32m10-25 21:10:24\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(529920,)\n",
      "\u001b[32m10-25 21:10:24\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、本当に。趣味の時間も楽しめなくなってしまって、以前は好きだったことも、今はただの義務みたいに感じてしまうんです。\n",
      "\u001b[32m10-25 21:10:25\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(414720,)\n",
      "\u001b[32m10-25 21:10:25\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうでしたか。趣味も楽しめないというのは、お気持ちがつらくなってしまいますよね。今日お話しいただいたことで、〇〇さんの抱えている不安の様子が少し見えてきたように思います。\n",
      "\u001b[32m10-25 21:10:25\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(569856,)\n",
      "\u001b[32m10-25 21:10:25\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "次回は、もう少し具体的に、どんな時にその『焦り』や『不安』を強く感じるのか、お話しいただけますか？そうすることで、もう少し具体的な対策を一緒に考えられるかもしれません。\n",
      "\u001b[32m10-25 21:10:25\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(589824,)\n",
      "\u001b[32m10-25 21:10:25\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、ありがとうございます。そうですね、少しずつなら話せる気がしてきました。よろしくお願いします。\n",
      "\u001b[32m10-25 21:10:25\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(352768,)\n",
      "\u001b[32m10-25 21:10:25\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ。今日はここまでですが、次回またお話しできるのを楽しみにしておりますね。お気をつけてお帰りください。\n",
      "\u001b[32m10-25 21:10:25\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(354304,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:37\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:04\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_16...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_16\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m147.306\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:35\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_16...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_16\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m146.233\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-25 21:16:02\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "〇〇さん、今日はセッションの振り返りをしてみませんか。終結に向けて、どんな変化があったか、お話しいただけると嬉しいです。\n",
      "\u001b[32m10-25 21:16:02\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(387072,)\n",
      "\u001b[32m10-25 21:16:02\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。最初は本当に漠然とした不安ばかりで、どうしたらいいか分からなかったです。\n",
      "\u001b[32m10-25 21:16:03\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(262144,)\n",
      "\u001b[32m10-25 21:16:03\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、漠然とした不安でしたね。そうでした。\n",
      "\u001b[32m10-25 21:16:03\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(154624,)\n",
      "\u001b[32m10-25 21:16:03\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ。でも、先生と話す中で、何に不安を感じているのか、具体的に見えてきた気がします。\n",
      "\u001b[32m10-25 21:16:03\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(290816,)\n",
      "\u001b[32m10-25 21:16:03\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど。認知や感情、行動の繋がりが整理できた、ということですね。\n",
      "\u001b[32m10-25 21:16:03\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(264192,)\n",
      "\u001b[32m10-25 21:16:03\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうです。『どうせうまくいかない』という考えが、感情や行動に繋がっていた。それに気づいて、少しずつ変えようと思えるようになりました。\n",
      "\u001b[32m10-25 21:16:03\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(432640,)\n",
      "\u001b[32m10-25 21:16:03\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、素晴らしい気づきですね。客観的に捉えられたのは大きな変化です。\n",
      "\u001b[32m10-25 21:16:04\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(253952,)\n",
      "\u001b[32m10-25 21:16:04\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。先週のディスカッションでも、前より落ち着いて意見を言えるようになったんです。一言でも言えたのが嬉しくて。\n",
      "\u001b[32m10-25 21:16:04\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(352256,)\n",
      "\u001b[32m10-25 21:16:04\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、一言でも言えたこと、素晴らしいですね。達成感はありましたか？\n",
      "\u001b[32m10-25 21:16:04\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(246784,)\n",
      "\u001b[32m10-25 21:16:04\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、ありました。失敗しても大丈夫だって思えたのが大きいです。\n",
      "\u001b[32m10-25 21:16:04\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(222208,)\n",
      "\u001b[32m10-25 21:16:04\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど、失敗しても大丈夫、と。悪循環を断ち切る糸口を見つけられた。今後も活かせそうですか。\n",
      "\u001b[32m10-25 21:16:04\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(356352,)\n",
      "\u001b[32m10-25 21:16:04\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。この経験を活かし、自分のペースで挑戦したいです。不安になっても、また整理すればいいって思えます。\n",
      "\u001b[32m10-25 21:16:05\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(364544,)\n",
      "\u001b[32m10-25 21:16:05\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、ご自身の力で不安と向き合う力がつきましたね。素晴らしいです。終結に向けて、あと数回で今後のサポートもお話ししましょうか。\n",
      "\u001b[32m10-25 21:16:05\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(455680,)\n",
      "\u001b[32m10-25 21:16:05\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、ありがとうございます。よろしくお願いします。\n",
      "\u001b[32m10-25 21:16:05\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(180224,)\n",
      "\u001b[32m10-25 21:16:05\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、承知いたしました。では、次回の予約ですが…\n",
      "\u001b[32m10-25 21:16:05\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(188928,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:21\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_17...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_17\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m130.001\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:19\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_17...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_17\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m129.526\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-25 21:21:38\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "Bさん、こんにちは。今日はどのようなお話しから始めましょうか？\n",
      "\u001b[32m10-25 21:21:38\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(206848,)\n",
      "\u001b[32m10-25 21:21:38\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。最近、常に落ち着かなくて…仕事もプライベートも、「こうすべき」「こうしなきゃ」って考えて、疲れてしまうんです。\n",
      "\u001b[32m10-25 21:21:38\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(400384,)\n",
      "\u001b[32m10-25 21:21:38\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "「〜すべき」「〜しなきゃ」というお気持ちが強いのですね。ええ、お疲れなんですね。\n",
      "\u001b[32m10-25 21:21:39\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(249856,)\n",
      "\u001b[32m10-25 21:21:39\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。会社で資料を作る時も、完璧じゃないとダメだと。少しの抜けでも「自分はダメだ」と落ち込むんです。周りはもっと要領よくこなしてるのに、って。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: JPCommonLabel_push_word() in jpcommon_label.c: First mora should not be long vowel symbol.\n",
      "WARNING: JPCommonLabel_insert_pause() in jpcommon_label.c: First mora should not be short pause.\n",
      "WARNING: JPCommonLabel_push_word() in jpcommon_label.c: First mora should not be long vowel symbol.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-25 21:21:39\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(512000,)\n",
      "\u001b[32m10-25 21:21:39\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "完璧でないとダメだと感じ、ご自身を責めるのですね。周りと比べてお辛い気持ち、よくわかります。\n",
      "\u001b[32m10-25 21:21:39\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(379392,)\n",
      "\u001b[32m10-25 21:21:39\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうなんです。休日も休みたいのに、「何か生産的なことをしなきゃ」って焦って。何もできないと自分にイライラしてしまいます。\n",
      "\u001b[32m10-25 21:21:39\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(400896,)\n",
      "\u001b[32m10-25 21:21:39\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、休日も「何か生産的なことをしなきゃ」という気持ちが強く、心が休まらない感覚でしょうか。\n",
      "\u001b[32m10-25 21:21:39\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(344064,)\n",
      "\u001b[32m10-25 21:21:39\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "まさに。休んでも休んだ気がしないというか…ずっと自分を縛り付けてしまうんじゃないかと不安なんです。\n",
      "\u001b[32m10-25 21:21:40\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(313344,)\n",
      "\u001b[32m10-25 21:21:40\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ご自身を縛り付けてしまう不安なお気持ちなのですね。この「〜すべき」「〜しなきゃ」という考え方が、Bさんの気持ちに影響しているのかもしれません。\n",
      "\u001b[32m10-25 21:21:40\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: JPCommonLabel_push_word() in jpcommon_label.c: First mora should not be long vowel symbol.\n",
      "WARNING: JPCommonLabel_push_word() in jpcommon_label.c: First mora should not be long vowel symbol.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(481280,)\n",
      "\u001b[32m10-25 21:21:40\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "今日は具体的にお話しくださり、ありがとうございました。次回は、この考え方について、どんな時に強く感じるのか、それがBさんの心にどんな影響を与えているのか、一緒に見ていけたらと思います。いかがですか？\n",
      "\u001b[32m10-25 21:21:40\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(723968,)\n",
      "\u001b[32m10-25 21:21:40\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、ありがとうございます。話せて少し楽になった気がします。ぜひ、そうさせてください。\n",
      "\u001b[32m10-25 21:21:40\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(310272,)\n",
      "\u001b[32m10-25 21:21:40\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "良かったです。では、次回の予約についてですが…\n",
      "\u001b[32m10-25 21:21:40\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(161280,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:24\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:04\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_18...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_18\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m135.148\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:17\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_18...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_18\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m129.274\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-25 21:26:39\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こんにちは。今日はよくいらっしゃいましたね。お話しできる範囲で構いませんので、今日はどんなことでしょうか？\n",
      "\u001b[32m10-25 21:26:39\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(339456,)\n",
      "\u001b[32m10-25 21:26:39\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、こんにちは。最近、仕事のこととか、なんだか漠然とした不安がずっとあって…。何から話したらいいのか、自分でもよくわからなくて。\n",
      "\u001b[32m10-25 21:26:39\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(476160,)\n",
      "\u001b[32m10-25 21:26:39\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうなんですね。漠然とした不安、ですか。はい、無理にまとめる必要はないですよ。少しずつお話を聞かせていただけますか？\n",
      "\u001b[32m10-25 21:26:40\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(428544,)\n",
      "\u001b[32m10-25 21:26:40\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ。なんだか、周りの人はみんなちゃんとやっているように見えるのに、自分だけ置いていかれているような気がして。仕事でも、小さなミスが気になって、ずっと引きずってしまうんです。\n",
      "\u001b[32m10-25 21:26:40\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(537600,)\n",
      "\u001b[32m10-25 21:26:40\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど、周りの方とご自分を比べてしまう、ということですね。小さなミスも気になってしまう、と。それは、とてもお辛いですね。\n",
      "\u001b[32m10-25 21:26:40\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(465920,)\n",
      "\u001b[32m10-25 21:26:40\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。夜も、なんだか寝つきが悪くなってしまって。頭の中が常にザワザワしているような感覚で…。\n",
      "\u001b[32m10-25 21:26:40\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(344576,)\n",
      "\u001b[32m10-25 21:26:40\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうなんですね。寝つきも悪くなっていらっしゃる、と。ええ、それはしんどいですね。そういったお気持ちは、いつ頃から感じることが増えましたか？\n",
      "\u001b[32m10-25 21:26:40\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(485888,)\n",
      "\u001b[32m10-25 21:26:40\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はっきりとは覚えてないんですが、半年くらい前からでしょうか。大きなプロジェクトが終わってから、なんだかずっと落ち着かなくて。\n",
      "\u001b[32m10-25 21:26:41\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(371200,)\n",
      "\u001b[32m10-25 21:26:41\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、なるほど。大きなプロジェクトがきっかけで、ずっと落ち着かない状態が続いているんですね。そうだったのですね。\n",
      "\u001b[32m10-25 21:26:41\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(380416,)\n",
      "\u001b[32m10-25 21:26:41\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。なんとか自分で解決しようと色々考えたんですけど、堂々巡りで、どうしたらいいか分からなくなってしまって。\n",
      "\u001b[32m10-25 21:26:41\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(349696,)\n",
      "\u001b[32m10-25 21:26:41\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうですよね、ご自分でなんとかしようと、たくさん努力されてきたんですね。それは、本当に素晴らしいことだと思いますよ。はい。\n",
      "\u001b[32m10-25 21:26:41\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(427520,)\n",
      "\u001b[32m10-25 21:26:41\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ありがとうございます。なんだか、少し話せただけでも、胸のつかえが取れたような気がします。\n",
      "\u001b[32m10-25 21:26:41\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(286720,)\n",
      "\u001b[32m10-25 21:26:42\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、そう言っていただけて良かったです。今日は、漠然とした不安や、仕事での焦りが日常生活にも影響している、ということや、そのきっかけなども少しお聞かせいただきましたね。\n",
      "\u001b[32m10-25 21:26:42\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(589824,)\n",
      "\u001b[32m10-25 21:26:42\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。そうです。\n",
      "\u001b[32m10-25 21:26:42\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(64512,)\n",
      "\u001b[32m10-25 21:26:42\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "今日はここまでにして、次回はもう少し、そういった漠然とした不安が、具体的にどのような場面で、どんな風に感じられるのか、詳しくお伺いしてもよろしいですか？お話してくださって、本当にありがとうございました。\n",
      "\u001b[32m10-25 21:26:42\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(732160,)\n",
      "\u001b[32m10-25 21:26:42\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、ぜひ。また次回もよろしくお願いします。\n",
      "\u001b[32m10-25 21:26:42\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(143872,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:31\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_19...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_19\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m140.255\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:31\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:04\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_19...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_19\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m141.163\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-25 21:32:19\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "最近、どうも気持ちが落ち着かなくて。仕事もプライベートも、もっときちんとこなさなきゃいけないのに、全然できてないなって感じがして。\n",
      "\u001b[32m10-25 21:32:20\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(407552,)\n",
      "\u001b[32m10-25 21:32:20\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうなんですね。気持ちが落ち着かない日々が続いているのですね。ええ。もっと「きちんとこなさなきゃ」というお気持ちなんですね。\n",
      "\u001b[32m10-25 21:32:20\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(392192,)\n",
      "\u001b[32m10-25 21:32:20\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。みんなはもっとテキパキやってるはずなのに、自分だけ取り残されてるような気がして。もっと頑張らないと、っていつも自分を追い込んじゃうんです。\n",
      "\u001b[32m10-25 21:32:20\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(431104,)\n",
      "\u001b[32m10-25 21:32:20\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど。周りの方と比べて「自分だけが」と感じていらっしゃるのですね。そして、「もっと頑張らないと」と、ご自身を追い込んでしまうと。ええ。\n",
      "\u001b[32m10-25 21:32:20\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(481280,)\n",
      "\u001b[32m10-25 21:32:20\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうなんです。例えば、休日にゆっくりしようと思っても、「これもやらなきゃ」「あれもやるべき」って、結局休んだ気がしないんです。\n",
      "\u001b[32m10-25 21:32:20\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(376832,)\n",
      "\u001b[32m10-25 21:32:20\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、そうなんですね。休日でも「〜すべき」というお気持ちが頭から離れないのですね。その気持ちはBさんにとって、どんな風に感じられますか？\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: JPCommonLabel_push_word() in jpcommon_label.c: First mora should not be long vowel symbol.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-25 21:32:21\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(473088,)\n",
      "\u001b[32m10-25 21:32:21\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "うーん…なんか、背中を押されるような、でもちょっと重い、みたいな。やらなきゃいけないって分かってるから、やらないとダメだって。\n",
      "\u001b[32m10-25 21:32:21\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(389632,)\n",
      "\u001b[32m10-25 21:32:21\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、はい。背中を押されるような気持ちと、少し重いお気持ちですね。もし、少しだけ「やらなくてもいい」と許してあげるとしたら、どう感じられるでしょう？\n",
      "\u001b[32m10-25 21:32:21\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(489472,)\n",
      "\u001b[32m10-25 21:32:21\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "でも、やらなかったら、もっと状況が悪くなるんじゃないかって、不安で。\n",
      "\u001b[32m10-25 21:32:21\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(223232,)\n",
      "\u001b[32m10-25 21:32:21\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうですね、不安に感じてしまうのですね。その「やらなければ」という気持ちは、Bさんを守る大切な気持ちでもありますね。少しだけ「本当にそうかな？」と問いかけてみるのも、大切かもしれません。\n",
      "\u001b[32m10-25 21:32:21\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(680448,)\n",
      "\u001b[32m10-25 21:32:21\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "「本当にそうかな」…ですか。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: JPCommonLabel_insert_pause() in jpcommon_label.c: First mora should not be short pause.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-25 21:32:22\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(109056,)\n",
      "\u001b[32m10-25 21:32:22\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ。今日は、Bさんの「〜すべき」というお気持ちについて、お話ができましたね。次回は、それがどんな場面で出てくるのか、もう少し詳しくお伺いしてもよろしいでしょうか。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: JPCommonLabel_push_word() in jpcommon_label.c: First mora should not be long vowel symbol.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-25 21:32:22\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(542208,)\n",
      "\u001b[32m10-25 21:32:22\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、わかりました。少し考えてみます。\n",
      "\u001b[32m10-25 21:32:22\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(164864,)\n",
      "\u001b[32m10-25 21:32:22\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ありがとうございます。今日はこのあたりで。また来週、同じ時間にお待ちしておりますね。\n",
      "\u001b[32m10-25 21:32:22\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(286720,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:45\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_20...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_20\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m154.812\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:28\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:04\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_20...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_20\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m139.489\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-25 21:38:21\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こんにちは、Bさん。今日は、これまでのセッションを振り返りましょうか。終結に向けての時期ですからね。\n",
      "\u001b[32m10-25 21:38:22\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(300544,)\n",
      "\u001b[32m10-25 21:38:22\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、そうですね。寂しさも少し、でもスッキリした部分もあって。\n",
      "\u001b[32m10-25 21:38:22\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(218624,)\n",
      "\u001b[32m10-25 21:38:22\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうなんですね。寂しさも感じますか。スッキリしたというのは大きな変化。何か「変わったな」と感じることは？\n",
      "\u001b[32m10-25 21:38:22\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(369152,)\n",
      "\u001b[32m10-25 21:38:22\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "うーん、以前は漠然とした不安が常にあったんですけど…。\n",
      "\u001b[32m10-25 21:38:22\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(172544,)\n",
      "\u001b[32m10-25 21:38:22\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。\n",
      "\u001b[32m10-25 21:38:22\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(20992,)\n",
      "\u001b[32m10-25 21:38:22\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "最近は、「あ、あの不安と同じだ」って気づけて。立ち止まって考えられるようになりました。\n",
      "\u001b[32m10-25 21:38:23\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(306688,)\n",
      "\u001b[32m10-25 21:38:23\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど。漠然としたものが、形あるものとして捉えられるように。思考に意識的になれたと。\n",
      "\u001b[32m10-25 21:38:23\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(327168,)\n",
      "\u001b[32m10-25 21:38:23\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、まさに。客観的に見られるようになったかなって。\n",
      "\u001b[32m10-25 21:38:23\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(173568,)\n",
      "\u001b[32m10-25 21:38:23\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ。それは大きな一歩です。ご自身と丁寧に向き合ってきた証拠ですよ。\n",
      "\u001b[32m10-25 21:38:23\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(250880,)\n",
      "\u001b[32m10-25 21:38:23\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ありがとうございます。先生と話して、頭の中が整理できました。\n",
      "\u001b[32m10-25 21:38:24\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(209920,)\n",
      "\u001b[32m10-25 21:38:24\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こちらこそ、Bさんが向き合ってくださったからですよ。気づきを得て、次の一歩を踏み出せたんです。\n",
      "\u001b[32m10-25 21:38:24\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(315904,)\n",
      "\u001b[32m10-25 21:38:24\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そう言っていただけると、嬉しいです。\n",
      "\u001b[32m10-25 21:38:24\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(105984,)\n",
      "\u001b[32m10-25 21:38:24\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。では、今日はここまでと。終結に向けてもう少しお話しできればと思います。次回の日程ですが…。\n",
      "\u001b[32m10-25 21:38:24\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(350720,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:16\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_21...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_21\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m127.368\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:11\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:04\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_21...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_21\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m121.136\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h\u001b[32m10-25 21:43:18\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "〇〇さん、こんにちは。今日はこれまでのセッションを振り返りながら、終結に向けて考えていけたらと思うのですが、いかがでしょうか。\n",
      "\u001b[32m10-25 21:43:19\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(400896,)\n",
      "\u001b[32m10-25 21:43:19\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、そうですね。もうそんな時期なんですね。\n",
      "\u001b[32m10-25 21:43:19\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(164864,)\n",
      "\u001b[32m10-25 21:43:19\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ。ここまで色々取り組まれましたが、何か変化を感じることはありますか？\n",
      "\u001b[32m10-25 21:43:19\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(247296,)\n",
      "\u001b[32m10-25 21:43:19\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "漠然とした不安が減って、何に困っていたのか具体的にわかるようになりました。\n",
      "\u001b[32m10-25 21:43:19\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(241152,)\n",
      "\u001b[32m10-25 21:43:19\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど。明確になったことで、気持ちの変化はありましたか？\n",
      "\u001b[32m10-25 21:43:20\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(200192,)\n",
      "\u001b[32m10-25 21:43:20\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。仕事のミスで「自分はダメだ」と引きずっていたのが、「まあ、仕方ないか」と受け流せるようになりました。\n",
      "\u001b[32m10-25 21:43:20\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(320000,)\n",
      "\u001b[32m10-25 21:43:20\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、それは素晴らしい変化ですね。ご自身の思考パターンに気づき、対応できるようになったのですね。\n",
      "\u001b[32m10-25 21:43:20\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(326144,)\n",
      "\u001b[32m10-25 21:43:20\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。気持ちも楽になりました。焦りも減り、不安ともうまく付き合えている気がします。\n",
      "\u001b[32m10-25 21:43:21\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(304128,)\n",
      "\u001b[32m10-25 21:43:21\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうなんですね。不安がゼロでなくても、その付き合い方が変わった。ご自身で向き合う力を培われた証だと感じます。\n",
      "\u001b[32m10-25 21:43:21\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(409088,)\n",
      "\u001b[32m10-25 21:43:21\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ありがとうございます。先生のおかげで、客観的に見られるようになりました。この経験を大切にしたいです。\n",
      "\u001b[32m10-25 21:43:21\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(343552,)\n",
      "\u001b[32m10-25 21:43:21\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、私がお手伝いできて良かったです。それでは、次回を最終回として、学びをまとめてみるのはいかがでしょうか。\n",
      "\u001b[32m10-25 21:43:21\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(361984,)\n",
      "\u001b[32m10-25 21:43:21\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、そうさせてください。\n",
      "\u001b[32m10-25 21:43:21\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(90624,)\n",
      "\u001b[32m10-25 21:43:21\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "承知いたしました。では、次回の〇月〇日にお待ちしておりますね。\n",
      "\u001b[32m10-25 21:43:22\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(209920,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:17\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_22...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_22\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m127.781\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:13\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_22...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_22\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m123.636\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-25 21:48:14\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "Bさん、今日はこれまでのセッションを振り返りながら、今後のことについてお話しできたらと思うのですが、いかがでしょうか。\n",
      "\u001b[32m10-25 21:48:14\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(371200,)\n",
      "\u001b[32m10-25 21:48:14\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、お願いします。あっという間でしたね。最初は漠然と不安で、何から話していいか分からなかったんですけど。\n",
      "\u001b[32m10-25 21:48:15\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(375808,)\n",
      "\u001b[32m10-25 21:48:15\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうでしたね。ええ。そこからここまで、Bさんご自身、何か変化を感じることはありますか？\n",
      "\u001b[32m10-25 21:48:15\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(301056,)\n",
      "\u001b[32m10-25 21:48:15\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。前は些細なことでクヨクヨしていましたが、最近は「なんとかなる」と思えることが増えました。気持ちが楽になったんです。\n",
      "\u001b[32m10-25 21:48:15\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(401408,)\n",
      "\u001b[32m10-25 21:48:15\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど、そうなんですね。それは大きな変化ですね。ご自身の力でそこまで来られたのは本当に素晴らしいことだと思います。さて、これまでのBさんの頑張りを見て、そろそろ私とのセッションも終わりに近づいているのかなと感じているのですが、いかがでしょう？\n",
      "\u001b[32m10-25 21:48:15\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(859648,)\n",
      "\u001b[32m10-25 21:48:15\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい…正直、ちょっと寂しい気持ちもあるんですが、でも、もう一人でも大丈夫かなって思えるようになりました。\n",
      "\u001b[32m10-25 21:48:16\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(347136,)\n",
      "\u001b[32m10-25 21:48:16\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうおっしゃっていただけて、私も嬉しいです。はい、そのお気持ち、よくわかりますよ。次回はこれまでの学びをどう活かすか計画し、その次の回で一旦区切りとさせていただくのはいかがでしょうか。\n",
      "\u001b[32m10-25 21:48:16\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(616960,)\n",
      "\u001b[32m10-25 21:48:16\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、お願いします！なんだか、心強いです。\n",
      "\u001b[32m10-25 21:48:16\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(179712,)\n",
      "\u001b[32m10-25 21:48:16\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ。では、次回の予約ですが…\n",
      "\u001b[32m10-25 21:48:16\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(123392,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:16\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:06\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_23...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_23\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m127.926\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:15\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_23...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_23\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m126.900\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-25 21:53:09\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "〇〇さん、今日はよくお越しくださいましたね。どうぞ楽にしてください。今日はどんなことをお話ししたい気分ですか？\n",
      "\u001b[32m10-25 21:53:09\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(351232,)\n",
      "\u001b[32m10-25 21:53:09\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ありがとうございます。なんだか最近、仕事もプライベートも、全部がうまくいってない気がして...漠然とした焦りばかり感じてしまうんです。\n",
      "\u001b[32m10-25 21:53:10\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(444416,)\n",
      "\u001b[32m10-25 21:53:10\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、そうなんですね。漠然とした焦りを感じていらっしゃるんですね。具体的にどんな時に、そう感じることが多いですか？\n",
      "\u001b[32m10-25 21:53:10\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(408576,)\n",
      "\u001b[32m10-25 21:53:10\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。仕事では、周りの人がどんどん成果を出しているのを見ると、自分だけ取り残されているような気がして。家に帰っても、あれもこれもやらなきゃって気持ちばかりで、結局何も手につかない、みたいな日々で...\n",
      "\u001b[32m10-25 21:53:10\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(687616,)\n",
      "\u001b[32m10-25 21:53:10\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど。そうでしたか。仕事で周りの方と比べてしまったり、お家に帰ってもやらなきゃいけないことに圧倒されてしまったり…、色々なことが重なって、疲れていらっしゃるのかもしれませんね。\n",
      "\u001b[32m10-25 21:53:11\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(617472,)\n",
      "\u001b[32m10-25 21:53:11\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうなんです。本当に、ずっと肩に力が入っている感じで、休まらないんです。\n",
      "\u001b[32m10-25 21:53:11\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(252928,)\n",
      "\u001b[32m10-25 21:53:11\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、とてもよく分かりますよ。この「休まらない」という感覚は、いつ頃から特に強く感じるようになりましたか？何かきっかけのようなものはありましたか？\n",
      "\u001b[32m10-25 21:53:11\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(515072,)\n",
      "\u001b[32m10-25 21:53:11\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "うーん、はっきりとは分からないんですけど、この半年前くらいからでしょうか。新しいプロジェクトが始まって、責任が大きくなったあたりから、ずっとこんな調子な気がします。\n",
      "\u001b[32m10-25 21:53:11\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(492032,)\n",
      "\u001b[32m10-25 21:53:11\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど、新しいプロジェクトがきっかけの一つになったのかもしれないですね。責任が大きくなるというのは、やはりプレッシャーも増えますからね。お話を伺っていると、〇〇さんはとても真面目に、そして一生懸命に頑張っていらっしゃるのが伝わってきますよ。\n",
      "\u001b[32m10-25 21:53:12\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(790016,)\n",
      "\u001b[32m10-25 21:53:12\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "...ありがとうございます。そう言っていただけると、少しだけ、ほっとします。\n",
      "\u001b[32m10-25 21:53:12\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(232960,)\n",
      "\u001b[32m10-25 21:53:12\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "いえいえ。今日は、この漠然とした焦りや、休まらない感覚について、少しお話しいただけて本当に良かったです。まずは、〇〇さんが今どんな状況で、どんなことを感じていらっしゃるのか、私に教えてくださってありがとうございます。次回は、この「焦り」や「休まらない感覚」が日常生活で具体的にどのように現れているか、もう少し詳しくお伺いしてもよろしいでしょうか？\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: JPCommonLabel_insert_pause() in jpcommon_label.c: First mora should not be short pause.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-25 21:53:12\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(1225216,)\n",
      "\u001b[32m10-25 21:53:12\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、ぜひ。今日は少し気持ちが楽になりました。\n",
      "\u001b[32m10-25 21:53:13\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(161280,)\n",
      "\u001b[32m10-25 21:53:13\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "良かったです。では、次回の予約ですが、来週の同じ曜日、時間はいかがでしょうか？それまでに、もし何か感じたことや、メモしておきたいことがあれば、気軽に書き留めておいてくださいね。\n",
      "\u001b[32m10-25 21:53:13\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(602624,)\n",
      "\u001b[32m10-25 21:53:13\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、分かりました。来週もよろしくお願いします。\n",
      "\u001b[32m10-25 21:53:13\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(183808,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:32\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_24...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_24\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m145.626\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:41\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_24...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_24\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m153.116\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-25 21:59:05\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こんにちは、Bさん。今日はどんな感じでお越しになりましたか？\n",
      "\u001b[32m10-25 21:59:05\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(176640,)\n",
      "\u001b[32m10-25 21:59:05\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こんにちは。最近、「こうすべき」という考えに縛られて、少し息苦しいんです。\n",
      "\u001b[32m10-25 21:59:05\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(242688,)\n",
      "\u001b[32m10-25 21:59:05\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "「こうすべき」ですか。なるほど。どんな時にそう感じますか？\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: JPCommonLabel_insert_pause() in jpcommon_label.c: First mora should not be short pause.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-25 21:59:06\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(203776,)\n",
      "\u001b[32m10-25 21:59:06\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "仕事で、もっと完璧にしなきゃと。ミスがあると、自分はダメだって落ち込んでしまうんです。\n",
      "\u001b[32m10-25 21:59:06\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(289792,)\n",
      "\u001b[32m10-25 21:59:06\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうなんですね。完璧でなければ、と感じてしまう。そのお気持ち、よく分かります。\n",
      "\u001b[32m10-25 21:59:06\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(301056,)\n",
      "\u001b[32m10-25 21:59:06\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。周りと比べて、自分だけができていないような焦りも感じてしまって。\n",
      "\u001b[32m10-25 21:59:06\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(251904,)\n",
      "\u001b[32m10-25 21:59:06\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "周りと比べて焦るお気持ち。この「〜すべき」という考えが、Bさんを苦しめているのかもしれませんね。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: JPCommonLabel_push_word() in jpcommon_label.c: First mora should not be long vowel symbol.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-25 21:59:07\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(345088,)\n",
      "\u001b[32m10-25 21:59:07\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうかもしれません。少し疲れてしまって…。\n",
      "\u001b[32m10-25 21:59:07\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(147968,)\n",
      "\u001b[32m10-25 21:59:07\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうですよね。今日は、この「〜すべき」という考えが、Bさんの心にどんな影響を与えているのか、お話しいただけてありがたかったです。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: JPCommonLabel_push_word() in jpcommon_label.c: First mora should not be long vowel symbol.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-25 21:59:07\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(427008,)\n",
      "\u001b[32m10-25 21:59:07\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こちらこそ、話せて少し楽になりました。\n",
      "\u001b[32m10-25 21:59:07\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(142848,)\n",
      "\u001b[32m10-25 21:59:07\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "よかったです。次回は、この「〜すべき」という考えについて、もう少しゆっくり見ていきましょうか。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: JPCommonLabel_push_word() in jpcommon_label.c: First mora should not be long vowel symbol.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-25 21:59:08\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(304640,)\n",
      "\u001b[32m10-25 21:59:08\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、お願いします。\n",
      "\u001b[32m10-25 21:59:08\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(78848,)\n",
      "\u001b[32m10-25 21:59:08\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "承知しました。では、今日はここまでに。次回は来週の同じ時間でよろしいでしょうか？\n",
      "\u001b[32m10-25 21:59:08\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(273920,)\n",
      "\u001b[32m10-25 21:59:08\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、大丈夫です。\n",
      "\u001b[32m10-25 21:59:08\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(74752,)\n",
      "\u001b[32m10-25 21:59:08\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ありがとうございます。お気をつけてお帰りくださいね。\n",
      "\u001b[32m10-25 21:59:09\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(156672,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:15\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_25...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_25\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m125.973\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:12\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_25...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_25\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m127.046\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-25 22:04:14\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こんにちは。Bさん、今日は前回少し触れた「〜すべき」というお気持ちについて、お話しできたらと思いますが、いかがでしょうか？\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: JPCommonLabel_push_word() in jpcommon_label.c: First mora should not be long vowel symbol.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-25 22:04:14\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(402432,)\n",
      "\u001b[32m10-25 22:04:14\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、最近「完璧にすべきだ」という気持ちが強くて、なんだか疲れてしまうんです。\n",
      "\u001b[32m10-25 22:04:14\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(262656,)\n",
      "\u001b[32m10-25 22:04:14\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど。「完璧にすべきだ」と。ええ、どんな時にそう感じますか？\n",
      "\u001b[32m10-25 22:04:15\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(248832,)\n",
      "\u001b[32m10-25 22:04:15\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "仕事でミスなく効率的に、とか。家でも「もっと綺麗にすべき」って考えてしまって。\n",
      "\u001b[32m10-25 22:04:15\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(274944,)\n",
      "\u001b[32m10-25 22:04:15\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、そうなんですね。それがBさんをどんな気持ちにさせますか？\n",
      "\u001b[32m10-25 22:04:15\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(215040,)\n",
      "\u001b[32m10-25 22:04:15\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "いつも焦りを感じて、気が休まりません。達成感も続かなくて。\n",
      "\u001b[32m10-25 22:04:16\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(233984,)\n",
      "\u001b[32m10-25 22:04:16\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうなんですね。もしその「〜すべき」が和らぐとしたら、どう感じられそうでしょうか？\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: JPCommonLabel_push_word() in jpcommon_label.c: First mora should not be long vowel symbol.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-25 22:04:16\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(256512,)\n",
      "\u001b[32m10-25 22:04:16\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "楽になれる気はします。でも、それが許されるのか…と。\n",
      "\u001b[32m10-25 22:04:16\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(186368,)\n",
      "\u001b[32m10-25 22:04:16\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、そのお気持ちも分かります。その「〜すべき」という考えが、Bさんを苦しめているのかもしれません。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: JPCommonLabel_push_word() in jpcommon_label.c: First mora should not be long vowel symbol.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-25 22:04:16\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(347136,)\n",
      "\u001b[32m10-25 22:04:16\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "苦しめている、ですか。当たり前だと思っていました。\n",
      "\u001b[32m10-25 22:04:16\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(164864,)\n",
      "\u001b[32m10-25 22:04:16\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうですよね。でも、その考えを少し緩めることで、心はもっと自由になれるかもしれません。今日はその可能性に触れられましたね。\n",
      "\u001b[32m10-25 22:04:17\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(453120,)\n",
      "\u001b[32m10-25 22:04:17\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。新しい見方だなと思いました。\n",
      "\u001b[32m10-25 22:04:17\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(135680,)\n",
      "\u001b[32m10-25 22:04:17\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、ありがとうございます。今日の気づきを心に留めて、次回お聞かせください。今日はここまでとしましょう。\n",
      "\u001b[32m10-25 22:04:17\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(358912,)\n",
      "\u001b[32m10-25 22:04:17\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、わかりました。ありがとうございます。\n",
      "\u001b[32m10-25 22:04:17\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(162816,)\n",
      "\u001b[32m10-25 22:04:17\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こちらこそ。次回は〇月〇日でいかがですか？\n",
      "\u001b[32m10-25 22:04:18\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(137728,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:17\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:04\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_26...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_26\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m133.167\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:16\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_26...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_26\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m133.504\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-25 22:09:30\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こんにちは。今日は、お話しにお越しいただきありがとうございます。どのようなことでお困りでしょうか？\n",
      "\u001b[32m10-25 22:09:31\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(303616,)\n",
      "\u001b[32m10-25 22:09:31\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、こんにちは。最近、漠然とした不安や焦りを感じていて…。仕事も日常生活も、どうもすっきりしないんです。\n",
      "\u001b[32m10-25 22:09:31\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(399360,)\n",
      "\u001b[32m10-25 22:09:31\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうなんですね。漠然とした不安や焦りを感じていらっしゃる。ええ、ありがとうございます。具体的に、どんな時にそう感じることが多いですか？\n",
      "\u001b[32m10-25 22:09:31\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(461312,)\n",
      "\u001b[32m10-25 22:09:31\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "うーん、仕事ではタスクに追われている感覚で。プライベートでも、休日にしっかり休めている感じがしなくて、常に何かに追われているような気持ちで…。\n",
      "\u001b[32m10-25 22:09:31\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(473600,)\n",
      "\u001b[32m10-25 22:09:32\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど。お仕事のプレッシャーと休息不足、両方で気持ちが休まらない状態なんですね。はい、それはお辛いですね。\n",
      "\u001b[32m10-25 22:09:32\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(428544,)\n",
      "\u001b[32m10-25 22:09:32\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、そうです。もう、どうしたらいいのか分からなくて。このままじゃいけない、と思いつつ、何も変えられない自分がいます。\n",
      "\u001b[32m10-25 22:09:32\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(389632,)\n",
      "\u001b[32m10-25 22:09:32\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、そうなんですね。このままじゃいけない、というお気持ちと、何も変えられないと感じるご自身に、もどかしさを感じていらっしゃるのですね。\n",
      "\u001b[32m10-25 22:09:32\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(464896,)\n",
      "\u001b[32m10-25 22:09:32\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。まさにそんな感じです。\n",
      "\u001b[32m10-25 22:09:32\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(107008,)\n",
      "\u001b[32m10-25 22:09:32\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ありがとうございます。今日は、漠然とした不安や焦りが、お仕事や日々の生活に影響していることをお伺いできました。大切な一歩ですよ。\n",
      "\u001b[32m10-25 22:09:33\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(472576,)\n",
      "\u001b[32m10-25 22:09:33\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうでしょうか…。\n",
      "\u001b[32m10-25 22:09:33\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(47104,)\n",
      "\u001b[32m10-25 22:09:33\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、もちろんです。ご自身の状態に気づき、こうして相談に来てくださったこと自体が、変化のきっかけになりますからね。次回はもう少し、どんな時にそうしたお気持ちになるのか、詳しくお伺いできたらと思いますが、いかがでしょうか？\n",
      "\u001b[32m10-25 22:09:33\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(759808,)\n",
      "\u001b[32m10-25 22:09:33\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、お願いします。少し話して、気持ちが少し軽くなった気がします。\n",
      "\u001b[32m10-25 22:09:34\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(250880,)\n",
      "\u001b[32m10-25 22:09:34\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そう言っていただけて嬉しいです。次回もゆっくりお話し伺わせていただきますね。今日はありがとうございました。\n",
      "\u001b[32m10-25 22:09:34\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(340992,)\n",
      "\u001b[32m10-25 22:09:34\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ありがとうございました。\n",
      "\u001b[32m10-25 22:09:34\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(61440,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:25\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:06\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_27...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_27\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m141.853\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:23\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_27...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_27\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m141.103\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-25 22:14:48\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "〇〇さん、今日はどうぞよろしくお願いいたします。今日はどのようなことをお話ししたいと思われますか？\n",
      "\u001b[32m10-25 22:14:48\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(302080,)\n",
      "\u001b[32m10-25 22:14:48\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、よろしくお願いいたします。えっと、最近、仕事でちょっと行き詰まっている感じで、それだけじゃなくて、なんだか普段の生活でも漠然とした不安とか焦りを感じていて…何から話せばいいのか、という状況なんです。\n",
      "\u001b[32m10-25 22:14:49\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(690688,)\n",
      "\u001b[32m10-25 22:14:49\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうなんですね。仕事のことだけでなく、日常生活でも不安や焦りを感じていらっしゃる、ということですね。ええ、まずは今感じていることを、〇〇さんのペースでゆっくりお聞かせいただけますか？\n",
      "\u001b[32m10-25 22:14:49\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(601600,)\n",
      "\u001b[32m10-25 22:14:49\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。仕事では、新しいプロジェクトを任されて、プレッシャーを感じているのは確かなんですけど、家に帰ってからも、やらなきゃいけないことばかり考えてしまって、全然リラックスできないんです。休日に趣味の時間を持っても、心から楽しめなくて…何か、ずっとソワソワしているような。\n",
      "\u001b[32m10-25 22:14:49\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(894464,)\n",
      "\u001b[32m10-25 22:14:49\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど、お仕事でのプレッシャーが、休日のリラックスタイムにも影響してしまっているのですね。はい、それはお辛いでしょう。漠然としたソワソワ感、ということでしたが、それはいつ頃から感じられるようになりましたか？\n",
      "\u001b[32m10-25 22:14:50\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(726016,)\n",
      "\u001b[32m10-25 22:14:50\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "具体的なきっかけは分からないんですけど、半年くらい前からでしょうか。仕事が忙しくなってきて、だんだんこう、気持ちが落ち着かなくなっていった気がします。朝起きる時も、「また今日が始まるのか」って憂鬱な気分で…\n",
      "\u001b[32m10-25 22:14:50\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(679936,)\n",
      "\u001b[32m10-25 22:14:50\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、そうでしたか。半年前くらいから、お仕事の忙しさとともに、その憂鬱な気持ちやソワソワ感が強まってきた、ということですね。はい。そうなんですね。漠然とした不安や焦り、そして朝の憂鬱感…まずは、今日それらを打ち明けてくださって、ありがとうございます。\n",
      "\u001b[32m10-25 22:14:50\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(945152,)\n",
      "\u001b[32m10-25 22:14:50\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "いえ、話せて少し楽になった気がします。\n",
      "\u001b[32m10-25 22:14:51\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(131072,)\n",
      "\u001b[32m10-25 22:14:51\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "それはよかったです。今日お話しいただいたことを踏まえて、次回はもう少し、〇〇さんがどのような時に、その不安や焦りを強く感じるのか、具体的に掘り下げていければと思いますがいかがでしょうか？\n",
      "\u001b[32m10-25 22:14:51\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(600064,)\n",
      "\u001b[32m10-25 22:14:51\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、ぜひお願いします。今日はありがとうございました。\n",
      "\u001b[32m10-25 22:14:51\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(182784,)\n",
      "\u001b[32m10-25 22:14:51\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こちらこそ、今日は貴重なお時間をありがとうございました。次回もどうぞよろしくお願いいたします。\n",
      "\u001b[32m10-25 22:14:51\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(310784,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:29\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_28...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_28\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m149.807\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:29\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_28...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_28\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m152.620\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-25 22:20:27\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こんにちは、Bさん。先週お話しいただいた、仕事でのプレッシャーについて、今日はもう少し詳しくお聞かせいただけますか？\n",
      "\u001b[32m10-25 22:20:27\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(383488,)\n",
      "\u001b[32m10-25 22:20:27\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、先生。最近、何をするにも『こうすべきだ』って考えてしまって、すごく息苦しいんです。\n",
      "\u001b[32m10-25 22:20:27\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(276480,)\n",
      "\u001b[32m10-25 22:20:27\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ああ、『こうすべきだ』、ですか。なるほど、そうなんですね。具体的には、どのような時にそう感じることが多いですか？\n",
      "\u001b[32m10-25 22:20:28\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(394240,)\n",
      "\u001b[32m10-25 22:20:28\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "仕事でミスをしてはいけない、とか、常に完璧でなければならない、とか…家でも、もっと家事をきちんとすべきだ、って。\n",
      "\u001b[32m10-25 22:20:28\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(369152,)\n",
      "\u001b[32m10-25 22:20:28\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、そうなんですね。仕事でもご家庭でも、『〜すべきだ』と感じていらっしゃるのですね。その『完璧でなければならない』という考えは、Bさんにとってどのような影響を与えていますか？\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: JPCommonLabel_push_word() in jpcommon_label.c: First mora should not be long vowel symbol.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-25 22:20:28\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(573440,)\n",
      "\u001b[32m10-25 22:20:28\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "いつも追い詰められているような気持ちになって、疲れてしまいます。少しでもできないと、自分はダメだって…\n",
      "\u001b[32m10-25 22:20:28\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(332800,)\n",
      "\u001b[32m10-25 22:20:28\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、お辛いですね。そうですね、常に完璧を目指すというのは、本当に大変なことですよね。もし、その『完璧でなければならない』という考えが、少しだけ緩んだとしたら、Bさんはどう感じられるでしょうか？\n",
      "\u001b[32m10-25 22:20:29\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(705024,)\n",
      "\u001b[32m10-25 22:20:29\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "え…？緩む、ですか。でも、緩めたらもっとダメになってしまうんじゃないかと…\n",
      "\u001b[32m10-25 22:20:29\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(246784,)\n",
      "\u001b[32m10-25 22:20:29\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど、そう感じられるのですね。それは、もしかしたら、その『すべき』という考え方が、Bさんを苦しめているのかもしれませんね。今日は、この『〜すべき』という考えについて、少し掘り下げてお話ができたように思います。\n",
      "\u001b[32m10-25 22:20:29\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(711168,)\n",
      "\u001b[32m10-25 22:20:29\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、少しだけ、そうかもしれません…\n",
      "\u001b[32m10-25 22:20:29\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(141312,)\n",
      "\u001b[32m10-25 22:20:29\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ。次回は、この『〜すべき』という考えが、本当にBさんにとって必要なものなのかどうか、一緒に考えていけたら嬉しいです。何か、今の気持ちで、心に残ったことはありますか？\n",
      "\u001b[32m10-25 22:20:30\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(607744,)\n",
      "\u001b[32m10-25 22:20:30\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい…少し、この考え方を見直すきっかけになった気がします。\n",
      "\u001b[32m10-25 22:20:30\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(201728,)\n",
      "\u001b[32m10-25 22:20:30\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そう言っていただけて良かったです。では、今日のところはここまでとしましょう。次回、またお話ししましょうね。\n",
      "\u001b[32m10-25 22:20:30\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(340480,)\n",
      "\u001b[32m10-25 22:20:30\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ありがとうございました。\n",
      "\u001b[32m10-25 22:20:30\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(61952,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h\u001b[?25l"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:24\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_29...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_29\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m145.305\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:27\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:06\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_29...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_29\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m151.583\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-25 22:26:16\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こんにちは、〇〇さん。今日はようこそお越しくださいました。ゆっくりお話しくださいね。\n",
      "\u001b[32m10-25 22:26:16\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(251904,)\n",
      "\u001b[32m10-25 22:26:16\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こんにちは。はい、ありがとうございます。なんだか最近、ずっと落ち着かなくて…。\n",
      "\u001b[32m10-25 22:26:17\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(289280,)\n",
      "\u001b[32m10-25 22:26:17\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうなんですね。落ち着かない、どんな時にそう感じることが多いですか？\n",
      "\u001b[32m10-25 22:26:17\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(228864,)\n",
      "\u001b[32m10-25 22:26:17\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ。仕事中もですが、家に帰ってからも漠然とした不安や焦りがずっとあって…。\n",
      "\u001b[32m10-25 22:26:17\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(278016,)\n",
      "\u001b[32m10-25 22:26:17\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど、仕事だけでなくご自宅でもですか。お辛いですね。\n",
      "\u001b[32m10-25 22:26:18\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(220672,)\n",
      "\u001b[32m10-25 22:26:18\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、仕事のプレッシャーが続いて、それが日常全体に広がった感じです。周りの期待に応えられてないのでは、と思うと、余計に。\n",
      "\u001b[32m10-25 22:26:18\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(448512,)\n",
      "\u001b[32m10-25 22:26:18\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、周りの期待ですね。しんどい感覚ですよね。頑張っていらっしゃるのに。睡眠や食欲など、何か影響は？\n",
      "\u001b[32m10-25 22:26:18\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(424960,)\n",
      "\u001b[32m10-25 22:26:18\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうなんです。夜も寝付けなくて、朝も億劫で。ご飯も味があまり分からなくなってしまって…。\n",
      "\u001b[32m10-25 22:26:18\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(304640,)\n",
      "\u001b[32m10-25 22:26:18\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、心配になりますよね。睡眠や食欲にも影響が出ているのですね。\n",
      "\u001b[32m10-25 22:26:18\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(257024,)\n",
      "\u001b[32m10-25 22:26:18\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なんとか自分で乗り越えようと頑張ってきましたが、もう限界かな、と。\n",
      "\u001b[32m10-25 22:26:19\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(210432,)\n",
      "\u001b[32m10-25 22:26:19\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、そうだったんですね。今日、こうしてお話ししに来てくださったこと、素晴らしい第一歩だと思います。〇〇さんの今のお気持ち、少し理解できたように思います。\n",
      "\u001b[32m10-25 22:26:19\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(524800,)\n",
      "\u001b[32m10-25 22:26:19\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、少し話せて、落ち着きました。\n",
      "\u001b[32m10-25 22:26:19\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(128000,)\n",
      "\u001b[32m10-25 22:26:19\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "良かったです。本日はこれで一旦おしまいにしましょうか。次回、もう少し詳しく伺えますか？\n",
      "\u001b[32m10-25 22:26:19\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(299520,)\n",
      "\u001b[32m10-25 22:26:19\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、お願いします。\n",
      "\u001b[32m10-25 22:26:19\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(78848,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:19\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_30...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_30\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m141.757\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:15\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:06\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_30...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_30\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m143.268\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-25 22:32:04\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こんにちは、Bさん。今日はどんなことをお話ししたい気分ですか？\n",
      "\u001b[32m10-25 22:32:04\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(187904,)\n",
      "\u001b[32m10-25 22:32:04\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こんにちは、先生。最近、「こうしなきゃいけない」って思うことが多くて、すごく負担なんです。\n",
      "\u001b[32m10-25 22:32:05\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(280064,)\n",
      "\u001b[32m10-25 22:32:05\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "「こうしなきゃいけない」ですか。ええ、どんな時にそう感じますか？\n",
      "\u001b[32m10-25 22:32:05\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(205312,)\n",
      "\u001b[32m10-25 22:32:05\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "仕事でも家でも、常に「〜すべき」と考えてしまって…。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: JPCommonLabel_insert_pause() in jpcommon_label.c: First mora should not be short pause.\n",
      "WARNING: JPCommonLabel_push_word() in jpcommon_label.c: First mora should not be long vowel symbol.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-25 22:32:05\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(174080,)\n",
      "\u001b[32m10-25 22:32:05\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、どんな風に感じますか？\n",
      "\u001b[32m10-25 22:32:05\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(107520,)\n",
      "\u001b[32m10-25 22:32:05\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "焦りますし、できないと「自分はダメだ」って自己嫌悪に陥ります。\n",
      "\u001b[32m10-25 22:32:06\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(203776,)\n",
      "\u001b[32m10-25 22:32:06\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど、「ダメだ」と感じてしまうのですね。ええ。その「〜すべき」という考えが、Bさんを苦しめているように聞こえます。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: JPCommonLabel_push_word() in jpcommon_label.c: First mora should not be long vowel symbol.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-25 22:32:06\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(409600,)\n",
      "\u001b[32m10-25 22:32:06\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。でも、周りに迷惑をかけるとか、評価が下がるのが怖くて。\n",
      "\u001b[32m10-25 22:32:06\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(232960,)\n",
      "\u001b[32m10-25 22:32:06\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、はい。優しい気持ちからなのですね。もし、少しその「〜すべき」を緩めてみたら、どうなると思いますか？\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: JPCommonLabel_push_word() in jpcommon_label.c: First mora should not be long vowel symbol.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-25 22:32:06\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(360448,)\n",
      "\u001b[32m10-25 22:32:06\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "不安はありますが、楽になれるなら、そういう考え方も…と、少し思いました。\n",
      "\u001b[32m10-25 22:32:07\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(264192,)\n",
      "\u001b[32m10-25 22:32:07\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうですね、その「少し」の気持ちを大切にしましょう。今日のお話から、Bさんを苦しめる「〜すべき思考」と、その背景にある優しい気持ちが分かりました。はい。次回は、これらとどう向き合うか、考えていきましょう。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: JPCommonLabel_push_word() in jpcommon_label.c: First mora should not be long vowel symbol.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-25 22:32:07\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(743424,)\n",
      "\u001b[32m10-25 22:32:07\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、お願いします。少し、気持ちが軽くなりました。\n",
      "\u001b[32m10-25 22:32:07\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(190976,)\n",
      "\u001b[32m10-25 22:32:07\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "よかったです。今日はお話ししてくださってありがとうございます。では、次回の予約ですが…。\n",
      "\u001b[32m10-25 22:32:07\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(287744,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:15\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:04\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_31...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_31\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m135.570\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:14\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:04\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_31...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_31\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m128.845\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-25 22:37:13\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こんにちは。今日はよくお越しくださいましたね。どうぞ、楽な姿勢でお座りください。\n",
      "\u001b[32m10-25 22:37:13\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(272384,)\n",
      "\u001b[32m10-25 22:37:13\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、ありがとうございます。なんだか、少し緊張しますね。\n",
      "\u001b[32m10-25 22:37:14\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(209408,)\n",
      "\u001b[32m10-25 22:37:14\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、そうですよね。初めての場所で、どんなことを話したらいいか、戸惑うこともあるかもしれませんね。今日は、まず、今お感じになっていることを、ゆっくりお聞かせいただければと思います。\n",
      "\u001b[32m10-25 22:37:14\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(588800,)\n",
      "\u001b[32m10-25 22:37:14\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。最近、なんだか漠然とした不安と焦りがあって…。仕事もそうなんですけど、プライベートでも、何をしていても落ち着かないというか。\n",
      "\u001b[32m10-25 22:37:14\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(453120,)\n",
      "\u001b[32m10-25 22:37:14\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうなんですね。漠然とした不安や焦り…はい。それは、お辛いですね。もう少し詳しく、どんな時に、その不安や焦りを感じることが多いか、お聞かせいただけますか？\n",
      "\u001b[32m10-25 22:37:14\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(594944,)\n",
      "\u001b[32m10-25 22:37:14\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。特に、職場の会議とか、人前で話す機会があると、頭が真っ白になってしまって。うまく話せないんじゃないかって、すごく不安になるんです。\n",
      "\u001b[32m10-25 22:37:15\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(440320,)\n",
      "\u001b[32m10-25 22:37:15\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど。会議中や人前で話す時なんですね。そうでしたか…。その時、例えば、どんなことを考えていらっしゃいますか？\n",
      "\u001b[32m10-25 22:37:15\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(408064,)\n",
      "\u001b[32m10-25 22:37:15\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "「どうせ自分なんかうまく言えない」とか、「変なことを言ったらどうしよう」って。そう思うと、すごく心臓がドキドキして、手も震えてきて、結局何も言えなくなっちゃうんです。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: JPCommonLabel_insert_pause() in jpcommon_label.c: First mora should not be short pause.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-25 22:37:15\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(515072,)\n",
      "\u001b[32m10-25 22:37:15\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、そうなんですね。「うまく言えないんじゃないか」という認知があって、心臓がドキドキしたり手が震えたりする感情、そして、何も言えなくなってしまう行動…。はい、とてもよくわかります。そういう時、お家で過ごされている時も、その気持ちが残っていたりしますか？\n",
      "\u001b[32m10-25 22:37:16\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(902656,)\n",
      "\u001b[32m10-25 22:37:16\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうですね…。会議が終わってからも、あの時もっとこう言えばよかったとか、ずっと考えてしまって。夜もなかなか眠れなくて、次の日も体が重いんです。\n",
      "\u001b[32m10-25 22:37:16\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(517120,)\n",
      "\u001b[32m10-25 22:37:16\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、そうなんですね。お仕事だけでなく、日常生活にも影響が出ているということでしたね。なるほど。今日はまず、そうやって漠然と感じていた不安が、少しずつ具体的に見えてきただけでも、大きな一歩だと思いますよ。\n",
      "\u001b[32m10-25 22:37:16\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(764928,)\n",
      "\u001b[32m10-25 22:37:16\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そう、ですか…。少し、話せて良かったです。\n",
      "\u001b[32m10-25 22:37:17\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(155648,)\n",
      "\u001b[32m10-25 22:37:17\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、ありがとうございます。では、今日は一旦ここまでとしまして、次回は、今日お話しいただいた「人前で話す時の不安」について、もう少し深くお話ししてみるのはいかがでしょうか。一緒に、どうしたら気持ちが楽になるか、考えていきましょう。\n",
      "\u001b[32m10-25 22:37:17\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(783360,)\n",
      "\u001b[32m10-25 22:37:17\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、よろしくお願いします。\n",
      "\u001b[32m10-25 22:37:17\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(102400,)\n",
      "\u001b[32m10-25 22:37:17\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、ありがとうございます。では、次回の予約ですが…\n",
      "\u001b[32m10-25 22:37:17\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(187904,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:37\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_32...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_32\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m158.789\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:40\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_32...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_32\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m164.012\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-25 22:43:30\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "Bさん、こんにちは。最近、何か気になっていることはありますか？\n",
      "\u001b[32m10-25 22:43:30\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(211456,)\n",
      "\u001b[32m10-25 22:43:30\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こんにちは。ええ、なんだか『ちゃんとしなきゃ』って焦ってしまって、疲れてしまうんです。\n",
      "\u001b[32m10-25 22:43:30\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(259072,)\n",
      "\u001b[32m10-25 22:43:30\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "『ちゃんとしなきゃ』ですね。はい。具体的には、どんな時にそう感じますか？\n",
      "\u001b[32m10-25 22:43:30\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(232960,)\n",
      "\u001b[32m10-25 22:43:30\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "仕事も家事も『完璧にこなすべきだ』と思ってしまって。そうすると、何も手につかなくて…\n",
      "\u001b[32m10-25 22:43:31\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(283136,)\n",
      "\u001b[32m10-25 22:43:31\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "完璧にこなすべき、と感じるのですね。ええ。それが、かえって動きを止めてしまう、ということでしたか。\n",
      "\u001b[32m10-25 22:43:31\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(339968,)\n",
      "\u001b[32m10-25 22:43:31\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、そうなんです。自分はダメだ、って責めてしまって。\n",
      "\u001b[32m10-25 22:43:31\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(177664,)\n",
      "\u001b[32m10-25 22:43:31\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど。ご自身を責めてしまうお気持ち、お辛いですよね。はい。その『完璧に』というお気持ちは、Bさんにとってどんな意味があるのでしょう。\n",
      "\u001b[32m10-25 22:43:31\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(479744,)\n",
      "\u001b[32m10-25 22:43:31\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "きっと、人から認められたい、ちゃんとした人間でいたい、という気持ちがあるんだと思います。\n",
      "\u001b[32m10-25 22:43:32\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(256512,)\n",
      "\u001b[32m10-25 22:43:32\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうなんですね。認められたい気持ちから、『こうすべき』という考えが生まれるのかもしれませんね。\n",
      "\u001b[32m10-25 22:43:32\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(301568,)\n",
      "\u001b[32m10-25 22:43:32\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。でも、それが苦しさに繋がっている気がします。\n",
      "\u001b[32m10-25 22:43:32\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(169984,)\n",
      "\u001b[32m10-25 22:43:32\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、そうですね。一生懸命なBさんだからこそ、でしょうか。今日は、その『〜すべきだ』というお気持ちについて、少しお話しができてよかったです。\n",
      "\u001b[32m10-25 22:43:33\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(454144,)\n",
      "\u001b[32m10-25 22:43:33\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、ありがとうございます。\n",
      "\u001b[32m10-25 22:43:33\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(98304,)\n",
      "\u001b[32m10-25 22:43:33\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "では、今日のセッションはここまでとしましょう。この『〜すべきだ』というお気持ち、次回までに少し意識してみていただけますか。無理のない範囲で大丈夫ですよ。\n",
      "\u001b[32m10-25 22:43:33\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(480768,)\n",
      "\u001b[32m10-25 22:43:33\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、わかりました。\n",
      "\u001b[32m10-25 22:43:33\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(76288,)\n",
      "\u001b[32m10-25 22:43:33\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。では、次回もまたお待ちしておりますね。お気をつけて。\n",
      "\u001b[32m10-25 22:43:33\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(208896,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:17\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:04\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_33...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_33\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m137.465\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:16\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:04\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_33...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_33\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m133.315\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-25 22:49:19\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こんにちは、Bさん。今日は、何かお話ししたいことはありますか？ええ。\n",
      "\u001b[32m10-25 22:49:19\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(221696,)\n",
      "\u001b[32m10-25 22:49:19\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、先生。「〜すべきだ」って思うことが多くて、それがプレッシャーなんです。仕事もプライベートも、もっとこうしなきゃって。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: JPCommonLabel_push_word() in jpcommon_label.c: First mora should not be long vowel symbol.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-25 22:49:20\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(400896,)\n",
      "\u001b[32m10-25 22:49:20\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど。「〜すべきだ」と感じることが多いのですね。ええ、具体的に、どんな時にそう思われますか？\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: JPCommonLabel_push_word() in jpcommon_label.c: First mora should not be long vowel symbol.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-25 22:49:20\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(337920,)\n",
      "\u001b[32m10-25 22:49:20\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "仕事だと「もっと効率的に」、家では「家事をきちんと」。友達といても「気の利いたことを言うべきだ」って。\n",
      "\u001b[32m10-25 22:49:20\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(343552,)\n",
      "\u001b[32m10-25 22:49:20\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうですか。様々な場面でそう感じるんですね。それはBさんにとって、どんな気持ちにつながっていますか？\n",
      "\u001b[32m10-25 22:49:20\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(350720,)\n",
      "\u001b[32m10-25 22:49:21\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、すごくしんどいです。完璧じゃないとダメって思って、何も手につかなかったり、自分を責めたりして…。\n",
      "\u001b[32m10-25 22:49:21\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(337920,)\n",
      "\u001b[32m10-25 22:49:21\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、お辛いですね。「完璧でなければ」という気持ちが、Bさんを苦しめているんですね。それは「すべき思考」かもしれませんね。\n",
      "\u001b[32m10-25 22:49:21\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(439296,)\n",
      "\u001b[32m10-25 22:49:21\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "「すべき思考」ですか…。なるほど、まさにそれです。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: JPCommonLabel_insert_pause() in jpcommon_label.c: First mora should not be short pause.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-25 22:49:21\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(178688,)\n",
      "\u001b[32m10-25 22:49:21\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。この「すべき思考」で、他に困ることはありますか？\n",
      "\u001b[32m10-25 22:49:21\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(178176,)\n",
      "\u001b[32m10-25 22:49:21\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "週末も「もっと有意義に」と、結局疲れて何もせず。人に頼るのも「自分で解決すべきだ」って思って。\n",
      "\u001b[32m10-25 22:49:22\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(374784,)\n",
      "\u001b[32m10-25 22:49:22\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど。すべき思考が、行動を妨げたり、頼ることを難しくしているんですね。ご自身の「すべき思考」に気づけたこと、大きな一歩ですよ。\n",
      "\u001b[32m10-25 22:49:22\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(502784,)\n",
      "\u001b[32m10-25 22:49:22\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ありがとうございます。納得できました。少し気持ちが楽になった気がします。\n",
      "\u001b[32m10-25 22:49:22\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(256512,)\n",
      "\u001b[32m10-25 22:49:22\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "良かったです。ええ。次回は、この「すべき思考」がBさんの行動や感情にどう影響しているか、もう少し詳しく見ていきましょうか？\n",
      "\u001b[32m10-25 22:49:22\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(416256,)\n",
      "\u001b[32m10-25 22:49:22\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、ぜひお願いします。少し前向きになれそうです。\n",
      "\u001b[32m10-25 22:49:23\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(185344,)\n",
      "\u001b[32m10-25 22:49:23\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、素晴らしいですね。また来週、同じ時間にお待ちしております。今日はありがとうございました。\n",
      "\u001b[32m10-25 22:49:23\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(329728,)\n",
      "\u001b[32m10-25 22:49:23\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ありがとうございました。\n",
      "\u001b[32m10-25 22:49:23\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(61440,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:25\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:06\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_34...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_34\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m145.880\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:22\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:04\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_34...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_34\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m134.469\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-25 22:54:53\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こんにちは、〇〇さん。本日はありがとうございます。今日はどんなことをお話ししたい気分ですか？\n",
      "\u001b[32m10-25 22:54:53\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(272384,)\n",
      "\u001b[32m10-25 22:54:53\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こんにちは。えっと、何から話したらいいか…最近、仕事もそうですが、漠然とした不安や焦りがずっとあって、落ち着かないんです。\n",
      "\u001b[32m10-25 22:54:54\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(434176,)\n",
      "\u001b[32m10-25 22:54:54\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうなんですね。漠然とした不安や焦り、ですか。ええ、ありがとうございます。まずはその『漠然とした不安』について、もう少し聞かせてもらえますか？\n",
      "\u001b[32m10-25 22:54:54\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(496640,)\n",
      "\u001b[32m10-25 22:54:54\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。なんだか、周りはちゃんとやっているのに自分だけ置いていかれている気がして…。仕事でも、これでいいのかなって考えて、休日もリラックスできないんです。\n",
      "\u001b[32m10-25 22:54:54\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(506880,)\n",
      "\u001b[32m10-25 22:54:54\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど。周りと比べてしまったり、仕事のことで考えてしまったり…休日もリラックスできないんですね。それはお辛いですね。\n",
      "\u001b[32m10-25 22:54:54\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(451584,)\n",
      "\u001b[32m10-25 22:54:54\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうなんです。以前は楽しめていたことが、最近は趣味にも集中できなくて…。このままでいいのかなって、本当に不安で。\n",
      "\u001b[32m10-25 22:54:55\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(399872,)\n",
      "\u001b[32m10-25 22:54:55\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、以前のようには楽しめないんですね。その不安な気持ち、よく分かります。具体的に『このままでいいのかな』と感じるのは、どんな時が多いですか？\n",
      "\u001b[32m10-25 22:54:55\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(508928,)\n",
      "\u001b[32m10-25 22:54:55\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "うーん…仕事中が多いですね。特に新しいプロジェクトが始まると、プレッシャーを感じて、失敗したらどうしようとか、期待に応えられないんじゃないかって考えてしまって。\n",
      "\u001b[32m10-25 22:54:55\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(526848,)\n",
      "\u001b[32m10-25 22:54:55\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、ありがとうございます。新しいプロジェクトのプレッシャーで、『このままでいいのかな』と感じるんですね。一生懸命取り組んでいらっしゃるからこそ、そう思われるのかもしれませんね。\n",
      "\u001b[32m10-25 22:54:56\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(547328,)\n",
      "\u001b[32m10-25 22:54:56\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうですね…どうしたらいいか分からなくて。\n",
      "\u001b[32m10-25 22:54:56\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(140288,)\n",
      "\u001b[32m10-25 22:54:56\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ。今日は、〇〇さんが漠然とした不安や焦りを感じ、それが仕事のプレッシャーとつながっていること、お聞かせいただきましたね。お話しくださったこと、大切な第一歩だと思います。\n",
      "\u001b[32m10-25 22:54:56\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(594944,)\n",
      "\u001b[32m10-25 22:54:56\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。\n",
      "\u001b[32m10-25 22:54:56\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(20480,)\n",
      "\u001b[32m10-25 22:54:56\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "今日のところはここまでとしましょうか。次回、今日お話しいただいたことをもう少し掘り下げてお聞きできればと思うのですが、いかがでしょう？何か気になることはありますか？\n",
      "\u001b[32m10-25 22:54:57\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(520192,)\n",
      "\u001b[32m10-25 22:54:57\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "大丈夫です。少し話せて良かったです。次回もよろしくお願いします。\n",
      "\u001b[32m10-25 22:54:57\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(239616,)\n",
      "\u001b[32m10-25 22:54:57\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、ありがとうございます。それでは、次回の予約についてご案内させていただきますね。今日はありがとうございました。\n",
      "\u001b[32m10-25 22:54:57\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(378880,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:28\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_35...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_35\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m149.311\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:33\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_35...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_35\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m142.794\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-25 23:00:25\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "今日で、もう〇回目のセッションになりますね。これまでのことを少し振り返ってみて、今どんなお気持ちですか？ ええ、このカウンセリングが始まってから、何か変化を感じることはありましたか？\n",
      "\u001b[32m10-25 23:00:25\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(583680,)\n",
      "\u001b[32m10-25 23:00:25\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうですね、最初は本当に漠然とした不安ばかりで、どうしたらいいかわからなかったんですけど…。ええ。でも、先生とお話しするうちに、少しずつ自分の気持ちを整理できるようになってきた気がします。\n",
      "\u001b[32m10-25 23:00:25\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(626176,)\n",
      "\u001b[32m10-25 23:00:25\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうなんですね。はい。漠然とした不安だったところが、整理できるようになってきた、ということですね。具体的に、どんなところでそう感じますか？\n",
      "\u001b[32m10-25 23:00:26\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(476160,)\n",
      "\u001b[32m10-25 23:00:26\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "例えば、以前は仕事で少しでもうまくいかないと、全部自分のせいだって思い込んでしまっていたんです。なるほど。でも、今は「これは自分の問題だけじゃないな」とか、「じゃあ、次はどうしようかな」って、少し冷静に考えられるようになったというか…。\n",
      "\u001b[32m10-25 23:00:26\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(763392,)\n",
      "\u001b[32m10-25 23:00:26\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、ええ。なるほど、それは大きな変化ですね。以前はご自身を責めてしまうことが多かったのが、今は客観的に、そして前向きに捉えられるようになった、ということでしょうか。\n",
      "\u001b[32m10-25 23:00:26\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(621056,)\n",
      "\u001b[32m10-25 23:00:26\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、まさにそんな感じです。はい。すごく気持ちが楽になった部分もありますし、自分で解決策を探す力もついてきた気がします。\n",
      "\u001b[32m10-25 23:00:26\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(439808,)\n",
      "\u001b[32m10-25 23:00:26\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "素晴らしいですね。ご自身で解決策を探す力、確かにBさん、とてもよく考え、行動されていましたものね。そうした変化が、日常生活の中ではどんな風に現れていますか？\n",
      "\u001b[32m10-25 23:00:27\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(631808,)\n",
      "\u001b[32m10-25 23:00:27\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "うーん、そうですね。以前は休日に家にいると、どんどん気分が落ち込んでいたんですけど、今は「あれしてみようかな」「これ行ってみようかな」って、少しずつ行動できるようになりました。はい。友達と会うのも、前ほど億劫じゃなくなりましたし。\n",
      "\u001b[32m10-25 23:00:27\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(754176,)\n",
      "\u001b[32m10-25 23:00:27\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうなんですね。ええ。ご自身の変化を実感されているんですね。漠然とした不安から、具体的な行動への変化。それは本当に素晴らしい進歩だと思います。\n",
      "\u001b[32m10-25 23:00:27\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(557056,)\n",
      "\u001b[32m10-25 23:00:27\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ありがとうございます。先生のおかげです。\n",
      "\u001b[32m10-25 23:00:28\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(126464,)\n",
      "\u001b[32m10-25 23:00:28\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "いえいえ、Bさんご自身が一生懸命取り組まれた結果ですよ。ご自身で気づき、行動されたからこそ、ですね。これで一旦、区切りにはなりますが、もしまた何かあったらいつでも相談してくださいね。今日はお話しいただき、ありがとうございました。\n",
      "\u001b[32m10-25 23:00:28\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(802304,)\n",
      "\u001b[32m10-25 23:00:28\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こちらこそ、本当にありがとうございました。また何かあったら、ぜひお願いします。\n",
      "\u001b[32m10-25 23:00:28\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(269312,)\n",
      "\u001b[32m10-25 23:00:28\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。では、お気をつけてお帰りくださいね。\n",
      "\u001b[32m10-25 23:00:28\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(146944,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:33\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_36...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_36\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m144.288\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:36\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:04\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_36...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_36\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m146.409\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-25 23:05:56\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こんにちは、Bさん。今日はなんだか、少しお疲れのようにも見えますが、いかがですか？\n",
      "\u001b[32m10-25 23:05:57\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(261120,)\n",
      "\u001b[32m10-25 23:05:57\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こんにちは、A先生。はい、少し、考えすぎてしまっているかもしれません。最近、仕事でもプライベートでも、「こうあるべきだ」って、自分を追い詰めてしまっているような気がして。\n",
      "\u001b[32m10-25 23:05:57\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(544768,)\n",
      "\u001b[32m10-25 23:05:57\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうなんですね。「こうあるべきだ」というお気持ちが強くなっているんですね。ええ、とてもよくわかります。どんな時に、特にそう感じることが多いですか？\n",
      "\u001b[32m10-25 23:05:57\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(479232,)\n",
      "\u001b[32m10-25 23:05:57\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "例えば、週末も「何か生産的なことをすべきだ」と思って、ずっと落ち着かないんです。ゆっくり休んでいても、「これでいいのかな」って不安になるというか…。\n",
      "\u001b[32m10-25 23:05:58\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(437248,)\n",
      "\u001b[32m10-25 23:05:58\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど。週末くらいはゆっくり休みたい、という気持ちと、「何かをすべきだ」という気持ちの間で揺れ動いているのですね。はい。それは、お辛いですね。\n",
      "\u001b[32m10-25 23:05:58\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(516608,)\n",
      "\u001b[32m10-25 23:05:58\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。結局、何もできなかった自分を責めてしまって、余計に疲れてしまう、みたいな。\n",
      "\u001b[32m10-25 23:05:58\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(303104,)\n",
      "\u001b[32m10-25 23:05:58\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうなんですね。自分を責めてしまうんですね。もし、少しだけ立ち止まって、「生産的なことをすべき」というそのお気持ちを、違う角度から見てみるとしたら、どうでしょう？例えば、休むことも、Bさんにとってはとても大切な「生産的なこと」だとは考えられないでしょうか？\n",
      "\u001b[32m10-25 23:05:59\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(871424,)\n",
      "\u001b[32m10-25 23:05:59\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "うーん…。休むことが生産的、ですか…。そう言われると、確かにそうかもしれない、とは思いますが、なかなかそうは思えなくて。\n",
      "\u001b[32m10-25 23:05:59\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(400896,)\n",
      "\u001b[32m10-25 23:05:59\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、すぐにそう思えなくても大丈夫ですよ。そうなんですね。ただ、その「すべき」という考えが、今のBさんの心を少し窮屈にさせてしまっているのかもしれませんね。\n",
      "\u001b[32m10-25 23:05:59\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(573952,)\n",
      "\u001b[32m10-25 23:05:59\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、そうだと思います。\n",
      "\u001b[32m10-25 23:05:59\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(84480,)\n",
      "\u001b[32m10-25 23:05:59\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ありがとうございます。今日は、その「こうあるべきだ」というお気持ちについて、Bさんが少し立ち止まって考えられるようなヒントを見つけられたら嬉しいです。来週までに、もしよかったら、週末に「何もしない時間」を少しだけ作ってみて、その時にどんな気持ちになるか、観察してみるというのはいかがでしょう？\n",
      "\u001b[32m10-25 23:06:00\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(921088,)\n",
      "\u001b[32m10-25 23:06:00\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "わかりました。試してみます。\n",
      "\u001b[32m10-25 23:06:00\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(107008,)\n",
      "\u001b[32m10-25 23:06:00\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ。無理のない範囲で大丈夫ですからね。今日はここまでにして、また来週、お話を伺えますか？\n",
      "\u001b[32m10-25 23:06:00\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(340992,)\n",
      "\u001b[32m10-25 23:06:00\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、よろしくお願いします。\n",
      "\u001b[32m10-25 23:06:00\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(100352,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:30\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:04\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_37...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_37\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m153.005\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:29\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:04\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_37...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_37\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m139.843\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-25 23:11:26\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こんにちは、今日は来てくださってありがとうございます。何か、お話ししたいことがあって、お越しくださったんですよね。\n",
      "\u001b[32m10-25 23:11:27\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(335360,)\n",
      "\u001b[32m10-25 23:11:27\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、あの、最近、なんだか漠然とした不安というか、焦りを感じることが多くて。仕事もそうですし、プライベートでも、ずっとモヤモヤしているんです。\n",
      "\u001b[32m10-25 23:11:27\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(500224,)\n",
      "\u001b[32m10-25 23:11:27\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうなんですね、漠然とした不安や焦り、ということでしたか。ええ、お話ししてくださり、ありがとうございます。どんな時に、特にそう感じられますか？\n",
      "\u001b[32m10-25 23:11:27\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(507392,)\n",
      "\u001b[32m10-25 23:11:27\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "仕事では、周りの期待に応えられていないんじゃないかとか、このままでいいのか、って考えてしまって。何か新しいことを始めたい気持ちもあるんですけど、結局、何もできていないんです。\n",
      "\u001b[32m10-25 23:11:28\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(589312,)\n",
      "\u001b[32m10-25 23:11:28\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど、周りの期待に応えられていないのではないか、というお気持ちと、何か始めたいけれど一歩踏み出せない、という感じなんですね。はい。それはお辛いですね。\n",
      "\u001b[32m10-25 23:11:28\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(551936,)\n",
      "\u001b[32m10-25 23:11:28\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。結局、休日は疲れてゴロゴロしてしまって、そのことに対してもまた自己嫌悪に陥る、みたいな。悪循環なんです。\n",
      "\u001b[32m10-25 23:11:28\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(419328,)\n",
      "\u001b[32m10-25 23:11:28\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、そうなんですね。仕事のストレスが、日常生活にも影響してしまっている、ということでしょうか。\n",
      "\u001b[32m10-25 23:11:28\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(342528,)\n",
      "\u001b[32m10-25 23:11:28\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "まさに、そうです。この状況をなんとかしたい、って思っているんですけど、何から手をつけていいのか、全然わからなくて。\n",
      "\u001b[32m10-25 23:11:29\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(352256,)\n",
      "\u001b[32m10-25 23:11:29\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうなんですね。漠然とした不安が、日々の生活の中で、どういう時に特に強く感じられるのか、今はまだはっきりしない状態なんですね。でも、今日こうしてお話ししてくださったこと、そして「なんとかしたい」と思ってここに来てくださったこと、それがもう大きな一歩だと思いますよ。\n",
      "\u001b[32m10-25 23:11:29\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(976896,)\n",
      "\u001b[32m10-25 23:11:29\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そう言っていただけると、少し気持ちが楽になります。\n",
      "\u001b[32m10-25 23:11:29\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(151040,)\n",
      "\u001b[32m10-25 23:11:29\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、今日はまず、その漠然としたお気持ちを言葉にできただけでも、本当に素晴らしいことです。次回は、もう少し具体的に、どんな時にその不安を感じるのか、もしよろしければ、お話しを聞かせていただけますか？\n",
      "\u001b[32m10-25 23:11:30\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(700416,)\n",
      "\u001b[32m10-25 23:11:30\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、わかりました。また、お願いします。\n",
      "\u001b[32m10-25 23:11:30\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(155648,)\n",
      "\u001b[32m10-25 23:11:30\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。では、今日はここまでにしましょう。次回も、またお話しできるのを楽しみにしていますね。\n",
      "\u001b[32m10-25 23:11:30\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(311296,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:35\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:04\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_38...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_38\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m147.929\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:28\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_38...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_38\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m138.085\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-25 23:16:49\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "〇〇さん、今日はよくお越しくださいました。どうぞ楽になさってくださいね。何か、最近気になることなど、お話しできる範囲で構いませんので、聞かせていただけますか？\n",
      "\u001b[32m10-25 23:16:49\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(539648,)\n",
      "\u001b[32m10-25 23:16:49\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、ありがとうございます。あの、最近、なんだか漠然とした不安というか、焦りを感じることが多くて…何から話せばいいのかも、正直よくわからなくて…\n",
      "\u001b[32m10-25 23:16:49\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(564736,)\n",
      "\u001b[32m10-25 23:16:49\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうなんですね。漠然とした不安や焦り、感じていらっしゃるのですね。ええ、大丈夫ですよ、何からでも構いません。思いつくままに、お話しいただけますか？\n",
      "\u001b[32m10-25 23:16:49\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(538624,)\n",
      "\u001b[32m10-25 23:16:49\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい…。仕事では、今までと同じようにやっているつもりなんですけど、なんだか効率が落ちたような気がして。周りも忙しそうで、なかなか相談もできなくて。\n",
      "\u001b[32m10-25 23:16:50\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(519680,)\n",
      "\u001b[32m10-25 23:16:50\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど。お仕事でのパフォーマンスについて、少し気になさっているのですね。ええ、周りの方も忙しそうだと、相談しにくい気持ち、よくわかります。お仕事以外の、日常生活のことで何か変化などはありましたか？\n",
      "\u001b[32m10-25 23:16:50\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(723456,)\n",
      "\u001b[32m10-25 23:16:50\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうですね…家に帰っても、気分が晴れないというか、何をしていても集中できなくて。休日も、以前は楽しめていた趣味も、今は手につかないことが多くて…\n",
      "\u001b[32m10-25 23:16:50\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(557568,)\n",
      "\u001b[32m10-25 23:16:50\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、そうなんですね。お仕事だけでなく、休日や趣味の面でも、以前との違いを感じていらっしゃるのですね。心から楽しめない状況が続いているのは、つらいことですよね。\n",
      "\u001b[32m10-25 23:16:51\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(597504,)\n",
      "\u001b[32m10-25 23:16:51\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、本当に…。このままじゃいけないって思うんですけど、どうすればいいのかわからなくて、またそれが余計に焦りにつながってしまって。\n",
      "\u001b[32m10-25 23:16:51\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(418816,)\n",
      "\u001b[32m10-25 23:16:51\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、そうですね。どうにかしたいという気持ちがあるのに、どうすればいいかわからない、その堂々巡りがさらに苦しさを増しているのかもしれませんね。そういったお気持ち、とてもよくわかります。\n",
      "\u001b[32m10-25 23:16:51\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(656896,)\n",
      "\u001b[32m10-25 23:16:51\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ありがとうございます。なんだか、話していると少し落ち着いてきました。\n",
      "\u001b[32m10-25 23:16:51\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(203776,)\n",
      "\u001b[32m10-25 23:16:51\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "よかったです。今日は漠然とした不安や焦り、お仕事や日常生活での変化についてお聞かせいただきました。〇〇さんの今のお気持ちを少しでも整理するお手伝いができたなら幸いです。今日のところはここまでとしましょうか。もしよろしければ、また来週、お話しを伺うお時間をいただけますでしょうか？\n",
      "\u001b[32m10-25 23:16:52\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(948736,)\n",
      "\u001b[32m10-25 23:16:52\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、ぜひお願いしたいです。今日はありがとうございました。\n",
      "\u001b[32m10-25 23:16:52\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(186880,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:33\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:04\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_39...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_39\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m144.002\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:32\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:04\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_39...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_39\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m140.753\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-25 23:22:46\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こんにちは、Bさん。今日はありがとうございます。何か、お話ししたいことはありますか？\n",
      "\u001b[32m10-25 23:22:47\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(261120,)\n",
      "\u001b[32m10-25 23:22:47\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、こんにちは。最近、仕事で頭がいっぱいで、なんだか落ち着かないんです。\n",
      "\u001b[32m10-25 23:22:47\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(265216,)\n",
      "\u001b[32m10-25 23:22:47\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうなんですね。仕事が離れないと。ええ、どんな時にそう感じますか？\n",
      "\u001b[32m10-25 23:22:47\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(258560,)\n",
      "\u001b[32m10-25 23:22:47\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "仕事中はもちろん、家でも休日でも。常に何かに追われている感覚です。\n",
      "\u001b[32m10-25 23:22:47\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(250880,)\n",
      "\u001b[32m10-25 23:22:47\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど。いつ頃からそう感じますか？\n",
      "\u001b[32m10-25 23:22:47\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(131072,)\n",
      "\u001b[32m10-25 23:22:47\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "去年の大きなプロジェクト後、特に。でも、以前から漠然とした焦りも。\n",
      "\u001b[32m10-25 23:22:48\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(237056,)\n",
      "\u001b[32m10-25 23:22:48\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうでしたか。それが日常生活にも影響は？睡眠とか…。\n",
      "\u001b[32m10-25 23:22:48\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(238592,)\n",
      "\u001b[32m10-25 23:22:48\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、夜は寝付けず、朝も体が重い。散歩も億劫で。\n",
      "\u001b[32m10-25 23:22:48\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(204800,)\n",
      "\u001b[32m10-25 23:22:48\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "それは辛いですね。睡眠が乱れると心身ともに疲れますよね。何か対処はされましたか？\n",
      "\u001b[32m10-25 23:22:49\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(324608,)\n",
      "\u001b[32m10-25 23:22:49\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "試しましたが、うまくいかなくて。気分転換しようとしても、仕事が頭から離れず、疲れてしまうんです。\n",
      "\u001b[32m10-25 23:22:49\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(349184,)\n",
      "\u001b[32m10-25 23:22:49\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、一生懸命されたんですね。今日はお話しくださってありがとうございます。次回、この「追われている感覚」について詳しく話せますか？\n",
      "\u001b[32m10-25 23:22:49\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(456704,)\n",
      "\u001b[32m10-25 23:22:49\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、よろしくお願いします。\n",
      "\u001b[32m10-25 23:22:49\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(101888,)\n",
      "\u001b[32m10-25 23:22:49\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、ありがとうございます。では、今日はこれで終わりに。次回の予約はスタッフからご案内します。お気をつけてお帰りください。\n",
      "\u001b[32m10-25 23:22:50\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(401408,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:14\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_40...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_40\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m137.016\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:14\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:04\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_40...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_40\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m130.303\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-25 23:27:51\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "Bさん、こんにちは。今日は前回お話しした「漠然とした焦り」について、もう少し詳しくお伺いしてもいいですか？\n",
      "\u001b[32m10-25 23:27:51\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(366592,)\n",
      "\u001b[32m10-25 23:27:51\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、もちろんです。最近、本当に何もかもが「もっとこうすべきだ」と感じてしまって…。\n",
      "\u001b[32m10-25 23:27:52\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(272384,)\n",
      "\u001b[32m10-25 23:27:52\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、そうなんですね。「もっとこうすべきだ」と感じる…、具体的にはどのような時にそう思われますか？\n",
      "\u001b[32m10-25 23:27:52\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(349696,)\n",
      "\u001b[32m10-25 23:27:52\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "例えば、仕事では「もっと早く終わらせるべき」とか、家では「もっと部屋をきれいにするべき」とか…。もう、常に何かを「すべき」って考えている気がします。\n",
      "\u001b[32m10-25 23:27:52\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(455168,)\n",
      "\u001b[32m10-25 23:27:52\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど…。本当に、常に「すべき」という言葉が頭の中にあるのですね。それはとてもお辛いことですよね。\n",
      "\u001b[32m10-25 23:27:52\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(392704,)\n",
      "\u001b[32m10-25 23:27:52\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、しんどいです。そう思っても、なかなか体が動かないと「どうしてできないんだろう」って自分を責めてしまって。\n",
      "\u001b[32m10-25 23:27:53\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(342528,)\n",
      "\u001b[32m10-25 23:27:53\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、そうでしたか。ご自身を責めてしまうのですね…。その「もっと早く終わらせるべき」とか、「もっときれいにするべき」という考えは、Bさんにとってどのような意味を持っているのでしょう？\n",
      "\u001b[32m10-25 23:27:53\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(589312,)\n",
      "\u001b[32m10-25 23:27:53\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "うーん…。意味、ですか。何でしょうね…。たぶん、周りの人に迷惑をかけちゃいけないとか、期待に応えなきゃいけないって気持ちがあるからだと思います。\n",
      "\u001b[32m10-25 23:27:53\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(472064,)\n",
      "\u001b[32m10-25 23:27:53\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、周りの方への配慮や、期待に応えたいというお気持ちがあるのですね。それは、Bさんのとても優しい一面でもありますね。\n",
      "\u001b[32m10-25 23:27:53\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(412672,)\n",
      "\u001b[32m10-25 23:27:53\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ありがとうございます。でも、それが自分を苦しめているような気もしてきて。\n",
      "\u001b[32m10-25 23:27:54\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(217088,)\n",
      "\u001b[32m10-25 23:27:54\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、そうですね。では、もし「すべき」という考えがなかったとしたら、Bさんはその時、どう感じて、どう行動したいと思われますか？少し考えてみませんか？\n",
      "\u001b[32m10-25 23:27:54\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(519168,)\n",
      "\u001b[32m10-25 23:27:54\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "えっ…、「すべき」がなかったら、ですか。想像したことがなかったです…。\n",
      "\u001b[32m10-25 23:27:54\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(215552,)\n",
      "\u001b[32m10-25 23:27:54\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうですよね。今日は、この「すべき」という考えがBさんにどのような影響を与えているのか、少し見つめ直す良いきっかけになったかもしれませんね。次回までに、もしよかったら、普段「すべき」と感じた時に、それが本当に必要か、少し立ち止まって考えてみる時間を持ってみるのはいかがでしょう？\n",
      "\u001b[32m10-25 23:27:55\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(949760,)\n",
      "\u001b[32m10-25 23:27:55\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、やってみます。なんだか、少し視点が変わったような気がします。\n",
      "\u001b[32m10-25 23:27:55\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(214016,)\n",
      "\u001b[32m10-25 23:27:55\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、素晴らしいです。今日はここまでとしましょうか。お疲れ様でした。\n",
      "\u001b[32m10-25 23:27:55\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(237568,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:32\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:04\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_41...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_41\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m153.795\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:31\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:04\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_41...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_41\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m142.139\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-25 23:33:28\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こんにちは、今日はどうぞよろしくお願いします。今日は、まずお話しをゆっくり聞かせていただきたいなと思っています。何か、今お困りのことや、話してみたいことなどありますか？\n",
      "\u001b[32m10-25 23:33:28\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(554496,)\n",
      "\u001b[32m10-25 23:33:28\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、よろしくお願いします。実は最近、仕事のこともそうですし、なんだか日常生活全般で、こう漠然とした不安というか、焦りみたいなものを感じていて...。何から話したらいいか、という感じなんですけど。\n",
      "\u001b[32m10-25 23:33:28\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(687104,)\n",
      "\u001b[32m10-25 23:33:28\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうなんですね。ええ、漠然とした不安や焦りを感じていらっしゃるんですね。ありがとうございます、話してくださって。そういった漠然とした、というお気持ちは、どんな時に特に感じられますか？何かきっかけのようなものがあったりしますか？\n",
      "\u001b[32m10-25 23:33:29\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(775680,)\n",
      "\u001b[32m10-25 23:33:29\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "きっかけというか、ずっと続いている感じなんですけど、休みの日に家で過ごしていても、なんだか気持ちが休まらないというか、常に何かしていないといけないような気がしてしまって。\n",
      "\u001b[32m10-25 23:33:29\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(515072,)\n",
      "\u001b[32m10-25 23:33:29\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど。休日に家で過ごされていても、気持ちが休まらない、常に何かしていないといけないと感じてしまうんですね。それはお辛いですね。ええ、そういったお気持ち、とてもよくわかります。\n",
      "\u001b[32m10-25 23:33:29\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(655360,)\n",
      "\u001b[32m10-25 23:33:29\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい...。周りの人はみんなもっと頑張っているんじゃないかとか、自分だけ取り残されているような、変な焦りを感じたりして。仕事も集中できない時があって、それがまた不安で...。\n",
      "\u001b[32m10-25 23:33:30\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(570368,)\n",
      "\u001b[32m10-25 23:33:30\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、ええ。周りの方と比べてしまったり、取り残されているような焦りを感じていらっしゃるんですね。それで、お仕事にも集中しにくい時があると。それは、とてもしんどいことだと思います。\n",
      "\u001b[32m10-25 23:33:30\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(631296,)\n",
      "\u001b[32m10-25 23:33:30\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうなんです。このままでいいのかな、ってずっと考えてしまって。\n",
      "\u001b[32m10-25 23:33:30\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(196096,)\n",
      "\u001b[32m10-25 23:33:30\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、そうですね。ご自身の中で、このままでいいのかな、という気持ちがずっとあるのですね。今日はお話ししてくださって、ありがとうございます。まずは、そういった漠然とした不安や、休日に休まらないと感じていらっしゃることをお聞かせいただけて、とても良かったと思います。\n",
      "\u001b[32m10-25 23:33:30\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(885760,)\n",
      "\u001b[32m10-25 23:33:30\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こちらこそ、ありがとうございます。少し話せて、落ち着きました。\n",
      "\u001b[32m10-25 23:33:31\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(192512,)\n",
      "\u001b[32m10-25 23:33:31\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、良かったです。今日お話しいただいたことを踏まえて、次回はもう少し、その「漠然とした不安」が具体的にどんな状況で強くなるのか、一緒に考えていく時間を取らせていただけますか？\n",
      "\u001b[32m10-25 23:33:31\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(611328,)\n",
      "\u001b[32m10-25 23:33:31\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、ぜひお願いします。\n",
      "\u001b[32m10-25 23:33:31\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(88576,)\n",
      "\u001b[32m10-25 23:33:31\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ありがとうございます。では、今日のところはこれで一区切りとさせていただいて、次回の面談でまたお話しを伺わせてください。今日はお越しくださって、ありがとうございました。\n",
      "\u001b[32m10-25 23:33:31\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(537600,)\n",
      "\u001b[32m10-25 23:33:31\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ありがとうございました。\n",
      "\u001b[32m10-25 23:33:31\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(60928,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:34\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:04\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_42...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_42\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m145.030\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:40\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:04\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_42...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_42\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m149.704\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-25 23:38:56\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "〇〇さん、これまで数回にわたってセッションを重ねてきましたが、いよいよ終結に向けての準備の時期に入ってきましたね。これまでの時間を少し振り返ってみて、今どんなお気持ちですか？\n",
      "\u001b[32m10-25 23:38:56\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(578560,)\n",
      "\u001b[32m10-25 23:38:56\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうですね…なんだかあっという間だったような気がします。最初は本当に漠然とした不安ばかりで、どうしていいか分からなかったんですけど…\n",
      "\u001b[32m10-25 23:38:57\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(425472,)\n",
      "\u001b[32m10-25 23:38:57\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、そうでしたね。漠然とした不安を感じていらっしゃいましたもんね。ええ。その中で、〇〇さんご自身で、何か変化を感じることはありましたか？\n",
      "\u001b[32m10-25 23:38:57\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(456192,)\n",
      "\u001b[32m10-25 23:38:57\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。例えば、以前は職場の人間関係で何かあると、ずっと引きずってしまって、何も手につかなかったんですが、最近は「まあ、そういうこともあるか」って、少し冷静に受け止められるようになった気がします。\n",
      "\u001b[32m10-25 23:38:57\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(594944,)\n",
      "\u001b[32m10-25 23:38:57\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど、それは大きな変化ですね。ええ。そうなんですね。具体的に、どんな風に捉え方が変わっていったんでしょう？\n",
      "\u001b[32m10-25 23:38:58\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(397312,)\n",
      "\u001b[32m10-25 23:38:58\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "うーん…カウンセラーさんと話しているうちに、自分の感情を客観的に見つめる練習ができたからかなって。感情に飲み込まれずに、一歩引いて考えられるようになったというか。\n",
      "\u001b[32m10-25 23:38:58\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(516608,)\n",
      "\u001b[32m10-25 23:38:58\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうなんですね。ご自身の感情を客観的に捉える、ということができたんですね。素晴らしいですね。その変化が、〇〇さんにとってどんな意味を持っていますか？\n",
      "\u001b[32m10-25 23:38:58\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(495616,)\n",
      "\u001b[32m10-25 23:38:58\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "すごく楽になりました。以前は小さなことでもくよくよして、それがまた次の不安につながる…みたいな悪循環だったのが、今は「これでいい」って思える瞬間が増えました。\n",
      "\u001b[32m10-25 23:38:58\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(562176,)\n",
      "\u001b[32m10-25 23:38:58\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、悪循環から抜け出せたというのは本当に大きなことですね。はい。ご自身でその変化に気づき、楽になれたことは、〇〇さんのこれからの生活にとっても、とても大切なことだと思います。\n",
      "\u001b[32m10-25 23:38:59\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(653824,)\n",
      "\u001b[32m10-25 23:38:59\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ありがとうございます。最初は本当にどうなることかと思いましたが、今は少し自信が持てるようになりました。\n",
      "\u001b[32m10-25 23:38:59\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(321536,)\n",
      "\u001b[32m10-25 23:38:59\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そう言っていただけると、私もとても嬉しいです。ええ。これからは、このセッションで得た気づきや、ご自身で身につけられた対処法を、ぜひ日常生活の中で意識的に使ってみてくださいね。\n",
      "\u001b[32m10-25 23:38:59\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(614400,)\n",
      "\u001b[32m10-25 23:38:59\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、そうします。これからも意識して、頑張ってみます。\n",
      "\u001b[32m10-25 23:38:59\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(172544,)\n",
      "\u001b[32m10-25 23:38:59\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。もしまた何か困ったことがあれば、いつでもご相談くださいね。今回のセッションは、〇〇さんにとってご自身を深く見つめ直す良い機会になったのではないでしょうか。次回で一旦区切りとなりますが、またお話ししましょう。\n",
      "\u001b[32m10-25 23:39:00\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(706048,)\n",
      "\u001b[32m10-25 23:39:00\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、ありがとうございました。次回もよろしくお願いします。\n",
      "\u001b[32m10-25 23:39:00\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(202752,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:34\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:04\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_43...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_43\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m143.268\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:31\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:04\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_43...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_43\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m141.741\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-25 23:44:45\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こんにちは。今日はどうされましたか？ゆっくりお話聞かせてもらえますか？\n",
      "\u001b[32m10-25 23:44:46\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(239616,)\n",
      "\u001b[32m10-25 23:44:46\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こんにちは…最近、仕事で漠然とした不安がずっとあって、落ち着かないんです。\n",
      "\u001b[32m10-25 23:44:46\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(252416,)\n",
      "\u001b[32m10-25 23:44:46\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうなんですね。お仕事の不安、ええ。どんな時にそう感じますか？\n",
      "\u001b[32m10-25 23:44:46\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(227840,)\n",
      "\u001b[32m10-25 23:44:46\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。朝から憂鬱で、仕事も集中できず、夜も寝付けなくて…。\n",
      "\u001b[32m10-25 23:44:46\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(252416,)\n",
      "\u001b[32m10-25 23:44:46\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど。朝から憂鬱で、睡眠にも影響が。お辛いでしょう。\n",
      "\u001b[32m10-25 23:44:47\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(221184,)\n",
      "\u001b[32m10-25 23:44:47\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ。このままでいいのかって焦る気持ちもあって。\n",
      "\u001b[32m10-25 23:44:47\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(156672,)\n",
      "\u001b[32m10-25 23:44:47\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、焦る気持ち、よく分かります。いつ頃からですか？\n",
      "\u001b[32m10-25 23:44:47\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(211456,)\n",
      "\u001b[32m10-25 23:44:47\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "半年くらい前からでしょうか。新しいプロジェクトが始まってから、特に。\n",
      "\u001b[32m10-25 23:44:48\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(219136,)\n",
      "\u001b[32m10-25 23:44:48\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうなんですね。プロジェクトがきっかけで。頑張ってこられましたね。\n",
      "\u001b[32m10-25 23:44:48\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(219136,)\n",
      "\u001b[32m10-25 23:44:48\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "どうしたらいいのか分からなくて…。\n",
      "\u001b[32m10-25 23:44:48\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(88576,)\n",
      "\u001b[32m10-25 23:44:48\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、お辛いですね。今日はまずお話しくださってありがとうございます。次回、もう少し詳しく背景を探り、一緒に楽になる方法を考えましょう。\n",
      "\u001b[32m10-25 23:44:48\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(516096,)\n",
      "\u001b[32m10-25 23:44:48\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、お願いします。少し楽になりました。\n",
      "\u001b[32m10-25 23:44:48\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(146432,)\n",
      "\u001b[32m10-25 23:44:48\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、良かったです。では、次回の予約、確認させてくださいね。\n",
      "\u001b[32m10-25 23:44:49\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(223232,)\n",
      "\u001b[32m10-25 23:44:49\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、お願いします。\n",
      "\u001b[32m10-25 23:44:49\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(79872,)\n",
      "\u001b[32m10-25 23:44:49\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ありがとうございます。今日はここまでとさせていただきます。お疲れ様でした。\n",
      "\u001b[32m10-25 23:44:49\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(245248,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:13\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:04\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_44...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_44\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m123.901\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:11\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_44...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_44\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m121.095\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-25 23:49:27\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "〇〇さん、今日はこれまでのセッションを少し振り返ってみませんか。このカウンセリングを通じて、どんな変化や気づきがありましたか？\n",
      "\u001b[32m10-25 23:49:27\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(386560,)\n",
      "\u001b[32m10-25 23:49:27\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうですね…。最初は漠然とした不安ばかりで、何から話せばいいのかもわからなかったんですけれど…本当に、もやもやしていました。\n",
      "\u001b[32m10-25 23:49:27\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(435712,)\n",
      "\u001b[32m10-25 23:49:27\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、はい、そうでしたね。最初は本当に、心の中に霧がかかっているようでしたよね。\n",
      "\u001b[32m10-25 23:49:27\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(302080,)\n",
      "\u001b[32m10-25 23:49:27\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。でも、先生とお話しするうちに、自分が何に不安を感じているのか、少しずつ具体的に見えてきた気がします。\n",
      "\u001b[32m10-25 23:49:28\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(374784,)\n",
      "\u001b[32m10-25 23:49:28\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど。ご自身の感情や考えが、少しずつ整理されていったということでしょうか。そうなんですね。\n",
      "\u001b[32m10-25 23:49:28\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(326656,)\n",
      "\u001b[32m10-25 23:49:28\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうです。特に、仕事で意見を言えないのは『失敗が怖い』という認知から来ているって、図で整理してもらった時、すごく腑に落ちて。目から鱗でした。\n",
      "\u001b[32m10-25 23:49:28\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(460800,)\n",
      "\u001b[32m10-25 23:49:28\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、あの時の作業ですね。ご自身の『認知』『感情』『行動』のつながりを整理することで、客観的に捉えられるようになったと。\n",
      "\u001b[32m10-25 23:49:29\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(412160,)\n",
      "\u001b[32m10-25 23:49:29\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。それからは、すぐに完璧にはできなくても、少しずつ試してみようと思えるようになりました。実際に、小さなことから発言してみたりして。\n",
      "\u001b[32m10-25 23:49:29\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(458240,)\n",
      "\u001b[32m10-25 23:49:29\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "素晴らしいですね。ご自身で具体的な行動に繋げていらっしゃる。そうなんですね、頑張っていらっしゃいますね。\n",
      "\u001b[32m10-25 23:49:29\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(357888,)\n",
      "\u001b[32m10-25 23:49:29\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ただ、まだ時々、これで本当に大丈夫かな、と不安になることもあって…。\n",
      "\u001b[32m10-25 23:49:29\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(231936,)\n",
      "\u001b[32m10-25 23:49:29\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、そうですね。それは自然なことだと思いますよ。今後、もしまたそういった気持ちになった時、ご自身でどのように乗り越えていけそうか、何か考えてみたりしましたか？\n",
      "\u001b[32m10-25 23:49:29\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(573952,)\n",
      "\u001b[32m10-25 23:49:29\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "あの時の図をもう一度書いてみたり、先生に教えてもらったリラックス法を試してみたり…ですかね。あと、ホームワークでやった「スモールステップ」も役立つかもしれません。\n",
      "\u001b[32m10-25 23:49:30\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(507392,)\n",
      "\u001b[32m10-25 23:49:30\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、とても良いですね。これまで一緒に見つけてきた方法を、ぜひご自身の力で活用してみてください。今日はこれまでの変化を改めて確認できて、私も大変嬉しく思います。終結に向けて、次回も引き続き、今後のことについてお話しできればと思いますがいかがでしょうか？\n",
      "\u001b[32m10-25 23:49:30\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(949248,)\n",
      "\u001b[32m10-25 23:49:30\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、お願いします。\n",
      "\u001b[32m10-25 23:49:30\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(78848,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:28\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:04\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_45...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_45\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m137.849\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:31\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:04\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_45...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_45\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m144.905\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-25 23:54:58\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こんにちは、〇〇さん。今日は来てくださってありがとうございます。少しでも楽にお話しできるよう、お手伝いさせていただきますね。\n",
      "\u001b[32m10-25 23:54:58\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(380928,)\n",
      "\u001b[32m10-25 23:54:58\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、よろしくお願いします。なんだか最近、ずっと気持ちが落ち着かなくて…漠然とした不安と焦りがあって、どうしたらいいのか分からなくて。\n",
      "\u001b[32m10-25 23:54:59\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(459264,)\n",
      "\u001b[32m10-25 23:54:59\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうですか…ずっと気持ちが落ち着かなくて、漠然とした不安と焦りを感じていらっしゃるんですね。ええ、そういったお気持ち、とてもよく分かります。具体的に、どんな時にそう感じることが多いですか？\n",
      "\u001b[32m10-25 23:54:59\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(671744,)\n",
      "\u001b[32m10-25 23:54:59\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "仕事中は、常に締め切りに追われているような感覚で、ミスがないか、もっとできるんじゃないかって。家に帰っても、あれこれ考えてしまって、なかなか休まらないんです。\n",
      "\u001b[32m10-25 23:54:59\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(544256,)\n",
      "\u001b[32m10-25 23:54:59\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど、仕事中も、そしてお家に帰られてからも、ずっと気が休まらない状態が続いているんですね。そうなんですね…お辛いですね。普段の生活の中で、何か変化があったり、特に気になっていることなどはありませんか？\n",
      "\u001b[32m10-25 23:55:00\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(788992,)\n",
      "\u001b[32m10-25 23:55:00\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうですね…以前は休日には趣味を楽しんだりできていたんですけど、最近はそれすら億劫で。このままだと、何もかも手につかなくなるんじゃないかって、また焦りが募るんです。\n",
      "\u001b[32m10-25 23:55:00\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(548352,)\n",
      "\u001b[32m10-25 23:55:00\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、そうなんですね。休日に趣味を楽しむことも億劫になってしまうと、またそれが焦りに繋がってしまう…悪循環のようにも感じられますね。漠然とした不安や焦り、というのは、具体的にどんな風な感情に近いんでしょうか？ もし言葉にしにくいようでしたら、無理にとは言いませんが…\n",
      "\u001b[32m10-25 23:55:00\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(970240,)\n",
      "\u001b[32m10-25 23:55:00\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "うーん…何というか、自分だけが立ち止まっているような感覚というか…周りはみんな前に進んでいるのに、自分はここで足踏みしているんじゃないか、置いていかれるんじゃないかって。それが、漠然とした不安に繋がっている気がします。\n",
      "\u001b[32m10-25 23:55:01\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(714240,)\n",
      "\u001b[32m10-25 23:55:01\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど…周りの方とご自身を比べてしまって、自分だけが立ち止まっているような、置いていかれているような感覚があるんですね。それは、とても苦しい気持ちですね。そういった、ご自身への期待のようなものも、大きな重圧になっているのかもしれませんね。\n",
      "\u001b[32m10-25 23:55:01\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(887808,)\n",
      "\u001b[32m10-25 23:55:01\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、そうなんです。自分で自分にプレッシャーをかけている部分もあるのかもしれません。\n",
      "\u001b[32m10-25 23:55:01\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(236032,)\n",
      "\u001b[32m10-25 23:55:01\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、ありがとうございます。今日はお話ししてくださって、少しずつ〇〇さんの感じていることの輪郭が見えてきたように思います。漠然とした不安の背景には、仕事のこと、そしてご自身の将来や周りの方との比較、そういった様々な思いが絡み合っているのかもしれませんね。\n",
      "\u001b[32m10-25 23:55:02\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(907264,)\n",
      "\u001b[32m10-25 23:55:02\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "今日はここまでにさせていただきますが、お話しいただいたことを踏まえて、次回はもう少し、〇〇さんが感じているその『焦り』や『不安』について、具体的にどんな時に、どんな風に感じるのか、もう少し詳しくお聞かせいただけますでしょうか。何かご質問はありますか？\n",
      "\u001b[32m10-25 23:55:02\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(821248,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:02:47\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_46...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_46\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m280.174\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:46\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_46...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_46\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m158.689\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-26 00:02:50\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こんにちは、今日は来てくださってありがとうございます。少しお話を伺ってもよろしいでしょうか？\n",
      "\u001b[32m10-26 00:02:50\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(285184,)\n",
      "\u001b[32m10-26 00:02:50\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、よろしくお願いします。最近、なんだか漠然とした不安というか、焦りを感じることが多くて…\n",
      "\u001b[32m10-26 00:02:51\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(336384,)\n",
      "\u001b[32m10-26 00:02:51\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうなんですね。漠然とした不安や焦りを感じていらっしゃるんですね。ええ、もう少し詳しくお聞かせいただけますか？\n",
      "\u001b[32m10-26 00:02:51\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(365056,)\n",
      "\u001b[32m10-26 00:02:51\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。仕事でもプライベートでも、何をしていても「これでいいのかな」とか、「もっと何かできるはずなのに」と思ってしまって。具体的に何が原因なのか、自分でもよくわからないんです。\n",
      "\u001b[32m10-26 00:02:51\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(585728,)\n",
      "\u001b[32m10-26 00:02:51\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど、「これでいいのかな」という気持ちや、「もっとできるはず」という焦りを感じるのですね。ええ、ご自身でも原因がはっきりしない中で、そのお気持ちが続いているのは、お辛いですね。最近、特にそう感じることが増えた、といったことはありますか？\n",
      "\u001b[32m10-26 00:02:51\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(835072,)\n",
      "\u001b[32m10-26 00:02:51\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうですね… 半年くらい前からでしょうか。仕事が忙しくなって、自分の時間が減ったあたりから、どんどんひどくなってきた気がします。夜もなかなか寝付けなくて、朝もスッキリしない日が多くて。\n",
      "\u001b[32m10-26 00:02:52\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(599040,)\n",
      "\u001b[32m10-26 00:02:52\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、半年ほど前から、お仕事の忙しさとともに、そのお気持ちが強くなってきたんですね。睡眠にも影響が出ている、ということでしたか。それは大変お辛い状況ですね。そうなんですね。もし差し支えなければ、その「仕事の忙しさ」というのは、具体的にどのような状況なのでしょうか？\n",
      "\u001b[32m10-26 00:02:52\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(979456,)\n",
      "\u001b[32m10-26 00:02:52\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "新しいプロジェクトを任されたんですが、責任が重くて、プレッシャーを感じています。常に締め切りに追われているような感覚で、家に帰っても仕事のことばかり考えてしまいます。\n",
      "\u001b[32m10-26 00:02:52\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(522240,)\n",
      "\u001b[32m10-26 00:02:52\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、新しいプロジェクトでの責任とプレッシャー、そして常に締め切りに追われているような感覚なんですね。ご自宅にいても仕事のことが頭から離れないというのは、心休まる時間が少ないということでしょうか。なるほど。今日はお話ししてくださって、ありがとうございます。漠然とした不安や焦り、それが仕事の状況と関連していること、そして睡眠にも影響が出ていること、少し見えてきたように思います。\n",
      "\u001b[32m10-26 00:02:53\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(1363968,)\n",
      "\u001b[32m10-26 00:02:53\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "今日はここまでとさせていただきますが、もしよろしければ、また次回、もう少し詳しくお話を伺う時間をいただけないでしょうか？\n",
      "\u001b[32m10-26 00:02:53\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(401408,)\n",
      "\u001b[32m10-26 00:02:53\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、ぜひお願いします。少し話せただけでも、少し楽になった気がします。\n",
      "\u001b[32m10-26 00:02:53\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(251392,)\n",
      "\u001b[32m10-26 00:02:53\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "それはよかったです。次回、またお話しできることを楽しみにしていますね。お気をつけてお帰りください。\n",
      "\u001b[32m10-26 00:02:54\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(323584,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:33\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:04\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_47...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_47\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m148.219\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:32\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:06\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_47...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_47\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m147.715\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-26 00:08:28\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こんにちは。今日はどんなことについてお話ししましょうか。何か気になっていることはありますか？\n",
      "\u001b[32m10-26 00:08:28\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(286720,)\n",
      "\u001b[32m10-26 00:08:28\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、なんだか最近、常に『〜しなきゃいけない』って考えてしまうんです。仕事でもプライベートでも、もっと頑張らなきゃって…\n",
      "\u001b[32m10-26 00:08:29\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(384000,)\n",
      "\u001b[32m10-26 00:08:29\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうなんですね。常に『〜しなきゃいけない』と感じていらっしゃるんですね。ええ。もう少し詳しくお聞かせいただけますか？\n",
      "\u001b[32m10-26 00:08:29\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(357376,)\n",
      "\u001b[32m10-26 00:08:29\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "例えば、仕事では「この資料は完璧に作らないとダメだ」とか、休みの日でも「何か生産的なことをしないと時間がもったいない」とか…\n",
      "\u001b[32m10-26 00:08:29\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(431616,)\n",
      "\u001b[32m10-26 00:08:29\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど。完璧にしないとダメ、とか、生産的なことをしないと時間がもったいない、と感じるのですね。はい。そう感じるとき、どんなお気持ちになりますか？\n",
      "\u001b[32m10-26 00:08:30\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(511488,)\n",
      "\u001b[32m10-26 00:08:30\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "プレッシャーを感じますし、もしできなかったら、自分はダメな人間だって落ち込んでしまいます。結局何も手につかなくて、余計に焦るんです。\n",
      "\u001b[32m10-26 00:08:30\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(408576,)\n",
      "\u001b[32m10-26 00:08:30\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、プレッシャーを感じて、できなかったときに自分を責めてしまうのですね。そして、それがまた焦りにつながってしまう、ということでしたか。\n",
      "\u001b[32m10-26 00:08:30\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(451584,)\n",
      "\u001b[32m10-26 00:08:30\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、まさにその通りです。この悪循環から抜け出したいんですけど、どうしたらいいのかわからなくて。\n",
      "\u001b[32m10-26 00:08:30\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(310784,)\n",
      "\u001b[32m10-26 00:08:30\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうなんですね。この『〜しなきゃいけない』というお気持ち、とても強く感じていらっしゃるのですね。はい。少し、そのお気持ちがどこから来ているのか、一緒に考えてみませんか？\n",
      "\u001b[32m10-26 00:08:31\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(549888,)\n",
      "\u001b[32m10-26 00:08:31\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、ぜひ。\n",
      "\u001b[32m10-26 00:08:31\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(47104,)\n",
      "\u001b[32m10-26 00:08:31\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ありがとうございます。今日は、Bさんが『〜しなきゃいけない』と感じる気持ちが、どのようにBさんを苦しめているのか、少し具体的に見ることができましたね。次回もこの部分をもう少し深掘りして、Bさんが少しでも楽になるための方法を一緒に探していきましょう。\n",
      "\u001b[32m10-26 00:08:31\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(837120,)\n",
      "\u001b[32m10-26 00:08:31\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、よろしくお願いします。\n",
      "\u001b[32m10-26 00:08:31\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(102912,)\n",
      "\u001b[32m10-26 00:08:31\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ。では、今日のところはこれで終わりにしましょう。お疲れ様でした。\n",
      "\u001b[32m10-26 00:08:32\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(228864,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:24\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_48...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_48\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m133.620\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:23\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:04\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_48...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_48\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m132.113\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-26 00:13:52\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こんにちは。今日はよくいらっしゃいましたね。今、どんなことを感じていますか？ゆっくりお聞かせください。\n",
      "\u001b[32m10-26 00:13:52\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(330752,)\n",
      "\u001b[32m10-26 00:13:52\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、最近どうも気分が晴れなくて。仕事もプライベートも、漠然とした焦りを感じています。\n",
      "\u001b[32m10-26 00:13:52\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(320512,)\n",
      "\u001b[32m10-26 00:13:52\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "漠然とした焦り、ということでしたか。ええ、それはお辛いですね。どんな時にそう感じますか？\n",
      "\u001b[32m10-26 00:13:53\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(334336,)\n",
      "\u001b[32m10-26 00:13:53\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "仕事中は、『もっとできるはずなのに』とか、『周りに遅れているんじゃないか』って。家に帰ってもモヤモヤが続いてしまって。\n",
      "\u001b[32m10-26 00:13:53\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(377856,)\n",
      "\u001b[32m10-26 00:13:53\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど、仕事中も、そしてお家に帰ってからもその気持ちが続くのですね。うんうん。何か特定のきっかけはありますか？\n",
      "\u001b[32m10-26 00:13:53\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(410112,)\n",
      "\u001b[32m10-26 00:13:53\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "会議で自分の意見が言えなかったり、同僚がテキパキしているのを見たり…。些細なことでどんどん自信をなくします。\n",
      "\u001b[32m10-26 00:13:53\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(343040,)\n",
      "\u001b[32m10-26 00:13:53\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、そうなんですね。周りの方と比べてしまうことで、自信をなくしてしまうのですね。なるほど、よく分かります。\n",
      "\u001b[32m10-26 00:13:54\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(403968,)\n",
      "\u001b[32m10-26 00:13:54\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。それで、休日もなかなかリフレッシュできず、『このままで本当にいいのかな』と、いつも考えてしまいます。\n",
      "\u001b[32m10-26 00:13:54\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(356864,)\n",
      "\u001b[32m10-26 00:13:54\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "休日もゆっくり休めないでいらっしゃるのですね。そうでしたか。常に頭の片隅で考えてしまうのは、お辛いですね。\n",
      "\u001b[32m10-26 00:13:54\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(402944,)\n",
      "\u001b[32m10-26 00:13:54\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ。どうしたら、この気持ちが少しでも楽になる方法があれば…と。\n",
      "\u001b[32m10-26 00:13:54\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(232960,)\n",
      "\u001b[32m10-26 00:13:54\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうですね。今日は、お仕事の焦りや不安が、日常生活にも影響していることについて、お話しくださいましたね。大切なことです。\n",
      "\u001b[32m10-26 00:13:55\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(454656,)\n",
      "\u001b[32m10-26 00:13:55\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "次回、もう少し詳しくお伺いできたらと思います。また来週の同じ時間でよろしいでしょうか？\n",
      "\u001b[32m10-26 00:13:55\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(295936,)\n",
      "\u001b[32m10-26 00:13:55\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、ありがとうございます。来週もよろしくお願いします。\n",
      "\u001b[32m10-26 00:13:55\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(196608,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:21\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:04\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_49...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_49\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m130.572\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:17\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:04\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_49...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_49\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m126.725\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-26 00:18:53\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "〇〇さん、こんにちは。今日はこれまでのセッションを振り返りながら、そろそろ終結に向けてのお話を少しずつしていきましょうか。いかがですか？\n",
      "\u001b[32m10-26 00:18:53\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(444928,)\n",
      "\u001b[32m10-26 00:18:53\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、お願いします。早いもので、もうそんな時期なんですね。あっという間でした。\n",
      "\u001b[32m10-26 00:18:53\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(294912,)\n",
      "\u001b[32m10-26 00:18:53\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、そうですね。初めてこちらにいらした頃と比べて、ご自身の中でどんな変化を感じていらっしゃいますか？\n",
      "\u001b[32m10-26 00:18:54\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(360960,)\n",
      "\u001b[32m10-26 00:18:54\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "うーん…以前は漠然とした不安ばかりで、仕事でも私生活でもどうしたらいいか分からなかったんですけど、今は少しずつ自分の気持ちを整理できるようになってきた気がします。\n",
      "\u001b[32m10-26 00:18:54\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(523264,)\n",
      "\u001b[32m10-26 00:18:54\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど、気持ちの整理ができるようになった、と感じていらっしゃるんですね。それは素晴らしいことですね。何か、具体的に「これは変わったな」と感じることはありますか？\n",
      "\u001b[32m10-26 00:18:54\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(520704,)\n",
      "\u001b[32m10-26 00:18:54\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。例えば、職場で何か問題が起きても、すぐに「もうダメだ」って落ち込むのではなく、「これはどういう状況なんだろう」って一度立ち止まって考えるようになりました。前はただ焦るばかりで…\n",
      "\u001b[32m10-26 00:18:55\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(626176,)\n",
      "\u001b[32m10-26 00:18:55\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうなんですね。それは大きな変化ですね。一度立ち止まって、客観的に捉えることができるようになった、ということでしょうか。ええ。\n",
      "\u001b[32m10-26 00:18:55\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(434176,)\n",
      "\u001b[32m10-26 00:18:55\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。それと、自分の感情を「不安だ」とか「これは悔しい」って言葉にできるようになって、同僚や家族に相談することも少しずつできるようになりました。以前はそれすら難しくて。\n",
      "\u001b[32m10-26 00:18:55\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(595456,)\n",
      "\u001b[32m10-26 00:18:55\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、素晴らしいですね。ご自身の感情を認識し、周りの方と共有できるようになられたのですね。ご自身の努力の賜物だと思いますよ。\n",
      "\u001b[32m10-26 00:18:55\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(485376,)\n",
      "\u001b[32m10-26 00:18:55\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ありがとうございます。そう言っていただけると嬉しいです。ただ、これでセッションが終わってしまうと思うと、少しだけ、また不安が出てくるような気もして…\n",
      "\u001b[32m10-26 00:18:56\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(454144,)\n",
      "\u001b[32m10-26 00:18:56\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうですね。そう感じられるのも自然なことだと思います。これまでご自身で築き上げてきたもの、そして身につけた考え方や対処法は、これからも〇〇さんご自身の中にある大切な力ですから。これからも大切に、ご自身のペースで続けていかれることが何よりも大切ですよ。\n",
      "\u001b[32m10-26 00:18:56\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(925696,)\n",
      "\u001b[32m10-26 00:18:56\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、そうですね…頑張ります。\n",
      "\u001b[32m10-26 00:18:56\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(136704,)\n",
      "\u001b[32m10-26 00:18:56\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ。今日の振り返りで、ご自身がどれだけ変化されたか、改めて感じていただけたかと思います。次回が最終回となりますが、それまでに何か気になることや話したいことがあれば、またお話しくださいね。\n",
      "\u001b[32m10-26 00:18:57\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(675328,)\n",
      "\u001b[32m10-26 00:18:57\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、ありがとうございます。\n",
      "\u001b[32m10-26 00:18:57\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(97792,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:30\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:04\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_50...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_50\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m145.724\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:34\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:04\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_50...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_50\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m142.861\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-26 00:24:52\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "〇〇さん、こんにちは。今日はどんなことをお話ししたいですか？\n",
      "\u001b[32m10-26 00:24:52\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(191488,)\n",
      "\u001b[32m10-26 00:24:52\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こんにちは。漠然とした不安や焦りがあって…少し話せたらと。\n",
      "\u001b[32m10-26 00:24:52\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(235008,)\n",
      "\u001b[32m10-26 00:24:52\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "不安や焦り、ですか。そうなんですね。どんな時に感じることが多いですか？\n",
      "\u001b[32m10-26 00:24:53\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(251392,)\n",
      "\u001b[32m10-26 00:24:53\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "仕事中も、家でも落ち着かなくて…何をしてても「これでいいのかな」って。\n",
      "\u001b[32m10-26 00:24:53\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(279552,)\n",
      "\u001b[32m10-26 00:24:53\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど。お仕事だけでなくお家でも、ですか。いつ頃からでしょう？\n",
      "\u001b[32m10-26 00:24:53\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(239104,)\n",
      "\u001b[32m10-26 00:24:53\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "半年くらい前から、じわじわと。空回りしているような気がして。\n",
      "\u001b[32m10-26 00:24:53\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(199168,)\n",
      "\u001b[32m10-26 00:24:53\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "うんうん、半年ほど前から。お辛いですね。何かきっかけは？\n",
      "\u001b[32m10-26 00:24:54\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(216576,)\n",
      "\u001b[32m10-26 00:24:54\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "最近、新しいプロジェクトを任されて。プレッシャーはあります。でも、それだけじゃない気が。\n",
      "\u001b[32m10-26 00:24:54\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(282112,)\n",
      "\u001b[32m10-26 00:24:54\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど。プロジェクトのプレッシャーだけではないと。複雑に絡んでいる感じでしょうか。\n",
      "\u001b[32m10-26 00:24:54\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(272384,)\n",
      "\u001b[32m10-26 00:24:54\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうですね。何が原因か分からなくて…正直、途方に暮れています。\n",
      "\u001b[32m10-26 00:24:54\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(257024,)\n",
      "\u001b[32m10-26 00:24:54\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、お気持ちわかります。まずは、漠然とした気持ちを一緒に整理していきましょう。\n",
      "\u001b[32m10-26 00:24:55\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(266240,)\n",
      "\u001b[32m10-26 00:24:55\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、ありがとうございます。お願いします。\n",
      "\u001b[32m10-26 00:24:55\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(160768,)\n",
      "\u001b[32m10-26 00:24:55\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ。今日は大切な一歩でしたね。次回、さらに詳しく伺えたらと思いますが、いかがですか。\n",
      "\u001b[32m10-26 00:24:55\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(322048,)\n",
      "\u001b[32m10-26 00:24:55\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、もちろんです。\n",
      "\u001b[32m10-26 00:24:55\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(77312,)\n",
      "\u001b[32m10-26 00:24:55\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、かしこまりました。では、次回の予約についてですが…\n",
      "\u001b[32m10-26 00:24:56\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(210432,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:17\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_51...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_51\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m127.612\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:12\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:04\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_51...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_51\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m125.968\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-26 00:30:18\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "〇〇さん、こんにちは。今日はセッションを振り返り、今後についてお話しできればと思います。\n",
      "\u001b[32m10-26 00:30:18\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(289792,)\n",
      "\u001b[32m10-26 00:30:18\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。もうそんな時期なんですね。少し寂しい気持ちです。\n",
      "\u001b[32m10-26 00:30:18\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(211456,)\n",
      "\u001b[32m10-26 00:30:18\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ。この期間、ご自身でどんな変化を感じていますか？\n",
      "\u001b[32m10-26 00:30:18\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(183808,)\n",
      "\u001b[32m10-26 00:30:18\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "漠然とした不安が、何に困っているか整理できた気がします。\n",
      "\u001b[32m10-26 00:30:19\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(198144,)\n",
      "\u001b[32m10-26 00:30:19\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど。その気づきは、どう役立っていますか？\n",
      "\u001b[32m10-26 00:30:19\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(155136,)\n",
      "\u001b[32m10-26 00:30:19\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "不安で動けなかったことが、『こうしてみよう』と思えるように。\n",
      "\u001b[32m10-26 00:30:19\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(167936,)\n",
      "\u001b[32m10-26 00:30:19\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "素晴らしいですね。ご自身で解決策を見つけられるように。\n",
      "\u001b[32m10-26 00:30:20\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(190464,)\n",
      "\u001b[32m10-26 00:30:20\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ありがとうございます。ただ、新しい環境ではまだ不安もあって。\n",
      "\u001b[32m10-26 00:30:20\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(200704,)\n",
      "\u001b[32m10-26 00:30:20\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうですね。でも、〇〇さんは乗り越える力をたくさん持っています。\n",
      "\u001b[32m10-26 00:30:20\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(194560,)\n",
      "\u001b[32m10-26 00:30:20\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。少し自信が持てました。自分で実践していきたいです。\n",
      "\u001b[32m10-26 00:30:20\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(216064,)\n",
      "\u001b[32m10-26 00:30:20\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "その前向きな気持ちがあれば、きっと大丈夫です。今日は良い振り返りができましたね。\n",
      "\u001b[32m10-26 00:30:21\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(268800,)\n",
      "\u001b[32m10-26 00:30:21\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "また何かあれば、いつでも相談してくださいね。\n",
      "\u001b[32m10-26 00:30:21\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(153088,)\n",
      "\u001b[32m10-26 00:30:21\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、本当にありがとうございました。\n",
      "\u001b[32m10-26 00:30:21\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(121344,)\n",
      "\u001b[32m10-26 00:30:21\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "いえ、それでは、今日は終わりにしましょうか。\n",
      "\u001b[32m10-26 00:30:21\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(136192,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:12\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_52...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_52\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m124.057\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:09\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_52...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_52\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m119.016\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-26 00:35:39\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "〇〇さん、前回のカウンセリングから、『〜すべき』と感じたことについて、何かお話しいただけますか？\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: JPCommonLabel_push_word() in jpcommon_label.c: First mora should not be long vowel symbol.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-26 00:35:39\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(292864,)\n",
      "\u001b[32m10-26 00:35:39\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。『もっと頑張るべき』とか、『完璧にすべき』って、いつも頭にあります。そうしないと、周りに迷惑をかけるんじゃないかと…。\n",
      "\u001b[32m10-26 00:35:40\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(399872,)\n",
      "\u001b[32m10-26 00:35:40\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど、『もっと頑張るべき』『完璧にすべき』という考えが、常に〇〇さんの中にあるんですね。どんな時に特に強く感じますか？\n",
      "\u001b[32m10-26 00:35:40\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(404992,)\n",
      "\u001b[32m10-26 00:35:40\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "仕事や家事で、きちんとやらなきゃと思う時です。少しでも手が抜けたら、もうダメだと落ち込んでしまって…。\n",
      "\u001b[32m10-26 00:35:40\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(337408,)\n",
      "\u001b[32m10-26 00:35:40\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、手が抜けたら『ダメだ』と感じるのですね。そうした『〜すべき』という考え、今どう影響していますか？\n",
      "\u001b[32m10-26 00:35:41\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(334848,)\n",
      "\u001b[32m10-26 00:35:41\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "いつも焦っているような気分です。休む時も、本当に休んでていいのかと、落ち着かなくて。\n",
      "\u001b[32m10-26 00:35:41\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(289792,)\n",
      "\u001b[32m10-26 00:35:41\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど、常に焦りや落ち着かなさを感じているのですね。もし、その『〜すべき』を少し緩められたら、どうでしょう？\n",
      "\u001b[32m10-26 00:35:41\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(363008,)\n",
      "\u001b[32m10-26 00:35:41\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "うーん…。少しは楽になるのかな、と。でも、何もできなくなる不安もあって…。\n",
      "\u001b[32m10-26 00:35:41\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(272384,)\n",
      "\u001b[32m10-26 00:35:41\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、楽になる一方で不安も感じられるのですね。この『〜すべき』という考え方を見つめ直すことで、焦りや不安が和らぐかもしれません。\n",
      "\u001b[32m10-26 00:35:41\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(449536,)\n",
      "\u001b[32m10-26 00:35:41\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうですね…。少し、考えてみます。\n",
      "\u001b[32m10-26 00:35:42\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(119808,)\n",
      "\u001b[32m10-26 00:35:42\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。まずはそう感じることが大切です。次回までに、普段『〜すべき』と感じたことをメモしてみませんか？一緒に見ていきましょう。\n",
      "\u001b[32m10-26 00:35:42\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(399872,)\n",
      "\u001b[32m10-26 00:35:42\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、わかりました。やってみます。\n",
      "\u001b[32m10-26 00:35:42\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(141824,)\n",
      "\u001b[32m10-26 00:35:42\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ありがとうございます。今日のところは、『〜すべき』という考え方と焦りや不安の繋がりについて、少し理解が深まったかと思います。次回は来週の同じ時間でよろしかったでしょうか？\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: JPCommonLabel_push_word() in jpcommon_label.c: First mora should not be long vowel symbol.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-26 00:35:42\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(560640,)\n",
      "\u001b[32m10-26 00:35:42\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、大丈夫です。よろしくお願いします。\n",
      "\u001b[32m10-26 00:35:43\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(145408,)\n",
      "\u001b[32m10-26 00:35:43\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、こちらこそ。今日はありがとうございました。\n",
      "\u001b[32m10-26 00:35:43\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(152576,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:21\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_53...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_53\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m132.208\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:22\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:04\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_53...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_53\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m135.078\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-26 00:40:59\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こんにちは、〇〇さん。今日は来てくださってありがとうございます。どうぞ楽にしてくださいね。ええ、今日はどんなお話しをされますか？\n",
      "\u001b[32m10-26 00:40:59\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(391168,)\n",
      "\u001b[32m10-26 00:40:59\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こんにちは。はい、ありがとうございます。えっと…最近、なんだか漠然と不安な気持ちが続いていて、どうしたらいいのか分からなくて…。\n",
      "\u001b[32m10-26 00:40:59\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(475648,)\n",
      "\u001b[32m10-26 00:40:59\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "漠然とした不安、なのですね。ええ、ありがとうございます。そうしたお気持ちが続いていらっしゃるのですね。もう少し詳しく、どんな時にそう感じることが多いですか？\n",
      "\u001b[32m10-26 00:41:00\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(555008,)\n",
      "\u001b[32m10-26 00:41:00\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうですね…特に仕事中はそう感じることが多いです。いつも時間に追われている感じで、焦ってしまうというか。家でも、次に何をすればいいか分からなくなって、何も手につかないこともあります…。\n",
      "\u001b[32m10-26 00:41:00\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(629248,)\n",
      "\u001b[32m10-26 00:41:00\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど、お仕事中に焦りを感じたり、お家にいらっしゃる時も、次どうしたらいいのか分からなくなってしまうこともあるのですね。はい、それはお辛いですね。そうした時、〇〇さんご自身は、どんなことを考えていらっしゃいますか？\n",
      "\u001b[32m10-26 00:41:00\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(721408,)\n",
      "\u001b[32m10-26 00:41:00\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "考えですか…『自分はちゃんとできていないんじゃないか』とか、『このままでいいのかな』とか、そんなことばかり考えてしまいます。周りの人はもっとしっかりしているのに、自分だけ置いていかれているような気がして…。\n",
      "\u001b[32m10-26 00:41:01\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(615424,)\n",
      "\u001b[32m10-26 00:41:01\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "『ちゃんとできていないんじゃないか』『このままでいいのかな』と、そう思われるのですね。はい。周りの方と比べてしまって、置いていかれているようなお気持ちになる、と。そうなんですね。それはとてもしんどいことですよね。そういったお気持ちは、日常生活全般に影響しているように感じますか？\n",
      "\u001b[32m10-26 00:41:01\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(961024,)\n",
      "\u001b[32m10-26 00:41:01\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、まさにそうです。何をするにも自信がなくて、結局何も進まない日もあったりして。この状況をどうにかしたいと思っているんですけど…。\n",
      "\u001b[32m10-26 00:41:01\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(456704,)\n",
      "\u001b[32m10-26 00:41:01\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、そうですね。自信が持てなくて、なかなか物事が進まない状況なのですね。今日はお話ししてくださって、ありがとうございます。ご自身の気持ちをこうして言葉にしてくださったことが、大きな一歩だと思います。今日は、〇〇さんが感じていらっしゃる漠然とした不安や、お仕事での焦りについてお話しいただきました。次回、そうしたお気持ちの背景をもう少し一緒に見ていけたらと思います。またお話しを伺ってもよろしいでしょうか？\n",
      "\u001b[32m10-26 00:41:02\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(1392128,)\n",
      "\u001b[32m10-26 00:41:02\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、ぜひお願いします。今日少し話せただけでも、少し気持ちが楽になった気がします。\n",
      "\u001b[32m10-26 00:41:02\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(297984,)\n",
      "\u001b[32m10-26 00:41:02\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "それはよかったです。はい、次回は〇〇さんのペースで、またゆっくりお話しを聞かせてくださいね。では、今日はこれで終わりにしましょうか。\n",
      "\u001b[32m10-26 00:41:02\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(427008,)\n",
      "\u001b[32m10-26 00:41:02\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、ありがとうございました。\n",
      "\u001b[32m10-26 00:41:03\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(103424,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:37\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:04\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_54...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_54\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m145.413\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:39\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:04\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_54...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_54\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m149.257\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-26 00:46:29\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "〇〇さん、こんにちは。今日はこれまでのカウンセリングを振り返りながら、そろそろ終結に向けてのお話ができればと思うのですが、いかがでしょうか？\n",
      "\u001b[32m10-26 00:46:29\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(451584,)\n",
      "\u001b[32m10-26 00:46:29\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、承知しています。なんだかあっという間だったような、でもすごく濃い時間だったような気がします。\n",
      "\u001b[32m10-26 00:46:30\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(317440,)\n",
      "\u001b[32m10-26 00:46:30\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうなんですね。この数ヶ月、〇〇さんが感じていらっしゃった漠然とした不安や焦りについて、色々な角度からお話ししてきましたね。ええ、ご自身の中で、何か変化を感じることはありますか？\n",
      "\u001b[32m10-26 00:46:30\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(621568,)\n",
      "\u001b[32m10-26 00:46:30\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。以前は、職場の人間関係とか、将来のこととか、考え出すと悪い方にばかり考えてしまって、どうしようもなかったんですけど…今は、少し冷静に「これは自分の認知だ」って思えるようになりました。\n",
      "\u001b[32m10-26 00:46:30\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(644096,)\n",
      "\u001b[32m10-26 00:46:30\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "「自分の認知だ」と、ええ、そう思えるようになったのですね。それは大きな変化ですね。具体的に、どんな時にそう感じますか？\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: JPCommonLabel_insert_pause() in jpcommon_label.c: First mora should not be short pause.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-26 00:46:30\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(423424,)\n",
      "\u001b[32m10-26 00:46:30\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "例えば、職場でちょっとしたミスがあった時、前なら「また失敗した、自分はダメだ」ってすぐ落ち込んでたんですけど、今は「これは失敗だけど、次にどう活かそうか」って、感情的になる前に考えられるようになったというか。\n",
      "\u001b[32m10-26 00:46:31\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(659456,)\n",
      "\u001b[32m10-26 00:46:31\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど。以前の「ダメだ」という認知から、改善策を考える行動に繋がっているのですね。はい、それは素晴らしいことですね。ご自身でも、そうした変化を実感されているのですね。\n",
      "\u001b[32m10-26 00:46:31\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(615424,)\n",
      "\u001b[32m10-26 00:46:31\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。完全に不安がなくなったわけじゃないですけど、前よりずっと楽になりました。ただ、この状態を自分で維持できるか、少し心配な気持ちもあります。\n",
      "\u001b[32m10-26 00:46:31\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(478208,)\n",
      "\u001b[32m10-26 00:46:31\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、そうですよね。不安が完全になくなることは、なかなか難しいことですし、心配になるお気持ちもよくわかります。これまで〇〇さんが取り組んでこられた「認知」や「行動」を整理する作業は、きっとこれからもご自身の力になると思いますよ。\n",
      "\u001b[32m10-26 00:46:32\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(800768,)\n",
      "\u001b[32m10-26 00:46:32\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そう言っていただけると、少し安心します。図や表で整理した時のことを思い出して、自分でできることを続けていきたいです。\n",
      "\u001b[32m10-26 00:46:32\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(360960,)\n",
      "\u001b[32m10-26 00:46:32\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、そうですね。これまで身につけられた考え方や、具体的な対処法を、ぜひこれからもご自身のペースで続けていってください。今回のセッションは、〇〇さんご自身が「悪循環」から抜け出すための糸口を見つけられた、本当に実り多い時間だったと感じています。\n",
      "\u001b[32m10-26 00:46:32\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(855552,)\n",
      "\u001b[32m10-26 00:46:32\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ありがとうございます。本当に助けられました。\n",
      "\u001b[32m10-26 00:46:33\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(140800,)\n",
      "\u001b[32m10-26 00:46:33\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、こちらこそ、〇〇さんがご自身の問題に真剣に取り組んでくださったからこその成果だと思います。次回のセッションで、今回の振り返りも踏まえ、今後についてさらに具体的に話し合いましょうか。\n",
      "\u001b[32m10-26 00:46:33\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(628224,)\n",
      "\u001b[32m10-26 00:46:33\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、お願いします。\n",
      "\u001b[32m10-26 00:46:33\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(73216,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:33\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_55...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_55\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m156.248\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:36\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_55...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_55\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m158.674\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-26 00:53:06\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "Bさん、こんにちは。今日はどうですか、お変わりありませんでしたか？\n",
      "\u001b[32m10-26 00:53:07\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(223744,)\n",
      "\u001b[32m10-26 00:53:07\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こんにちは。毎日『〜すべき』って焦ってしまって。仕事も家事も、できてない自分にモヤモヤします。\n",
      "\u001b[32m10-26 00:53:07\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(323584,)\n",
      "\u001b[32m10-26 00:53:07\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "『〜すべき』というお気持ちで焦ってしまうのですね。はい、ええ。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: JPCommonLabel_push_word() in jpcommon_label.c: First mora should not be long vowel symbol.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-26 00:53:07\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(186880,)\n",
      "\u001b[32m10-26 00:53:07\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ。休日に家事を完璧に『すべき』なのに、疲れてしまって。そうすると、ダメだって。\n",
      "\u001b[32m10-26 00:53:07\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(271360,)\n",
      "\u001b[32m10-26 00:53:07\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "できないとご自身を責めてしまうのですね。なるほど。\n",
      "\u001b[32m10-26 00:53:08\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(174080,)\n",
      "\u001b[32m10-26 00:53:08\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "友達との会話でも、『もっと面白く言うべき』って思って、結局何も話せないこともあって…。\n",
      "\u001b[32m10-26 00:53:08\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(293376,)\n",
      "\u001b[32m10-26 00:53:08\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "人間関係でも『〜すべき』が浮かぶのですね。その時、どんなお気持ちに？\n",
      "\u001b[32m10-26 00:53:08\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(229376,)\n",
      "\u001b[32m10-26 00:53:08\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "落ち込んで、情けなくなって。自分には価値がないんじゃないかって…。\n",
      "\u001b[32m10-26 00:53:08\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(219648,)\n",
      "\u001b[32m10-26 00:53:08\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "『価値がない』と感じるほどお辛いのです**ね。うん、うん。**その『〜すべき』という考えが、Bさんを苦しめているのかもしれませんね。\n",
      "\u001b[32m10-26 00:53:09\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(423424,)\n",
      "\u001b[32m10-26 00:53:09\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうかもしれません。常に頑張っていないと、って。\n",
      "\u001b[32m10-26 00:53:09\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(159744,)\n",
      "\u001b[32m10-26 00:53:09\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "今日は『〜すべき』というお気持ちが、Bさんをどんなふうに苦しめているか見えてきましたね。次回、もう少し詳しくお話しできたらと思うのですが、いかがでしょう？\n",
      "\u001b[32m10-26 00:53:09\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(486912,)\n",
      "\u001b[32m10-26 00:53:09\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、ぜひ。少し楽になれると嬉しいです。\n",
      "\u001b[32m10-26 00:53:09\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(138752,)\n",
      "\u001b[32m10-26 00:53:09\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "良かったです。では、今日のことを踏まえて、また次回お話ししましょうか。次回の予約はいつ頃がご都合よろしいですか？\n",
      "\u001b[32m10-26 00:53:09\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(381440,)\n",
      "\u001b[32m10-26 00:53:09\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "来週の同じ時間でお願いします。\n",
      "\u001b[32m10-26 00:53:10\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(97792,)\n",
      "\u001b[32m10-26 00:53:10\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "承知いたしました。では、また来週、お待ちしておりますね。\n",
      "\u001b[32m10-26 00:53:10\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(201728,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:16\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_56...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_56\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m133.753\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:15\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_56...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_56\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m134.713\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-26 00:58:48\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こんにちは、Bさん。今日はよくお越しくださいました。どのようなお悩みですか？\n",
      "\u001b[32m10-26 00:58:49\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(242688,)\n",
      "\u001b[32m10-26 00:58:49\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こんにちは。最近、仕事もそうですが、漠然と将来が不安で、焦りを感じていて...。\n",
      "\u001b[32m10-26 00:58:49\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(293376,)\n",
      "\u001b[32m10-26 00:58:49\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうなんですね。漠然とした不安や焦り、お辛いですね。もう少しお聞かせください。\n",
      "\u001b[32m10-26 00:58:49\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(298496,)\n",
      "\u001b[32m10-26 00:58:49\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。仕事は忙しいのに、これでいいのかって。日々に追われ、やりたいことができてない気がして。\n",
      "\u001b[32m10-26 00:58:49\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(327680,)\n",
      "\u001b[32m10-26 00:58:49\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど。忙しさで、「本当にやりたいこと」が見えない感覚ですか。いつ頃からでしょう？\n",
      "\u001b[32m10-26 00:58:50\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(301056,)\n",
      "\u001b[32m10-26 00:58:50\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "うーん、今の部署に異動して半年くらいからです。期待に応えなきゃと、自分の力が足りないような気がして。\n",
      "\u001b[32m10-26 00:58:50\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(339456,)\n",
      "\u001b[32m10-26 00:58:50\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "半年ほど前からでしたか。ええ。日常生活で変化はありましたか？\n",
      "\u001b[32m10-26 00:58:50\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(235520,)\n",
      "\u001b[32m10-26 00:58:50\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "前は楽しかった趣味も集中できなくて。夜も寝つきが悪くなって。このままで大丈夫かなって。\n",
      "\u001b[32m10-26 00:58:50\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(333824,)\n",
      "\u001b[32m10-26 00:58:50\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど。趣味に集中できなかったり、寝つきが悪くなったり。心身ともに疲れているのかもしれませんね。\n",
      "\u001b[32m10-26 00:58:51\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(345600,)\n",
      "\u001b[32m10-26 00:58:51\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "今日は、Bさんが感じている不安や焦り、それが仕事や日常生活に与える影響についてお話しいただきました。\n",
      "\u001b[32m10-26 00:58:51\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(364032,)\n",
      "\u001b[32m10-26 00:58:51\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "少し状況が見えてきましたね。次回、もう少し掘り下げて聞かせてください。何か心に残りましたか？\n",
      "\u001b[32m10-26 00:58:51\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(341504,)\n",
      "\u001b[32m10-26 00:58:51\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "話を聞いてもらえて、少し楽になりました。ありがとうございます。次回もよろしくお願いします。\n",
      "\u001b[32m10-26 00:58:51\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(312320,)\n",
      "\u001b[32m10-26 00:58:51\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "よかったです。では、また来週同じ曜日と時間で大丈夫でしょうか？\n",
      "\u001b[32m10-26 00:58:52\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(203264,)\n",
      "\u001b[32m10-26 00:58:52\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、大丈夫です。\n",
      "\u001b[32m10-26 00:58:52\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(77824,)\n",
      "\u001b[32m10-26 00:58:52\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ありがとうございます。気をつけてお帰りくださいね。\n",
      "\u001b[32m10-26 00:58:52\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(151552,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:19\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:06\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_57...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_57\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m140.828\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:18\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_57...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_57\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m129.709\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-26 01:03:58\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こんにちは、今日はよくお越しくださいましたね。何か、今お話ししたいことなどありますでしょうか？\n",
      "\u001b[32m10-26 01:03:58\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(293888,)\n",
      "\u001b[32m10-26 01:03:58\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、こんにちは。最近、なんだか漠然とした不安というか、焦りのようなものを感じていて…。仕事のこともそうなんですけど、日常生活でも気分が晴れないことが多くて。\n",
      "\u001b[32m10-26 01:03:59\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(580096,)\n",
      "\u001b[32m10-26 01:03:59\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうなんですね。ええ、漠然とした不安や焦り、ですか。それは、いつ頃から感じられるようになりましたか？\n",
      "\u001b[32m10-26 01:03:59\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(369152,)\n",
      "\u001b[32m10-26 01:03:59\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "いつから、と言われると難しいんですけど、半年くらい前からでしょうか。仕事で新しいプロジェクトが始まって、そのプレッシャーもあってか、なんとなくずっと落ち着かないんです。\n",
      "\u001b[32m10-26 01:03:59\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(489472,)\n",
      "\u001b[32m10-26 01:03:59\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど、新しいプロジェクトがきっかけの一つになったのですね。はい。その「落ち着かない」というのは、具体的にどのような時に強く感じられますか？\n",
      "\u001b[32m10-26 01:03:59\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(485376,)\n",
      "\u001b[32m10-26 01:03:59\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうですね…。朝、目が覚めた瞬間から「今日も一日、ちゃんとやれるかな」って考えてしまったり、休日でも、何かしないといけない気がして、ゆっくり休めないんです。\n",
      "\u001b[32m10-26 01:04:00\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(523264,)\n",
      "\u001b[32m10-26 01:04:00\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、朝からそういったお気持ちになるんですね。休日もゆっくり休めないというのは、お辛いですね。何か、そういった気持ちになることで、日常生活で困っていることはありますか？\n",
      "\u001b[32m10-26 01:04:00\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(592896,)\n",
      "\u001b[32m10-26 01:04:00\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。集中力が続かなくなって、仕事のミスが増えたり、夜もなかなか寝付けなくて…。疲れが取れないのに、また次の日が来るのが怖くて。\n",
      "\u001b[32m10-26 01:04:00\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(492032,)\n",
      "\u001b[32m10-26 01:04:00\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうなんですね、集中力や睡眠にも影響が出ているんですね。それはとてもしんどいですね。今お話ししてくださったこと、とてもよく分かりました。\n",
      "\u001b[32m10-26 01:04:01\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(515584,)\n",
      "\u001b[32m10-26 01:04:01\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい…。\n",
      "\u001b[32m10-26 01:04:01\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(22528,)\n",
      "\u001b[32m10-26 01:04:01\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "今日は、漠然とした不安や焦り、それが仕事や日常生活、そして睡眠にも影響しているというお話を伺うことができました。これまでの経験も踏まえながら、これから一緒に、Bさんの気持ちが少しでも楽になるように、具体的な対策を考えていけたらと思います。\n",
      "\u001b[32m10-26 01:04:01\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(904192,)\n",
      "\u001b[32m10-26 01:04:01\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、ありがとうございます。少し、話せてよかったです。\n",
      "\u001b[32m10-26 01:04:01\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(196096,)\n",
      "\u001b[32m10-26 01:04:01\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ。次回は、もう少し詳しく、この不安や焦りがどんな状況で強くなるのか、一緒に見ていけたらと思いますが、いかがでしょうか？\n",
      "\u001b[32m10-26 01:04:02\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(432640,)\n",
      "\u001b[32m10-26 01:04:02\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、ぜひお願いします。\n",
      "\u001b[32m10-26 01:04:02\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(89088,)\n",
      "\u001b[32m10-26 01:04:02\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "では、次回の予約を調整させていただければと思います。今日はここまでとなりますが、何か質問はありますか？\n",
      "\u001b[32m10-26 01:04:02\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(359936,)\n",
      "\u001b[32m10-26 01:04:02\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "いえ、大丈夫です。よろしくお願いします。\n",
      "\u001b[32m10-26 01:04:02\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(136192,)\n",
      "\u001b[32m10-26 01:04:02\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、こちらこそ。今日はありがとうございました。\n",
      "\u001b[32m10-26 01:04:02\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(153088,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:33\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_58...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_58\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m153.190\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:34\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_58...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_58\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m144.871\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-26 01:09:36\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "〇〇さん、こんにちは。今日は前回お話しいただいた、漠然とした不安について、もう少しお聞かせいただけますか？\n",
      "\u001b[32m10-26 01:09:36\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(372224,)\n",
      "\u001b[32m10-26 01:09:36\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、なんだか常に「もっとこうするべきだ」とか「これは完璧にやるべきだ」という気持ちに追い立てられているような気がして…それがしんどいんです。特に仕事でも、もっとできるはずなのに、と自分を責めてしまいます。\n",
      "\u001b[32m10-26 01:09:36\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(667136,)\n",
      "\u001b[32m10-26 01:09:36\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうなんですね。常に「もっとこうするべきだ」「完璧にやるべきだ」と感じていらっしゃるんですね。ええ、お辛いお気持ち、よくわかりますよ。例えば、どんな時にそう強く感じられるのでしょう？\n",
      "\u001b[32m10-26 01:09:37\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(620032,)\n",
      "\u001b[32m10-26 01:09:37\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "最近だと、資料作成で時間をかけすぎた時ですかね。もっと早く、もっと質の高いものを作るべきだった、って。結局徹夜してしまって、次の日も疲れてしまって…悪循環なんです。\n",
      "\u001b[32m10-26 01:09:37\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(642048,)\n",
      "\u001b[32m10-26 01:09:37\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど。資料作成で時間をかけすぎてしまった時に、「もっと早く、もっと質の高いものを作るべきだった」と強く感じられた、ということなんですね。はい。その「〜するべきだった」というお気持ち、もう少し詳しくお聞かせいただけますか？\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: JPCommonLabel_push_word() in jpcommon_label.c: First mora should not be long vowel symbol.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-26 01:09:37\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(763392,)\n",
      "\u001b[32m10-26 01:09:37\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "だって、周りの人はみんなもっとテキパキやってるように見えるし、自分だけが要領悪いんじゃないかって。完璧にできないと、評価もされないんじゃないかと思ってしまうんです。\n",
      "\u001b[32m10-26 01:09:37\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(472576,)\n",
      "\u001b[32m10-26 01:09:37\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、そうなんですね。周りの方と比べてしまって、「完璧にできないと評価されない」と感じてしまうのですね。そういった考えが浮かんでくると、〇〇さんはどんなお気持ちになりますか？\n",
      "\u001b[32m10-26 01:09:38\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(574976,)\n",
      "\u001b[32m10-26 01:09:38\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "すごく焦りますし、不安になります。結果的に、余計に時間がかかってしまったり、手が進まなくなったりして、もっと落ち込むんです。\n",
      "\u001b[32m10-26 01:09:38\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(377856,)\n",
      "\u001b[32m10-26 01:09:38\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、焦りや不安を感じ、それがかえって作業の効率を下げてしまう…そういった経験をされていらっしゃるんですね。もし、「完璧にやるべきだ」という考えが少し和らいだとしたら、何か気持ちに変化はありそうですか？\n",
      "\u001b[32m10-26 01:09:38\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(711680,)\n",
      "\u001b[32m10-26 01:09:38\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "うーん…そうですね。少しは楽になる、かもしれません。でも、本当にそうしてもいいのか、自信がないです。\n",
      "\u001b[32m10-26 01:09:39\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(369152,)\n",
      "\u001b[32m10-26 01:09:39\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、まだ少し自信が持てない、というお気持ちなのですね。今日のところは、「〜するべき」という考えが、〇〇さんの気持ちをどのように動かしているのか、少し見えてきたように思います。次回までに、もし可能でしたら、日常生活の中で「〜するべきだ」と感じた時に、一度立ち止まって、その考えが本当か、少しだけ考えてみてもいいかもしれませんね。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: JPCommonLabel_push_word() in jpcommon_label.c: First mora should not be long vowel symbol.\n",
      "WARNING: JPCommonLabel_push_word() in jpcommon_label.c: First mora should not be long vowel symbol.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-26 01:09:39\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(1121792,)\n",
      "\u001b[32m10-26 01:09:39\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、試してみます。なんだか、少し頭が整理された気がします。\n",
      "\u001b[32m10-26 01:09:39\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(224256,)\n",
      "\u001b[32m10-26 01:09:39\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "それは良かったです。はい。では、今日はここまでとしましょうか。次回の予約ですが、いつ頃がよろしいでしょうか？\n",
      "\u001b[32m10-26 01:09:39\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(366080,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:38\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_59...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_59\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m146.988\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h\u001b[?25l"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:39\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:06\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_59...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_59\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m150.324\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-26 01:15:22\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こんにちは、〇〇さん。今日は来てくださってありがとうございます。少しお話しにくいこともあるかもしれませんが、ゆっくりお聞かせくださいね。\n",
      "\u001b[32m10-26 01:15:22\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(408064,)\n",
      "\u001b[32m10-26 01:15:22\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、ありがとうございます。なんだか最近、ずっと気持ちが落ち着かなくて…漠然とした不安があるんです。\n",
      "\u001b[32m10-26 01:15:22\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(359424,)\n",
      "\u001b[32m10-26 01:15:22\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうなんですね。漠然とした不安を感じていらっしゃるんですね。ええ、具体的に、どんな時にそう感じることが多いですか？\n",
      "\u001b[32m10-26 01:15:23\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(391680,)\n",
      "\u001b[32m10-26 01:15:23\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "えっと、仕事をしている時もそうなんですが、休日でも、なんだか常に焦っているような…このままでいいのかな、って考えてしまって。\n",
      "\u001b[32m10-26 01:15:23\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(440832,)\n",
      "\u001b[32m10-26 01:15:23\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど、お仕事中はもちろん、お休みの日でさえも、漠然とした焦りや不安がつきまとっていらっしゃるんですね。はい。\n",
      "\u001b[32m10-26 01:15:23\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(383488,)\n",
      "\u001b[32m10-26 01:15:23\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。特に何か具体的な問題があるわけじゃないんですけど、この先どうなるんだろう、とか、周りの人はもっと頑張っているんじゃないか、とか、そういうことが頭から離れなくて。\n",
      "\u001b[32m10-26 01:15:23\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(523776,)\n",
      "\u001b[32m10-26 01:15:23\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうなんですね…周りの方と比べてしまったり、先のことを考えてしまったりする中で、ずっと緊張感が続いていらっしゃる、ということでしょうか。\n",
      "\u001b[32m10-26 01:15:24\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(479232,)\n",
      "\u001b[32m10-26 01:15:24\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、まさにそんな感じです。リラックスしようと思っても、なかなかできなくて…\n",
      "\u001b[32m10-26 01:15:24\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(261632,)\n",
      "\u001b[32m10-26 01:15:24\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "無理にリラックスしようとすると、かえって疲れてしまうこともありますよね。なるほど。今日はお話ししてくださって、ありがとうございます。漠然とした不安や焦りが、〇〇さんの日常生活にも影響していることが分かりました。\n",
      "\u001b[32m10-26 01:15:24\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(713728,)\n",
      "\u001b[32m10-26 01:15:24\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい…少し話せて、落ち着きました。\n",
      "\u001b[32m10-26 01:15:24\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(134144,)\n",
      "\u001b[32m10-26 01:15:24\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、それは良かったです。まだお話ししたいこと、たくさんあると思いますので、もしよろしければ、また来週、今日のお話の続きを伺ってもよろしいでしょうか？\n",
      "\u001b[32m10-26 01:15:25\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(512512,)\n",
      "\u001b[32m10-26 01:15:25\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、ぜひお願いします。来週も来ます。\n",
      "\u001b[32m10-26 01:15:25\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(153600,)\n",
      "\u001b[32m10-26 01:15:25\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ありがとうございます。では、来週の同じお時間で、ご予約をお取りしますね。今日は本当にありがとうございました。\n",
      "\u001b[32m10-26 01:15:25\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(381952,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:26\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_60...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_60\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m137.521\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:24\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_60...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_60\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m141.911\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-26 01:20:49\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こんにちは、〇〇さん。最近の調子はいかがですか？何か気になっていることはありますか？\n",
      "\u001b[32m10-26 01:20:49\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(260096,)\n",
      "\u001b[32m10-26 01:20:49\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こんにちは、先生。最近も仕事で「もっとこうすべきだ」とか、「完璧にこなさなきゃ」という気持ちが強くて、それが結構しんどいなと感じています。\n",
      "\u001b[32m10-26 01:20:50\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(443904,)\n",
      "\u001b[32m10-26 01:20:50\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、そうなんですね。「こうすべきだ」「完璧にこなさなきゃ」と、しんどい気持ちなのですね。\n",
      "\u001b[32m10-26 01:20:50\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(328704,)\n",
      "\u001b[32m10-26 01:20:50\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。常にそう思っていないと、ダメなんじゃないかって。少しでも手を抜くと、いけないような気がして…。\n",
      "\u001b[32m10-26 01:20:50\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(327680,)\n",
      "\u001b[32m10-26 01:20:50\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど。「ダメなんじゃないか」という気持ち、よく分かります。それは、いつ頃から特に強く感じますか？\n",
      "\u001b[32m10-26 01:20:50\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(358912,)\n",
      "\u001b[32m10-26 01:20:50\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "いつからでしょう…。学生の頃もそうでしたが、仕事をするようになって、特にですね。\n",
      "\u001b[32m10-26 01:20:51\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(257024,)\n",
      "\u001b[32m10-26 01:20:51\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、お仕事でより強く感じていらっしゃるのですね。その「完璧にこなさなければ」という考え方、〇〇さんにとって、どんな影響がありますか？\n",
      "\u001b[32m10-26 01:20:51\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(450048,)\n",
      "\u001b[32m10-26 01:20:51\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうですね…。いつも焦っていて、心が休まらないんです。失敗が怖くて、新しいことにも挑戦しづらいなと。\n",
      "\u001b[32m10-26 01:20:51\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(393216,)\n",
      "\u001b[32m10-26 01:20:51\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど、心が休まらず、挑戦も難しく感じると。はい。その考え方、良い面もあれば、辛い面もあるのかもしれませんね。\n",
      "\u001b[32m10-26 01:20:51\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(476160,)\n",
      "\u001b[32m10-26 01:20:51\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、今は辛い方が大きいです。\n",
      "\u001b[32m10-26 01:20:52\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(124416,)\n",
      "\u001b[32m10-26 01:20:52\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、そうでしたか。今日お話しいただいた「こうすべきだ」という〇〇さんの考え方は、〇〇さんの真面目さの証でもありますね。この「すべき思考」について、次回もう少し詳しく見ていきませんか？\n",
      "\u001b[32m10-26 01:20:52\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(609280,)\n",
      "\u001b[32m10-26 01:20:52\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、ぜひお願いします。楽になれたら嬉しいです。\n",
      "\u001b[32m10-26 01:20:52\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(175104,)\n",
      "\u001b[32m10-26 01:20:52\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ありがとうございます。では、今日の気づきを心に留めていただいて、次回、この「すべき思考」を具体的に掘り下げていきましょう。次回の面談は、来週の同じ時間でよろしいでしょうか？\n",
      "\u001b[32m10-26 01:20:53\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(589824,)\n",
      "\u001b[32m10-26 01:20:53\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、大丈夫です。よろしくお願いします。\n",
      "\u001b[32m10-26 01:20:53\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(145408,)\n",
      "\u001b[32m10-26 01:20:53\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、では今日はここまでとしましょう。お疲れ様でした。\n",
      "\u001b[32m10-26 01:20:53\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(181248,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:24\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:04\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_61...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_61\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m133.900\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:24\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_61...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_61\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m138.637\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-26 01:26:38\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "〇〇さん、今日はこれまでのセッションを振り返り、終結に向けてお話しできたらと思います。\n",
      "\u001b[32m10-26 01:26:38\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(271872,)\n",
      "\u001b[32m10-26 01:26:38\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。あっという間でしたが、色々変わったなと思います。\n",
      "\u001b[32m10-26 01:26:38\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(176640,)\n",
      "\u001b[32m10-26 01:26:39\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、そうですか。ご自身の中で、どんな変化を感じていますか？\n",
      "\u001b[32m10-26 01:26:39\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(223232,)\n",
      "\u001b[32m10-26 01:26:39\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "漠然とした不安ばかりでしたが、今は自分の感情に気づけるのが大きいですね。前は焦っていたのが、『これが原因でこう感じる』と冷静に分析できるようになりました。\n",
      "\u001b[32m10-26 01:26:39\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(518656,)\n",
      "\u001b[32m10-26 01:26:39\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど。感情を整理する練習が実を結んだのですね。素晴らしいです。\n",
      "\u001b[32m10-26 01:26:39\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(247808,)\n",
      "\u001b[32m10-26 01:26:39\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ありがとうございます。まさか自分がこんな風になれるとは思いませんでした。\n",
      "\u001b[32m10-26 01:26:40\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(223232,)\n",
      "\u001b[32m10-26 01:26:40\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "それは〇〇さんが真剣に向き合った結果です。今後また壁にぶつかる時、どう乗り越えられそうですか？\n",
      "\u001b[32m10-26 01:26:40\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(306688,)\n",
      "\u001b[32m10-26 01:26:40\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "『一時的』と落ち着いて、認知、感情、行動を整理すると思います。自分なりのホームワークも考えてみます。\n",
      "\u001b[32m10-26 01:26:40\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(345600,)\n",
      "\u001b[32m10-26 01:26:40\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、素晴らしい。ご自身で問題を捉え、対策を考える力がつきましたね。今日で定期セッションは終わりですが、困ったらいつでもご連絡ください。\n",
      "\u001b[32m10-26 01:26:40\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(493568,)\n",
      "\u001b[32m10-26 01:26:40\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、本当にありがとうございました。自信を持って前に進めそうです。\n",
      "\u001b[32m10-26 01:26:40\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(232960,)\n",
      "\u001b[32m10-26 01:26:40\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、よかったです。〇〇さんのこれからを心から応援しています。\n",
      "\u001b[32m10-26 01:26:40\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(203264,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:15\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:06\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_62...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_62\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m124.614\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:12\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_62...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_62\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m125.888\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-26 01:31:44\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こんにちは。Bさん、今日はどんなことをお話ししたいですか？ご様子はいかがでしたか？\n",
      "\u001b[32m10-26 01:31:44\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(279040,)\n",
      "\u001b[32m10-26 01:31:44\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こんにちは。最近、「もっとちゃんとしなきゃ」という気持ちが強くて、落ち着かないんです。\n",
      "\u001b[32m10-26 01:31:44\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(240640,)\n",
      "\u001b[32m10-26 01:31:44\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "「もっとちゃんとしなきゃ」、ですか。ええ、そうなんですね。どんな時にそう感じますか？\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: JPCommonLabel_insert_pause() in jpcommon_label.c: First mora should not be short pause.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-26 01:31:45\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(281600,)\n",
      "\u001b[32m10-26 01:31:45\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "仕事で完璧に、家事も友だちへの連絡もすぐに返さなきゃ、って。\n",
      "\u001b[32m10-26 01:31:45\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(205824,)\n",
      "\u001b[32m10-26 01:31:45\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど。その「〜しなきゃ」が強い時、どんな気持ちになりますか？\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: JPCommonLabel_push_word() in jpcommon_label.c: First mora should not be long vowel symbol.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-26 01:31:45\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(219648,)\n",
      "\u001b[32m10-26 01:31:45\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "焦って、もしできなかったらどう思われるか不安になります。\n",
      "\u001b[32m10-26 01:31:45\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(172032,)\n",
      "\u001b[32m10-26 01:31:45\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "焦りや不安ですね。もし、完璧じゃなくても、すぐに返せなくても、何か困ることはありますか？\n",
      "\u001b[32m10-26 01:31:46\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(335872,)\n",
      "\u001b[32m10-26 01:31:46\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "うーん…実際は困らないのかも。でも、そうしなきゃって思ってしまうんです。\n",
      "\u001b[32m10-26 01:31:46\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(248832,)\n",
      "\u001b[32m10-26 01:31:46\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、そうですよね。では、もし「完璧じゃなくても大丈夫」と少し自分に許可を出せたら、どう感じそうですか？\n",
      "\u001b[32m10-26 01:31:46\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(373760,)\n",
      "\u001b[32m10-26 01:31:46\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "少し、ホッとできるかもしれません。\n",
      "\u001b[32m10-26 01:31:46\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(98816,)\n",
      "\u001b[32m10-26 01:31:46\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "その「ホッとする」感覚、大切ですね。今日は、「〜すべき」というお気持ちが、Bさんを苦しめているのかもしれない、と見ていきました。いかがでしたか？\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: JPCommonLabel_push_word() in jpcommon_label.c: First mora should not be long vowel symbol.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-26 01:31:47\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(503296,)\n",
      "\u001b[32m10-26 01:31:47\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "自分を追い詰めているのは、「〜しなきゃ」という気持ちだと気づけました。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: JPCommonLabel_push_word() in jpcommon_label.c: First mora should not be long vowel symbol.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-26 01:31:47\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(191488,)\n",
      "\u001b[32m10-26 01:31:47\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうなんですね。次回までに、もし「〜すべき」と感じた時に、ご自身の気持ちを少し観察してみていただけますか？\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: JPCommonLabel_push_word() in jpcommon_label.c: First mora should not be long vowel symbol.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-26 01:31:47\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(375296,)\n",
      "\u001b[32m10-26 01:31:47\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、やってみます。\n",
      "\u001b[32m10-26 01:31:48\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(79872,)\n",
      "\u001b[32m10-26 01:31:48\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ありがとうございます。では、今日はここまでとしましょう。次回の予約を決めましょうね。\n",
      "\u001b[32m10-26 01:31:48\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(279040,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:20\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_63...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_63\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m132.842\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:14\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_63...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_63\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m126.547\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-26 01:36:39\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こんにちは、〇〇さん。今日は、どんなことでお越しになりましたか？お話しいただける範囲で、ゆっくりお聞かせくださいね。\n",
      "\u001b[32m10-26 01:36:39\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(352768,)\n",
      "\u001b[32m10-26 01:36:39\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、なんだか最近、うまくいかないことばかりで…仕事もそうなんですけど、なんだか毎日が不安で、焦る気持ちがあります。\n",
      "\u001b[32m10-26 01:36:40\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(440832,)\n",
      "\u001b[32m10-26 01:36:40\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうなんですね。なんだか、お辛い気持ちを抱えていらっしゃるのですね。仕事のことも、そして、毎日の生活のことも、漠然とした不安や焦りを感じていらっしゃる、と。\n",
      "\u001b[32m10-26 01:36:40\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(572416,)\n",
      "\u001b[32m10-26 01:36:40\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、そうです。朝起きるのも億劫で、仕事に行っても集中できなくて。家に帰っても、何をする気にもなれないんです。\n",
      "\u001b[32m10-26 01:36:40\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(384000,)\n",
      "\u001b[32m10-26 01:36:40\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど。朝起きるのも億劫で、お仕事でも集中が続かなかったり、お家に帰られても、なかなかこう、気持ちが向かないような状態なんですね。\n",
      "\u001b[32m10-26 01:36:40\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(510976,)\n",
      "\u001b[32m10-26 01:36:40\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。今までなら、こうすればいい、って思えたことも、今はどうしたらいいのか分からなくて…このままでいいのかな、ってずっと考えてしまいます。\n",
      "\u001b[32m10-26 01:36:41\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(483840,)\n",
      "\u001b[32m10-26 01:36:41\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、そうなんですね。今まで当たり前にできていたことが、今は少し、負担に感じていらっしゃるのかもしれませんね。このままでいいのかな、というお気持ち、とてもよく分かります。\n",
      "\u001b[32m10-26 01:36:41\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(603136,)\n",
      "\u001b[32m10-26 01:36:41\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "（少し間をおいて）…はい。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: JPCommonLabel_insert_pause() in jpcommon_label.c: First mora should not be short pause.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-26 01:36:41\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(104448,)\n",
      "\u001b[32m10-26 01:36:41\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "色々と、お心の中で抱えられていることが、たくさんあるようですね。今日は、そんな漠然とした不安や焦りの背景について、少しだけお話を聞かせていただけて、ありがとうございます。\n",
      "\u001b[32m10-26 01:36:42\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(610816,)\n",
      "\u001b[32m10-26 01:36:42\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こちらこそ、ありがとうございます。少し、話せてよかったです。\n",
      "\u001b[32m10-26 01:36:42\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(181248,)\n",
      "\u001b[32m10-26 01:36:42\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、嬉しいです。今日は、〇〇さんが今抱えていらっしゃる気持ちの大きな部分に触れることができたと思います。次回は、もう少し具体的に、この不安や焦りがどんな時に強く感じられるかなど、お話しできたらと思いますがいかがでしょうか？\n",
      "\u001b[32m10-26 01:36:42\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(755200,)\n",
      "\u001b[32m10-26 01:36:42\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、お願いします。\n",
      "\u001b[32m10-26 01:36:42\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(78848,)\n",
      "\u001b[32m10-26 01:36:42\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、承知いたしました。では、次回の予約についてですが…\n",
      "\u001b[32m10-26 01:36:43\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(215040,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:23\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_64...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_64\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m137.458\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:26\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_64...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_64\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m138.311\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-26 01:41:54\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "〇〇さん、こんにちは。今日は、これまで数回お話を伺ってきましたが、そろそろ終結に向けて、これまでを少し振り返ってみませんか？\n",
      "\u001b[32m10-26 01:41:55\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(449536,)\n",
      "\u001b[32m10-26 01:41:55\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうですね…。なんか、あっという間でしたけど、色々ありましたね。\n",
      "\u001b[32m10-26 01:41:55\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(212480,)\n",
      "\u001b[32m10-26 01:41:55\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、そうですね。このカウンセリングを通じて、〇〇さんの中で、何か変化したことや、気づかれたことなど、ございますか？\n",
      "\u001b[32m10-26 01:41:55\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(390144,)\n",
      "\u001b[32m10-26 01:41:55\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。以前は漠然と「性格を直したい」って思ってたんですけど、集団面接で話せない自分にどうしていいか分からなくて。でも、先生と話していくうちに、あ、自分が「口下手だから失敗する」って勝手に思い込んで、それで不安になって、結局話さない、っていう悪循環だったんだなって。\n",
      "\u001b[32m10-26 01:41:55\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(884224,)\n",
      "\u001b[32m10-26 01:41:55\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど、そうなんですね。ご自身の「認知」、「感情」、「行動」のつながりに、気づかれたのですね。ええ。\n",
      "\u001b[32m10-26 01:41:56\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(368128,)\n",
      "\u001b[32m10-26 01:41:56\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。それに気づいてからは、「じゃあ、どうしたらいいんだろう」って考えるようになって…ホームワークで、まずは小さなことでも意見を言ってみる、っていうのをやってみたりしました。\n",
      "\u001b[32m10-26 01:41:56\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(524800,)\n",
      "\u001b[32m10-26 01:41:56\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、素晴らしいですね。実際に、試されてみて、いかがでしたか？\n",
      "\u001b[32m10-26 01:41:56\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(219648,)\n",
      "\u001b[32m10-26 01:41:56\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "最初はやっぱりドキドキしたんですけど、意外と誰も自分の発言を批判したりしないんだ、って分かって。少しずつ、言えるようになってきたんです。完全に不安がなくなったわけじゃないですけど、前よりは自信が持てるようになった気がします。\n",
      "\u001b[32m10-26 01:41:57\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(681472,)\n",
      "\u001b[32m10-26 01:41:57\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、それは大きな変化ですね。ご自身の力で、悪循環を断ち切り、良い循環へと向かわれたのですね。そうなんですね。\n",
      "\u001b[32m10-26 01:41:57\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(435712,)\n",
      "\u001b[32m10-26 01:41:57\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。なんか、客観的に自分のことを見れるようになった、っていうのが大きいかもしれないです。\n",
      "\u001b[32m10-26 01:41:57\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(259072,)\n",
      "\u001b[32m10-26 01:41:57\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、客観的にご自身を見つめられるようになったことで、気持ちも少し楽になったり、対処法も見えてきたということでしょうか。\n",
      "\u001b[32m10-26 01:41:57\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(400384,)\n",
      "\u001b[32m10-26 01:41:57\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、まさにそんな感じです。本当に、ありがとうございます。\n",
      "\u001b[32m10-26 01:41:57\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(228352,)\n",
      "\u001b[32m10-26 01:41:57\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "いえいえ、〇〇さんの頑張りがあってこそですよ。これからは、ご自身でその気づきを活かしながら、さらにステップアップしていかれることと思います。ええ。今後のことも含めて、あと数回、またお話できればと思いますが、いかがでしょうか？\n",
      "\u001b[32m10-26 01:41:58\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(733696,)\n",
      "\u001b[32m10-26 01:41:58\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、お願いします。まだ少し不安なこともあるので、相談させてください。\n",
      "\u001b[32m10-26 01:41:58\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(247808,)\n",
      "\u001b[32m10-26 01:41:58\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、承知いたしました。では、次回の予約について、この後少しお話できますか？\n",
      "\u001b[32m10-26 01:41:58\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(299008,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:34\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_65...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_65\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m147.785\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:27\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_65...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_65\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m138.131\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-26 01:47:57\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こんにちは。今日はよくいらっしゃいましたね。お座りください。今日のご気分はいかがですか？\n",
      "\u001b[32m10-26 01:47:58\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(286208,)\n",
      "\u001b[32m10-26 01:47:58\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こんにちは。ありがとうございます。なんだかいつも心がざわざわしていて、落ち着かない感じです...。\n",
      "\u001b[32m10-26 01:47:58\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(335872,)\n",
      "\u001b[32m10-26 01:47:58\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうなんですね、心がざわざわされるのですね。まずは今感じていること、どんなことでもお話しくださいね。\n",
      "\u001b[32m10-26 01:47:58\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(377344,)\n",
      "\u001b[32m10-26 01:47:58\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい...。仕事のことが一番大きいと思うんですけど、最近は家でも休日でも、常に何かに追われているような焦りを感じていて。漠然とした不安があります。\n",
      "\u001b[32m10-26 01:47:58\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(521216,)\n",
      "\u001b[32m10-26 01:47:58\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど。お仕事が一番大きいんですね。休日でも焦りが。それはお辛いですね。お仕事ではどんな時に焦りを感じることが多いですか？\n",
      "\u001b[32m10-26 01:47:59\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(489472,)\n",
      "\u001b[32m10-26 01:47:59\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。特に新しいプロジェクトを任された時なんかは、期待に応えなきゃって思う反面、自分にできるか不安で...。完璧を求めすぎてしまい、結果が出ないと自己嫌悪に陥ります。\n",
      "\u001b[32m10-26 01:47:59\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(588800,)\n",
      "\u001b[32m10-26 01:47:59\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうなんですね。期待と不安がせめぎ合うのですね。その自己嫌悪、日常生活にも影響していますか？\n",
      "\u001b[32m10-26 01:47:59\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(360448,)\n",
      "\u001b[32m10-26 01:47:59\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ。家に帰っても仕事が頭から離れず、夜もなかなか眠れないことが増えました。休日に気分転換しようと思っても、何も手につかなくて、余計に疲れてしまうんです。\n",
      "\u001b[32m10-26 01:47:59\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(565248,)\n",
      "\u001b[32m10-26 01:47:59\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうでしたか。仕事が頭から離れず眠れない、休日にかえって疲れてしまう。大変お辛いですね。今日はお話しいただきありがとうございます。漠然とした不安や焦りについて、少しお伺いできました。\n",
      "\u001b[32m10-26 01:48:00\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(703488,)\n",
      "\u001b[32m10-26 01:48:00\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "次回は、その「焦り」や「不安」がどんな時に特に強く現れるのか、具体的に掘り下げていけたらと思うのですが、いかがでしょうか？\n",
      "\u001b[32m10-26 01:48:00\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(427520,)\n",
      "\u001b[32m10-26 01:48:00\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。そうですね。話すと少し楽になった気がします。次回もお願いします。\n",
      "\u001b[32m10-26 01:48:00\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(261120,)\n",
      "\u001b[32m10-26 01:48:00\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ありがとうございます。それでは、次回の予約ですが...。\n",
      "\u001b[32m10-26 01:48:00\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(171008,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:25\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_66...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_66\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m139.021\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:20\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_66...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_66\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m133.339\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-26 01:53:07\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こんにちは、〇〇さん。今日は、〇〇さんのお話をゆっくりお聞かせいただきたいと思います。何か、今、お話ししたいことや、気になっていることがあれば、どんなことでも構いませんので、教えていただけますか？\n",
      "\u001b[32m10-26 01:53:08\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(608256,)\n",
      "\u001b[32m10-26 01:53:08\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。ええと、最近、仕事のこともそうなんですけど、なんかこう、日常生活全体に漠然とした不安とか、焦りを感じていて…。何から話したらいいのか、自分でもよくわからなくて。\n",
      "\u001b[32m10-26 01:53:08\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(612864,)\n",
      "\u001b[32m10-26 01:53:08\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうなんですね。はい、漠然とした不安や焦り、感じていらっしゃるのですね。ええ、何から話したらいいか分からない、というお気持ち、とてもよく分かりますよ。もしよかったら、どんな時に、その不安や焦りを感じることが多いですか？ 例えば、具体的な場面などあれば…。\n",
      "\u001b[32m10-26 01:53:08\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(907264,)\n",
      "\u001b[32m10-26 01:53:08\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうですね…。例えば、仕事で新しいプロジェクトを任されたりすると、すごくプレッシャーを感じて、夜も眠れなくなったりしますし、休日もなんだかリラックスできなくて…。\n",
      "\u001b[32m10-26 01:53:08\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(534016,)\n",
      "\u001b[32m10-26 01:53:08\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど、新しいプロジェクトのプレッシャーで、夜も眠れなかったり、休日もリラックスできなかったりするんですね。ええ。それは、お辛いですね。具体的に、そういった時に、〇〇さんはどんな風に感じたり、考えたりすることが多いですか？\n",
      "\u001b[32m10-26 01:53:09\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(760832,)\n",
      "\u001b[32m10-26 01:53:09\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "うーん。「私にできるのかな」とか、「また失敗したらどうしよう」って、どうしても悪い方にばかり考えてしまって。そうすると、もう、何も手につかなくなって、結局、締め切りギリギリまで動けなくなっちゃったりして…。\n",
      "\u001b[32m10-26 01:53:09\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(680448,)\n",
      "\u001b[32m10-26 01:53:09\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、はい。そうなんですね。「私にできるのかな」「失敗したらどうしよう」って考えてしまって、結果として、何も手につかなくなる…。ええ、そのお気持ち、よく伝わってきますよ。ご自身のことを、客観的に捉えようとされているのですね。\n",
      "\u001b[32m10-26 01:53:09\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(817664,)\n",
      "\u001b[32m10-26 01:53:09\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうです、まさに。どんどん悪い方に考えてしまう自分が嫌になって、また落ち込むんです。\n",
      "\u001b[32m10-26 01:53:10\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(262144,)\n",
      "\u001b[32m10-26 01:53:10\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、悪循環のように感じてしまうのですね。お一人で抱え込まずに、今日こうして、お話ししに来てくださったこと、本当に素晴らしいと思いますよ。\n",
      "\u001b[32m10-26 01:53:10\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(498176,)\n",
      "\u001b[32m10-26 01:53:10\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ありがとうございます。少し、話せただけでも、少し楽になった気がします。\n",
      "\u001b[32m10-26 01:53:10\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(232960,)\n",
      "\u001b[32m10-26 01:53:10\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "良かったです。ええ。今日は、〇〇さんがどんな時に不安を感じやすいのか、少し具体的なお話が聞けて、私もとてもよく理解できました。次回は、今日お話しいただいたことについて、もう少し詳しく掘り下げて、その不安に対して、どんなことができるか一緒に考えていきませんか？\n",
      "\u001b[32m10-26 01:53:10\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(918528,)\n",
      "\u001b[32m10-26 01:53:10\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、ぜひお願いします。\n",
      "\u001b[32m10-26 01:53:11\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(86528,)\n",
      "\u001b[32m10-26 01:53:11\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ありがとうございます。では、今日のところはここまでとさせていただきますね。次回のご予約は、〇月〇日の〇時でよろしいでしょうか？\n",
      "\u001b[32m10-26 01:53:11\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(394752,)\n",
      "\u001b[32m10-26 01:53:11\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、大丈夫です。よろしくお願いします。\n",
      "\u001b[32m10-26 01:53:11\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(144896,)\n",
      "\u001b[32m10-26 01:53:11\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、こちらこそ。今日はありがとうございました。お気をつけてお帰りくださいね。\n",
      "\u001b[32m10-26 01:53:11\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(254464,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:45\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_67...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_67\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m156.684\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:43\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_67...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_67\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m156.946\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-26 01:58:56\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こんにちは、今日は来てくださってありがとうございます。どうぞ楽にしてくださいね。今日はどんなことをお話ししたい気分ですか？\n",
      "\u001b[32m10-26 01:58:56\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(395776,)\n",
      "\u001b[32m10-26 01:58:56\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こんにちは。ありがとうございます。えっと…何から話せばいいのか、漠然とした不安があるというか、焦りを感じていて…\n",
      "\u001b[32m10-26 01:58:56\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(420864,)\n",
      "\u001b[32m10-26 01:58:56\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうなんですね。漠然とした不安と焦り、ですか。はい。具体的にどんな時に、そういった気持ちになることが多いですか？\n",
      "\u001b[32m10-26 01:58:57\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(418304,)\n",
      "\u001b[32m10-26 01:58:57\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "特にこれといったきっかけがあるわけじゃないんですけど、仕事でうまくいかないと「このままでいいのかな」って考えたり、家で一人でいる時も、ふと「何やってるんだろう」って思っちゃったりして…\n",
      "\u001b[32m10-26 01:58:57\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(548864,)\n",
      "\u001b[32m10-26 01:58:57\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、なるほど。仕事の状況も、お一人の時間も、ですね。そういった時、例えば「何やってるんだろう」と思うと、どんな気持ちになりますか？\n",
      "\u001b[32m10-26 01:58:57\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(506368,)\n",
      "\u001b[32m10-26 01:58:57\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なんだか、取り残されているような…周りの人はみんなちゃんと頑張ってるのに、自分だけが立ち止まっているような気がして、もっと頑張らなきゃって思うんですけど、何から手をつけていいか分からなくて。\n",
      "\u001b[32m10-26 01:58:57\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(587776,)\n",
      "\u001b[32m10-26 01:58:57\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、周りの方と比べて、ご自身だけが立ち止まっているように感じてしまうのですね。もっと頑張りたいのに、その一歩がなかなか踏み出せないような…そんなお気持ちでしょうか。\n",
      "\u001b[32m10-26 01:58:58\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(592384,)\n",
      "\u001b[32m10-26 01:58:58\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "まさに、そんな感じです。この状況をどうにかしたいとは思うんですけど…\n",
      "\u001b[32m10-26 01:58:58\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(217088,)\n",
      "\u001b[32m10-26 01:58:58\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。お話を聞かせていただき、ありがとうございます。漠然とした不安や焦り、そして「取り残されている」というお気持ち。まずはそういったお気持ちを、今日、こうしてお話ししてくださったこと自体が、とても大きな一歩だと思いますよ。\n",
      "\u001b[32m10-26 01:58:58\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(776192,)\n",
      "\u001b[32m10-26 01:58:58\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうですか…少し、気持ちが楽になった気がします。\n",
      "\u001b[32m10-26 01:58:58\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(168448,)\n",
      "\u001b[32m10-26 01:58:58\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、そう言っていただけて嬉しいです。今日は、Bさんが今感じていることを少しお聞かせいただきましたが、次回は、もう少し具体的に、どんな状況でそういったお気持ちが強くなるのか、一緒に整理していくのはいかがでしょうか？\n",
      "\u001b[32m10-26 01:58:59\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(699392,)\n",
      "\u001b[32m10-26 01:58:59\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、お願いします。\n",
      "\u001b[32m10-26 01:58:59\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(78848,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:30\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_68...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_68\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m141.880\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:26\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_68...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_68\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m139.917\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-26 02:04:11\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "〇〇さん、今日はこれまでのセッションを振り返りながら、そろそろ終結に向けてのお話をしていきましょうか。いかがですか？\n",
      "\u001b[32m10-26 02:04:12\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(360448,)\n",
      "\u001b[32m10-26 02:04:12\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、なんだかあっという間でしたね。でも、少し寂しいような気もします。\n",
      "\u001b[32m10-26 02:04:12\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(243712,)\n",
      "\u001b[32m10-26 02:04:12\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、そうなんですね。寂しさを感じていらっしゃるのですね。この数ヶ月、〇〇さんがご自身と向き合ってこられた中で、何か特に印象に残っていることや、ご自身の変化について感じることはありますか？\n",
      "\u001b[32m10-26 02:04:12\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(660992,)\n",
      "\u001b[32m10-26 02:04:12\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうですね…。最初は本当に漠然とした不安ばかりで、どうしたらいいか分からなかったんですけど、カウンセラーさんとお話ししていく中で、何が自分を不安にさせているのか、少しずつ整理できるようになってきた気がします。\n",
      "\u001b[32m10-26 02:04:13\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(660992,)\n",
      "\u001b[32m10-26 02:04:13\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。なるほど、ご自身の感情や思考を整理できるようになったのですね。\n",
      "\u001b[32m10-26 02:04:13\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(237056,)\n",
      "\u001b[32m10-26 02:04:13\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。特に、あの…、自分の考えや行動、感情を図に書き出すっていうのが、すごく役立ちました。客観的に見られるようになって、気持ちが楽になる瞬間もあって。\n",
      "\u001b[32m10-26 02:04:13\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(562176,)\n",
      "\u001b[32m10-26 02:04:13\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、そうなんですね。ご自身の内面を、図や表を使って整理できたことが、大きな気づきになったのですね。素晴らしいことです。\n",
      "\u001b[32m10-26 02:04:13\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(436224,)\n",
      "\u001b[32m10-26 02:04:13\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ありがとうございます。もちろん、まだ完全に不安がなくなったわけじゃないんですけど、前よりも、『どうしよう』って途方に暮れることが減った気がします。自分でどうにかできるって、少し自信が持てるようになってきて。\n",
      "\u001b[32m10-26 02:04:14\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(600576,)\n",
      "\u001b[32m10-26 02:04:14\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、そうなんですね。ご自身で問題への対策の糸口を見つけられるようになったというのは、本当に大きな進歩ですね。\n",
      "\u001b[32m10-26 02:04:14\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(400384,)\n",
      "\u001b[32m10-26 02:04:14\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、本当に。カウンセラーさんのおかげです。\n",
      "\u001b[32m10-26 02:04:14\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(165888,)\n",
      "\u001b[32m10-26 02:04:14\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "いえいえ、〇〇さんご自身が一生懸命取り組まれた結果ですよ。今後も、ご自身のペースで、今日の学びを大切にしながら過ごしていってくださいね。もしまた何かありましたらいつでもご相談ください。\n",
      "\u001b[32m10-26 02:04:14\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(641024,)\n",
      "\u001b[32m10-26 02:04:14\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、そうします。本当にありがとうございました。\n",
      "\u001b[32m10-26 02:04:15\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(165888,)\n",
      "\u001b[32m10-26 02:04:15\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、〇〇さんのこれからの毎日を応援しています。今日で一旦、終結とさせていただきますね。お疲れ様でした。\n",
      "\u001b[32m10-26 02:04:15\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(363008,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:27\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_69...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_69\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m138.039\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:26\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_69...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_69\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m138.975\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-26 02:09:59\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こんにちは、Bさん。今日は前回お話しいただいた「もっと完璧にしないと」というお気持ちについて、もう少し詳しくお伺いしてもよろしいでしょうか？\n",
      "\u001b[32m10-26 02:09:59\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(442368,)\n",
      "\u001b[32m10-26 02:09:59\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、先生。最近、特に仕事で「もっと完璧にしないと」っていつも考えてしまって、何をしていても焦りを感じるんです。\n",
      "\u001b[32m10-26 02:10:00\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(391168,)\n",
      "\u001b[32m10-26 02:10:00\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうなんですね。その「完璧に」というお気持ち、どんな時に特に強く感じられますか？\n",
      "\u001b[32m10-26 02:10:00\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(294912,)\n",
      "\u001b[32m10-26 02:10:00\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "例えば資料作成だと、小さな誤字脱字一つでも許せなくて。「これじゃダメだ」って自分を追い詰めてしまって、提出も遅れて困っています。\n",
      "\u001b[32m10-26 02:10:00\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(444416,)\n",
      "\u001b[32m10-26 02:10:00\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど、完璧でないと、という気持ちが行動を阻むことも。その時、Bさんはどんな感情になりますか？\n",
      "\u001b[32m10-26 02:10:01\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(356352,)\n",
      "\u001b[32m10-26 02:10:01\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "失敗するんじゃないかと不安で、周りから「しっかりしてない」と思われるのも怖くて。\n",
      "\u001b[32m10-26 02:10:01\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(248832,)\n",
      "\u001b[32m10-26 02:10:01\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "不安や恐れですね。周りの目が気になる、と。その「完璧であるべき」というお気持ちは、いつ頃から感じていますか？\n",
      "\u001b[32m10-26 02:10:01\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(399360,)\n",
      "\u001b[32m10-26 02:10:01\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "うーん…昔からかもしれません。小さい頃から「ちゃんとしないと」と言われて育ったので、それが当たり前になってしまって。\n",
      "\u001b[32m10-26 02:10:01\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(377856,)\n",
      "\u001b[32m10-26 02:10:01\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうなんですね。長年の習慣のようになっているのかもしれません。もし、その「完璧であるべき」を少し緩めてみたら、どうなると思いますか？\n",
      "\u001b[32m10-26 02:10:02\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(451072,)\n",
      "\u001b[32m10-26 02:10:02\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええっ…不安で手につかなくなる気もします。でも、少し楽になるのかな、とも。\n",
      "\u001b[32m10-26 02:10:02\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(258048,)\n",
      "\u001b[32m10-26 02:10:02\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど、不安を感じる一方で、楽になる可能性も少し感じているのですね。では、次回までのホームワークとして、「完璧であるべき」と感じた時に、どんな気持ちになり、どんな行動を取ったのか、少し意識して観察してみるのはいかがでしょう。\n",
      "\u001b[32m10-26 02:10:02\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(798208,)\n",
      "\u001b[32m10-26 02:10:02\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "観察ですね。はい、やってみます。焦りから抜け出すきっかけになるなら。\n",
      "\u001b[32m10-26 02:10:02\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(240128,)\n",
      "\u001b[32m10-26 02:10:02\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、そうですね。まずはご自身の心の中で何が起こっているのかを客観的に見つめることが大切ですから。今日は「すべき思考」についてお話しいただきありがとうございました。次回また、観察で気づいたことをお聞かせくださいね。\n",
      "\u001b[32m10-26 02:10:03\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(757760,)\n",
      "\u001b[32m10-26 02:10:03\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、ありがとうございます。次もよろしくお願いします。\n",
      "\u001b[32m10-26 02:10:03\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(192512,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:27\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_70...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_70\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m138.268\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:26\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_70...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_70\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m139.105\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-26 02:15:25\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こんにちは、Bさん。今日はどんなお気持ちですか？\n",
      "\u001b[32m10-26 02:15:25\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(145408,)\n",
      "\u001b[32m10-26 02:15:25\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こんにちは、先生。なんだかモヤモヤしていて。やらなきゃいけないことがたくさんあるのに、全然できてないなって焦りを感じるんです。\n",
      "\u001b[32m10-26 02:15:25\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(398336,)\n",
      "\u001b[32m10-26 02:15:25\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうなんですね。やらなきゃいけないことが多くて、ええ。それができていないと感じて焦りを感じていらっしゃる、と。\n",
      "\u001b[32m10-26 02:15:25\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(348672,)\n",
      "\u001b[32m10-26 02:15:26\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。例えば、もっと早く仕事を終わらせて、家に帰ったらすぐご飯を作って、部屋もいつも綺麗にしておくべきだって思うのに、実際は追いつかなくて…。\n",
      "\u001b[32m10-26 02:15:26\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(477184,)\n",
      "\u001b[32m10-26 02:15:26\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど。「〜すべき」というお気持ちが、Bさんの心の中に強くあるのですね。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: JPCommonLabel_push_word() in jpcommon_label.c: First mora should not be long vowel symbol.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-26 02:15:26\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(268800,)\n",
      "\u001b[32m10-26 02:15:26\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。周りの人はみんなもっとちゃんとやっている気がして、自分だけができてないって思うと、余計に苦しくなってしまいます。\n",
      "\u001b[32m10-26 02:15:26\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(392192,)\n",
      "\u001b[32m10-26 02:15:26\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、そうなんですね。周りと比べて、ご自身に「〜すべき」という期待をかけていらっしゃる。その考え方は、Bさんにとって、どんなふうに感じられますか？\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: JPCommonLabel_push_word() in jpcommon_label.c: First mora should not be long vowel symbol.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-26 02:15:27\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(492544,)\n",
      "\u001b[32m10-26 02:15:27\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "うーん…楽には、ならないですね。むしろ、もっと追い詰められるような、息苦しい感じがします。\n",
      "\u001b[32m10-26 02:15:27\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(334848,)\n",
      "\u001b[32m10-26 02:15:27\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、息苦しく感じるんですね。無理に「〜すべき」とご自身を縛りつけると、心は休まらないかもしれませんね。今日は、その「〜すべき」が心にどう影響しているか、見つめ直すきっかけになったでしょうか。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: JPCommonLabel_push_word() in jpcommon_label.c: First mora should not be long vowel symbol.\n",
      "WARNING: JPCommonLabel_push_word() in jpcommon_label.c: First mora should not be long vowel symbol.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-26 02:15:27\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(678400,)\n",
      "\u001b[32m10-26 02:15:27\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうですね…。自分を苦しめているのが、この「〜すべき」っていう考え方なのかもしれないって、少し気づけた気がします。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: JPCommonLabel_push_word() in jpcommon_label.c: First mora should not be long vowel symbol.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-26 02:15:28\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(362496,)\n",
      "\u001b[32m10-26 02:15:28\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、素晴らしい気づきですね。次回は、この「〜すべき」から、もう少し楽になれるような見方について一緒に考えていきましょうか。来週の同じ時間でよろしいでしょうか？\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: JPCommonLabel_push_word() in jpcommon_label.c: First mora should not be long vowel symbol.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-26 02:15:28\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(530432,)\n",
      "\u001b[32m10-26 02:15:28\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、大丈夫です。よろしくお願いします。\n",
      "\u001b[32m10-26 02:15:28\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(143872,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:20\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_71...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_71\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m131.754\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:21\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_71...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_71\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m135.358\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-26 02:21:14\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こんにちは。今日はよくいらっしゃいましたね。今、何かお話ししたいこと、お聞かせいただけますか？\n",
      "\u001b[32m10-26 02:21:14\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(316928,)\n",
      "\u001b[32m10-26 02:21:14\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。最近、仕事も生活も、漠然とした不安や焦りを感じています。\n",
      "\u001b[32m10-26 02:21:14\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(246272,)\n",
      "\u001b[32m10-26 02:21:14\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、そうなんですね。漠然とした不安、ですか。どんな時に感じることが多いですか？\n",
      "\u001b[32m10-26 02:21:15\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(302080,)\n",
      "\u001b[32m10-26 02:21:15\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "小さなことでも『これでいいのかな』と不安になり、先のことを考えると息苦しくなってしまいます。\n",
      "\u001b[32m10-26 02:21:15\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(281088,)\n",
      "\u001b[32m10-26 02:21:15\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど。息苦しさ、ですね。何かきっかけはありましたか？\n",
      "\u001b[32m10-26 02:21:15\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(217600,)\n",
      "\u001b[32m10-26 02:21:15\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "これといって。ただ、この半年で仕事の役割が増えて、それがプレッシャーなのかもしれません。\n",
      "\u001b[32m10-26 02:21:16\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(290816,)\n",
      "\u001b[32m10-26 02:21:16\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、そうなんですね。そのプレッシャーが、日常生活にも影響していると感じますか？\n",
      "\u001b[32m10-26 02:21:16\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(287744,)\n",
      "\u001b[32m10-26 02:21:16\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ。家でも仕事が頭から離れず、休日にリフレッシュしようとしても晴れないんです。\n",
      "\u001b[32m10-26 02:21:16\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(266240,)\n",
      "\u001b[32m10-26 02:21:16\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど。お辛いですね。睡眠には影響がありますか？\n",
      "\u001b[32m10-26 02:21:16\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(198656,)\n",
      "\u001b[32m10-26 02:21:16\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。夜中に目が覚めたり、朝も体が重くて。寝ても疲れが取れないんです。\n",
      "\u001b[32m10-26 02:21:16\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(263680,)\n",
      "\u001b[32m10-26 02:21:16\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうなんですね。睡眠にも影響が出ているんですね。今日は、不安がお仕事や睡眠にも繋がっていることが見えてきました。お話しいただきありがとうございます。次回、もう少し詳しくお伺いするお時間をいただけますか？\n",
      "\u001b[32m10-26 02:21:17\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(676864,)\n",
      "\u001b[32m10-26 02:21:17\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、ぜひお願いします。少し話せて、なんだか気持ちが楽になりました。\n",
      "\u001b[32m10-26 02:21:17\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(259072,)\n",
      "\u001b[32m10-26 02:21:17\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "それはよかったです。では、次回は〇月〇日〇時でいかがでしょうか？今日はお疲れ様でした。\n",
      "\u001b[32m10-26 02:21:17\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(275456,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:21\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_72...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_72\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m131.875\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:15\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_72...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_72\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m127.601\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-26 02:26:12\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "〇〇さん、こんにちは。今日は少し、これまでのセッションを振り返りながら、今後のことについてもお話しできたらと思うのですが、いかがでしょうか？\n",
      "\u001b[32m10-26 02:26:12\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(445952,)\n",
      "\u001b[32m10-26 02:26:12\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、こんにちは。そうですね、もうそんな時期なんですね。あっという間でした。\n",
      "\u001b[32m10-26 02:26:12\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(286208,)\n",
      "\u001b[32m10-26 02:26:12\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、そうですね。この数ヶ月、〇〇さんが本当にたくさんのことに取り組んでこられた時間でしたね。特に、最初にお話しされていた漠然とした不安や、仕事への焦りといった点で、何か変化を感じることはありますか？\n",
      "\u001b[32m10-26 02:26:13\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(709120,)\n",
      "\u001b[32m10-26 02:26:13\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "うーん、そうですね。以前は、本当に何をするにも不安で、特に仕事のことも、自分が何をしたいのかもわからなくて、ずっとモヤモヤしていたんです。でも、最近は、少しずつですけど、自分の考えを整理できるようになってきた気がします。\n",
      "\u001b[32m10-26 02:26:13\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(740352,)\n",
      "\u001b[32m10-26 02:26:13\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど、自分の考えを整理できるようになった、というのは大きな変化ですね。ええ、具体的にはどんな時にそう感じますか？\n",
      "\u001b[32m10-26 02:26:13\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(418816,)\n",
      "\u001b[32m10-26 02:26:13\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。例えば、以前だったら、急な仕事が入ると「また失敗するんじゃないか」ってすぐに焦って、何も手につかなかったんですけど、今は「じゃあ、どうしたらいいかな」って、落ち着いて考えられるようになったというか…\n",
      "\u001b[32m10-26 02:26:14\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(610304,)\n",
      "\u001b[32m10-26 02:26:14\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうなんですね。素晴らしい変化ですね。以前は「失敗するんじゃないか」という気持ちに囚われがちだったのが、今は「どうしたらいいか」と、具体的な対策に目を向けられるようになった、ということでしょうか。\n",
      "\u001b[32m10-26 02:26:14\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(646144,)\n",
      "\u001b[32m10-26 02:26:14\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、まさにそんな感じです。カウンセリングで、自分の感情とか、その時の行動を書き出す練習をしたのが、すごく役立っていると思います。客観的に見られるようになりました。\n",
      "\u001b[32m10-26 02:26:14\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(523264,)\n",
      "\u001b[32m10-26 02:26:14\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、お話しましたね。ご自身の認知や感情、行動を整理するワークが役立ったのですね。ご自身で問題に取り組む力が、本当に身についた証拠だと思います。\n",
      "\u001b[32m10-26 02:26:14\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(555008,)\n",
      "\u001b[32m10-26 02:26:14\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ありがとうございます。でも、正直、これで完全に大丈夫なのかな、って少し不安になる時もあります…。\n",
      "\u001b[32m10-26 02:26:15\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(307200,)\n",
      "\u001b[32m10-26 02:26:15\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、そうですよね。完全に不安がなくなる、というのは難しいことかもしれません。でも、〇〇さんには、これまで身につけてこられた、ご自身の感情や思考を客観的に捉える力、そして具体的な対策を考える力が確実にあるはずです。今後も、何か困ったことがあったら、その力を信じて、またあの「書き出す」ワークを試してみるのも良いかもしれませんね。\n",
      "\u001b[32m10-26 02:26:15\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(1201664,)\n",
      "\u001b[32m10-26 02:26:15\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、そうですね。また困ったら、あの方法を使ってみます。なんだか、少し安心しました。\n",
      "\u001b[32m10-26 02:26:16\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(325120,)\n",
      "\u001b[32m10-26 02:26:16\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。これまでの〇〇さんの頑張りが、きっと今後の支えになるはずです。それでは、次回のセッションを最終回として、今日の振り返りを踏まえながら、終結に向けての準備を進めていきましょう。\n",
      "\u001b[32m10-26 02:26:16\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(595968,)\n",
      "\u001b[32m10-26 02:26:16\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、よろしくお願いします。\n",
      "\u001b[32m10-26 02:26:16\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(99328,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:38\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_73...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_73\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m153.977\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:40\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_73...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_73\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m154.300\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-26 02:32:05\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こんにちは、Bさん。先週お話しされていた「漠然とした焦り」について、今日はもう少しお伺いできたらと思います。いかがでしょうか？\n",
      "\u001b[32m10-26 02:32:05\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(413184,)\n",
      "\u001b[32m10-26 02:32:05\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、なんだか最近、何をしていても「こうすべきだ」って考えが頭から離れないんです。完璧にこなさなきゃ、って。\n",
      "\u001b[32m10-26 02:32:06\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(380928,)\n",
      "\u001b[32m10-26 02:32:06\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうなんですね。「完璧にこなすべきだ」、と。ええ、なるほど。そういったお気持ちが、Bさんを少し息苦しくさせているように聞こえますね。\n",
      "\u001b[32m10-26 02:32:06\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(491520,)\n",
      "\u001b[32m10-26 02:32:06\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、本当に。例えば、仕事でちょっとしたミスでも、「なんでこんなこともできないんだろう」って、ずっと考えてしまって。\n",
      "\u001b[32m10-26 02:32:06\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(382976,)\n",
      "\u001b[32m10-26 02:32:06\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど。些細なことでも、ご自分を責めてしまうのですね。はい。そういった「〜すべき」という考え方があると、常に気が張ってしまいそうですね。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: JPCommonLabel_push_word() in jpcommon_label.c: First mora should not be long vowel symbol.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-26 02:32:06\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(505344,)\n",
      "\u001b[32m10-26 02:32:06\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、そうです。それが疲れる原因になっているのは分かっているんですが、止められないんです。\n",
      "\u001b[32m10-26 02:32:07\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(257536,)\n",
      "\u001b[32m10-26 02:32:07\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうなんですね。止められない、と。はい。もしかしたら、その「〜すべき」という考え方が、Bさんの今の不安や焦りに繋がっているのかもしれませんね。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: JPCommonLabel_push_word() in jpcommon_label.c: First mora should not be long vowel symbol.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-26 02:32:07\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(494592,)\n",
      "\u001b[32m10-26 02:32:07\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そう、かもしれません…。\n",
      "\u001b[32m10-26 02:32:07\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(86528,)\n",
      "\u001b[32m10-26 02:32:07\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ。この「〜すべき」という考え方について、今日は少しお話しできましたね。次回は、この考え方がBさんにとってどんな意味を持っているのか、もう少し一緒に掘り下げていけたらと思いますが、いかがでしょうか？\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: JPCommonLabel_push_word() in jpcommon_label.c: First mora should not be long vowel symbol.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-26 02:32:07\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(688128,)\n",
      "\u001b[32m10-26 02:32:07\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、ぜひお願いします。\n",
      "\u001b[32m10-26 02:32:08\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(87552,)\n",
      "\u001b[32m10-26 02:32:08\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ありがとうございます。では、本日はここまでとしましょう。お疲れ様でした。\n",
      "\u001b[32m10-26 02:32:08\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(255488,)\n",
      "\u001b[32m10-26 02:32:08\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ありがとうございました。\n",
      "\u001b[32m10-26 02:32:08\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(60928,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:18\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_74...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_74\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m133.416\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:16\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_74...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_74\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m128.390\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-26 02:37:09\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "〇〇さん、こんにちは。今日はこれまでのセッションを少し振り返りながら、終結に向けてのお話もできればと思うのですが、いかがでしょうか？\n",
      "\u001b[32m10-26 02:37:10\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(423936,)\n",
      "\u001b[32m10-26 02:37:10\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こんにちは。はい、そうですね。あっという間でしたけど、色々と変わったなと感じています。\n",
      "\u001b[32m10-26 02:37:10\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(295424,)\n",
      "\u001b[32m10-26 02:37:10\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、そうなんですね。この数ヶ月、〇〇さんが本当に色々なことに向き合ってこられましたものね。特に、ご自身の中で「これは変わったな」と感じることはありますか？\n",
      "\u001b[32m10-26 02:37:10\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(517632,)\n",
      "\u001b[32m10-26 02:37:10\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "うーん…はい。以前は、仕事でミスをすると「自分はダメだ」って、すごく落ち込んでしまっていたんです。それが、最近は「次はどうすればいいかな」って、少し冷静に考えられるようになった気がします。\n",
      "\u001b[32m10-26 02:37:10\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(603136,)\n",
      "\u001b[32m10-26 02:37:10\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど。ご自身を責めるのではなく、客観的に状況を捉えられるようになった、と。はい、素晴らしい変化ですね。\n",
      "\u001b[32m10-26 02:37:11\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(402432,)\n",
      "\u001b[32m10-26 02:37:11\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。先生と話しながら、自分の気持ちと行動がどう繋がっているのか、図で整理したのがすごく分かりやすくて。悪循環だったんだって、気づけたのが大きかったです。\n",
      "\u001b[32m10-26 02:37:11\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(499200,)\n",
      "\u001b[32m10-26 02:37:11\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、そうでしたね。ご自身の「認知」「感情」「行動」の繋がりを客観的に見て、「気づき」を得られたこと、それは〇〇さんの大きな力です。\n",
      "\u001b[32m10-26 02:37:11\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(452096,)\n",
      "\u001b[32m10-26 02:37:11\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ありがとうございます。でも、正直、これから一人で大丈夫かな、っていう不安も少しあって…。\n",
      "\u001b[32m10-26 02:37:12\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(278016,)\n",
      "\u001b[32m10-26 02:37:12\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、そうですよね。不安に感じるのは当然のことかと思います。でも、〇〇さんには、あの時ご自身で見つけた悪循環を断ち切るヒントや、客観的に考える力がもう備わっていますから、大丈夫ですよ。\n",
      "\u001b[32m10-26 02:37:12\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(679424,)\n",
      "\u001b[32m10-26 02:37:12\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、そうですね。なんだか少し安心しました。\n",
      "\u001b[32m10-26 02:37:12\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(161792,)\n",
      "\u001b[32m10-26 02:37:12\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、よかったです。もしまた困ったことがあれば、いつでもご相談くださいね。次回のセッションで、今日のお話も踏まえて、具体的に終結に向けての準備をしていきましょうか。\n",
      "\u001b[32m10-26 02:37:12\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(564736,)\n",
      "\u001b[32m10-26 02:37:12\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、お願いします。\n",
      "\u001b[32m10-26 02:37:13\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(81408,)\n",
      "\u001b[32m10-26 02:37:13\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "では、今日はこの辺で一区切りにしましょう。〇〇さん、今日もお話しいただきありがとうございました。\n",
      "\u001b[32m10-26 02:37:13\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(301568,)\n",
      "\u001b[32m10-26 02:37:13\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ありがとうございました。\n",
      "\u001b[32m10-26 02:37:13\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(60928,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:28\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_75...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_75\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m140.874\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:27\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_75...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_75\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m139.880\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-26 02:42:31\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "Bさん、こんにちは。今日はどのような感じでお過ごしでしたか？何か気になっていることなどありますか？\n",
      "\u001b[32m10-26 02:42:31\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(322560,)\n",
      "\u001b[32m10-26 02:42:31\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こんにちは。最近、もっと頑張らなきゃいけないのに、全然できてないって、ずっと考えてしまって…。会社でも家でも、『すべき』ことが山積みな気がして。\n",
      "\u001b[32m10-26 02:42:32\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(482304,)\n",
      "\u001b[32m10-26 02:42:32\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど、そうなんですね。『すべき』ことがたくさんある、と感じていらっしゃるのですね。ええ。\n",
      "\u001b[32m10-26 02:42:32\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(292864,)\n",
      "\u001b[32m10-26 02:42:32\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。例えば、仕事でも『もっと早く終わらせるべきだった』とか、家でも『週末くらいは掃除を完璧にするべきだ』とか、つい考えてしまって。\n",
      "\u001b[32m10-26 02:42:32\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(422400,)\n",
      "\u001b[32m10-26 02:42:32\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、そうなんですね。仕事でもお家でも、『〜すべき』という考えが頭の中を巡るのですね。そうした考えが浮かんでくるとき、Bさんはどのようなお気持ちになりますか？\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: JPCommonLabel_push_word() in jpcommon_label.c: First mora should not be long vowel symbol.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-26 02:42:32\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(544256,)\n",
      "\u001b[32m10-26 02:42:32\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "うーん…なんだか、いつも焦っているような、自分を責めているような気持ちになります。やらなきゃ、って思うのに、体が動かないというか…。\n",
      "\u001b[32m10-26 02:42:33\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(434176,)\n",
      "\u001b[32m10-26 02:42:33\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど、焦りやご自身を責めるお気持ちになるのですね。ええ、体が動かない感覚もあるのですね。\n",
      "\u001b[32m10-26 02:42:33\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(353280,)\n",
      "\u001b[32m10-26 02:42:33\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。結局、何もできてない自分にまたがっかりして、また『もっと頑張るべきなのに』って、悪循環なのは分かっているんですけど…。\n",
      "\u001b[32m10-26 02:42:33\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(407552,)\n",
      "\u001b[32m10-26 02:42:33\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうなんですね。その悪循環、ご自身でも感じていらっしゃるのですね。この『〜すべき』という考え方、Bさんの生活に大きな影響を与えているように思います。今日はここまでで一度整理をして、次回、この『〜すべき』という考え方について、Bさんにとって負担にならないような、新しい見方を探していくお手伝いができればと思いますが、いかがでしょうか？\n",
      "\u001b[32m10-26 02:42:34\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(1152512,)\n",
      "\u001b[32m10-26 02:42:34\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、ぜひお願いします。少し、気持ちが楽になった気がします。\n",
      "\u001b[32m10-26 02:42:34\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(211968,)\n",
      "\u001b[32m10-26 02:42:34\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "それはよかったです。では、次回の予約ですが、〇月〇日の〇時でよろしかったでしょうか？\n",
      "\u001b[32m10-26 02:42:34\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(262656,)\n",
      "\u001b[32m10-26 02:42:34\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、大丈夫です。\n",
      "\u001b[32m10-26 02:42:34\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(76288,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:25\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_76...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_76\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m139.550\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:22\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_76...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_76\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m132.070\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-26 02:47:51\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "〇〇さん、今日はこれまでのセッションを少し振り返りませんか。何か変化や気づきはありましたか？\n",
      "\u001b[32m10-26 02:47:52\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(287744,)\n",
      "\u001b[32m10-26 02:47:52\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうですね、最初に来た頃に比べると、漠然とした不安が少し軽くなったような気がします。特に何がどう、って説明するのは難しいんですけど。\n",
      "\u001b[32m10-26 02:47:52\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(463872,)\n",
      "\u001b[32m10-26 02:47:52\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、漠然とした不安が軽くなったんですね。大切な気づきです。どんな時にそう感じられましたか？\n",
      "\u001b[32m10-26 02:47:52\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(325632,)\n",
      "\u001b[32m10-26 02:47:52\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "以前は仕事でミスをすると「もうダメだ」って落ち込んでたんですけど、最近は「まあ、そういうこともあるか」って思えるようになりました。\n",
      "\u001b[32m10-26 02:47:53\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(352256,)\n",
      "\u001b[32m10-26 02:47:53\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど。以前と違う受け止め方ができるようになった、素晴らしい変化ですね。きっかけはありましたか？\n",
      "\u001b[32m10-26 02:47:53\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(344576,)\n",
      "\u001b[32m10-26 02:47:53\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "先生と話していく中で、自分の考え方のクセに気づけたからだと思います。完璧じゃないと、って思いすぎていたんだなって。\n",
      "\u001b[32m10-26 02:47:53\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(371712,)\n",
      "\u001b[32m10-26 02:47:53\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうなんですね、ご自身の考え方のクセに気づかれた。はい、それは大きな発見です。ご自身の力で気づけたことが素晴らしいです。\n",
      "\u001b[32m10-26 02:47:54\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(466944,)\n",
      "\u001b[32m10-26 02:47:54\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ありがとうございます。まだ焦りを感じることもあるんですけど、前よりは落ち着いて考えられるようになった気がします。\n",
      "\u001b[32m10-26 02:47:54\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(323584,)\n",
      "\u001b[32m10-26 02:47:54\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、焦りを感じることも、そうですね。ただ、「落ち着いて考えられるようになった」のは大きな一歩です。これからの生活で、この気づきをどう活かしていけそうでしょうか？\n",
      "\u001b[32m10-26 02:47:54\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(532992,)\n",
      "\u001b[32m10-26 02:47:54\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "また不安になった時に、一度立ち止まって、「これはいつもの考え方のクセかな？」って、自分に問いかけてみようと思います。\n",
      "\u001b[32m10-26 02:47:54\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(357888,)\n",
      "\u001b[32m10-26 02:47:54\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、なるほど。立ち止まって問いかける、良い方法だと思います。これまでの学びを、ご自身で活かしていこうとされているんですね。\n",
      "\u001b[32m10-26 02:47:55\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(425984,)\n",
      "\u001b[32m10-26 02:47:55\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "今日は振り返りができましたね。次回のセッションで、この学びをどう活かすか、具体的に考えていきましょう。次回が最後のセッションになりますが、よろしいでしょうか？\n",
      "\u001b[32m10-26 02:47:55\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(519680,)\n",
      "\u001b[32m10-26 02:47:55\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、ありがとうございます。お願いします。\n",
      "\u001b[32m10-26 02:47:55\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(159232,)\n",
      "\u001b[32m10-26 02:47:55\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、では次回の予約を確認させていただきますね。\n",
      "\u001b[32m10-26 02:47:55\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(166912,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:30\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_77...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_77\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m145.435\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:21\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_77...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_77\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m134.762\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-26 02:53:16\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こんにちは。Bさん、今日は少しお話しを聞かせていただけますか？最近、何か気になることや、心の中で感じていることはありますか？\n",
      "\u001b[32m10-26 02:53:16\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(448512,)\n",
      "\u001b[32m10-26 02:53:16\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、先生。最近、なんだかずっと気持ちが落ち着かなくて…。もっとこう、ちゃんとしないといけないのに、全然できていないような気がして。\n",
      "\u001b[32m10-26 02:53:16\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(436224,)\n",
      "\u001b[32m10-26 02:53:16\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうなんですね。ちゃんとしないといけない、と感じていらっしゃるんですね。ええ、具体的には、どのような時にそう思われることが多いですか？\n",
      "\u001b[32m10-26 02:53:16\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(424448,)\n",
      "\u001b[32m10-26 02:53:16\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "仕事でも、家でも、やるべきことがたくさんあるはずなのに、なかなか手がつかなくて。周りの人はもっと効率的にこなしているのに、自分はダメだなって。\n",
      "\u001b[32m10-26 02:53:17\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(487424,)\n",
      "\u001b[32m10-26 02:53:17\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど、周りの方はもっと効率的にこなしているように見えて、「自分はダメだ」と感じてしまうのですね。はい。その「やるべきこと」というのは、Bさんにとってどのようなことなのでしょう？\n",
      "\u001b[32m10-26 02:53:17\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(602112,)\n",
      "\u001b[32m10-26 02:53:17\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "例えば、職場では「もっと積極的に意見を言うべきだ」とか、家では「部屋をもっと綺麗に保つべきだ」とか…。そういう「〜べき」がたくさん頭の中にあって。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: JPCommonLabel_push_word() in jpcommon_label.c: First mora should not be long vowel symbol.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-26 02:53:17\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(452608,)\n",
      "\u001b[32m10-26 02:53:17\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、たくさんの「〜べき」が頭の中にあるのですね。そうなんですね。もし、仮にですが、その「積極的に意見を言うべき」というのを、今日一日、少しだけ「言わない」という選択をしてみたら、Bさんにとってどんな気持ちになるでしょう？\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: JPCommonLabel_push_word() in jpcommon_label.c: First mora should not be long vowel symbol.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-26 02:53:18\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(728576,)\n",
      "\u001b[32m10-26 02:53:18\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "え、言わない選択…ですか。うーん、きっと不安になると思います。「また何も言えなかった」って、後悔しそうですし。\n",
      "\u001b[32m10-26 02:53:18\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(397312,)\n",
      "\u001b[32m10-26 02:53:18\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうですね、不安や後悔を感じるかもしれませんね。はい。では、その「言わない」選択をしたとして、実際に周りの方や状況に、何か具体的な変化が起こると思いますか？\n",
      "\u001b[32m10-26 02:53:18\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(583168,)\n",
      "\u001b[32m10-26 02:53:18\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "うーん…。もしかしたら、そんなに大きな変化はないのかもしれません。誰もそこまで私の発言に注目しているわけではない、のかもしれませんね…。\n",
      "\u001b[32m10-26 02:53:18\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(444928,)\n",
      "\u001b[32m10-26 02:53:18\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど、そうかもしれませんね。周りの方々の反応について、少し違う見方もできるかもしれない、ということですね。ええ。今日は、Bさんの心の中にある「〜べき」というお気持ちについて、少しお話しを伺うことができました。この「〜べき」という気持ちが、もしかしたらBさんの心を少し窮屈にさせているのかもしれませんね。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: JPCommonLabel_push_word() in jpcommon_label.c: First mora should not be long vowel symbol.\n",
      "WARNING: JPCommonLabel_push_word() in jpcommon_label.c: First mora should not be long vowel symbol.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-26 02:53:19\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(1097728,)\n",
      "\u001b[32m10-26 02:53:19\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、そうかもしれません…。なんだか、少し考えさせられます。\n",
      "\u001b[32m10-26 02:53:19\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(211456,)\n",
      "\u001b[32m10-26 02:53:19\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、今日はここまでとしましょうか。次回の面接では、この「〜べき」というお気持ちについて、もう少し詳しく一緒に考えていけたらと思います。何か、今日の面接で気づいたことや、考えてみたいことがあれば、メモしておいていただけますか？\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: JPCommonLabel_push_word() in jpcommon_label.c: First mora should not be long vowel symbol.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-26 02:53:19\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(780288,)\n",
      "\u001b[32m10-26 02:53:19\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、わかりました。ありがとうございます。\n",
      "\u001b[32m10-26 02:53:20\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(161792,)\n",
      "\u001b[32m10-26 02:53:20\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "では、また来週お会いしましょう。\n",
      "\u001b[32m10-26 02:53:20\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(104960,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:38\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_78...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_78\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m151.950\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:44\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_78...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_78\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m155.912\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-26 02:59:01\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こんにちは、今日はよくお越しくださいました。どうぞ、楽になさってくださいね。\n",
      "\u001b[32m10-26 02:59:02\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(261120,)\n",
      "\u001b[32m10-26 02:59:02\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こんにちは。ありがとうございます。なんだか、少し緊張しています。\n",
      "\u001b[32m10-26 02:59:02\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(233472,)\n",
      "\u001b[32m10-26 02:59:02\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、そうですよね。初めてお話しされるのは、どなたでも緊張されるものですから。今日はどんなことをお話しいただけますか？無理のない範囲で、ゆっくりお聞かせくださいね。\n",
      "\u001b[32m10-26 02:59:02\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(565248,)\n",
      "\u001b[32m10-26 02:59:02\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。最近、仕事のことで漠然とした不安があるのと、日常生活でもなんだか焦燥感を感じることが多くて…。\n",
      "\u001b[32m10-26 02:59:03\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(366592,)\n",
      "\u001b[32m10-26 02:59:03\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうなんですね。お仕事での不安と、日常生活での焦燥感、ですか。ええ、具体的にどのような時にそう感じられますか？\n",
      "\u001b[32m10-26 02:59:03\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(444928,)\n",
      "\u001b[32m10-26 02:59:03\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "仕事では、周りの人がどんどん先に進んでいるように感じてしまって。自分だけ立ち止まっているような気がして、それがすごくストレスになっています。家に帰っても、あれもこれもやらなきゃって気持ちばかりが募って、結局何も手につかない、みたいな…。\n",
      "\u001b[32m10-26 02:59:03\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(792064,)\n",
      "\u001b[32m10-26 02:59:03\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど。周りの方と比べてご自身が立ち止まっているように感じていらっしゃるのですね。はい。それで、お家に帰られても、その「やらなきゃ」という気持ちが募って、かえって何も手につかなくなってしまう、と。それはとてもお辛いですね。その「漠然とした不安」や「焦燥感」というのは、もう少し詳しくお伺いできますか？\n",
      "\u001b[32m10-26 02:59:04\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(1067008,)\n",
      "\u001b[32m10-26 02:59:04\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。そうですね…。漠然としているのは、何が不安なのか、自分でもはっきり分からないんです。ただ、このままでいいのか、もっと頑張らないといけないんじゃないかって、いつも何かに追い立てられているような感覚です。\n",
      "\u001b[32m10-26 02:59:04\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(688640,)\n",
      "\u001b[32m10-26 02:59:04\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうなんですね。「このままでいいのか」「もっと頑張らないといけないんじゃないか」というお気持ちなんですね。ええ。そうしたお気持ちを抱えていらっしゃる中で、日々過ごされるのは、本当に大変なことだったと思います。今日は、お話しくださって、ありがとうございます。\n",
      "\u001b[32m10-26 02:59:04\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(861696,)\n",
      "\u001b[32m10-26 02:59:04\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "いえ、こうしてお話しできて、少し楽になった気がします。\n",
      "\u001b[32m10-26 02:59:05\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(185856,)\n",
      "\u001b[32m10-26 02:59:05\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、それはよかったです。今日はここまでとなりますが、また次回、もう少し詳しくお話しを伺わせてくださいね。今日お話しいただいたことを整理して、一緒に考えていきましょう。\n",
      "\u001b[32m10-26 02:59:05\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(589824,)\n",
      "\u001b[32m10-26 02:59:05\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、ぜひお願いします。今日はありがとうございました。\n",
      "\u001b[32m10-26 02:59:05\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(182784,)\n",
      "\u001b[32m10-26 02:59:05\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こちらこそ、ありがとうございました。またお待ちしておりますね。\n",
      "\u001b[32m10-26 02:59:05\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(201216,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:32\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_79...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_79\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m144.184\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:36\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_79...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_79\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m145.511\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-26 03:04:34\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こんにちは、Bさん。今週はいかがお過ごしでしたか？\n",
      "\u001b[32m10-26 03:04:35\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(157696,)\n",
      "\u001b[32m10-26 03:04:35\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こんにちは、先生。うーん、なんだかバタバタして、あまり落ち着かなかった一週間でした。\n",
      "\u001b[32m10-26 03:04:35\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(297472,)\n",
      "\u001b[32m10-26 03:04:35\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうでしたか。落ち着かない一週間だったんですね。\n",
      "\u001b[32m10-26 03:04:35\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(154624,)\n",
      "\u001b[32m10-26 03:04:35\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。仕事でも、もっと効率よくこなすべきなのに、思うようにいかなくて。家でも、ちゃんと家事を完璧にやらなきゃって。\n",
      "\u001b[32m10-26 03:04:36\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(389632,)\n",
      "\u001b[32m10-26 03:04:36\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど。『もっと効率よくこなすべき』とか、『完璧にやらなきゃ』と感じていらっしゃる。\n",
      "\u001b[32m10-26 03:04:36\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(276992,)\n",
      "\u001b[32m10-26 03:04:36\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、そうなんです。なんだかいつも、もっとこうすべきだ、ああすべきだって、自分を追い立てているような気がして。\n",
      "\u001b[32m10-26 03:04:36\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(337920,)\n",
      "\u001b[32m10-26 03:04:36\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ご自身を追い立てているような感じがすると。それは、どんな時に特に強く感じられますか？\n",
      "\u001b[32m10-26 03:04:36\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(293376,)\n",
      "\u001b[32m10-26 03:04:36\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "他の人と比べてしまった時でしょうか。周りの人はもっと上手にやってるように見えて、自分だけができてないんじゃないかって。\n",
      "\u001b[32m10-26 03:04:36\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(366592,)\n",
      "\u001b[32m10-26 03:04:36\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "周りの方と比べてしまう時に、そういった『〜すべきだ』というお気持ちが強くなるんですね。そう感じると、どんなお気持ちになりますか？\n",
      "\u001b[32m10-26 03:04:37\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(421888,)\n",
      "\u001b[32m10-26 03:04:37\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "焦りとか、自分はダメだなって。もっと頑張らなきゃって思うんですけど、疲れてしまって。\n",
      "\u001b[32m10-26 03:04:37\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(284672,)\n",
      "\u001b[32m10-26 03:04:37\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "焦りや『自分はダメだ』というお気持ちにつながってしまうんですね。それがまた『もっと頑張らなきゃ』とつながって、お疲れになる、ということでしょうか。\n",
      "\u001b[32m10-26 03:04:37\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(444928,)\n",
      "\u001b[32m10-26 03:04:37\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、まさにその通りです。\n",
      "\u001b[32m10-26 03:04:37\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(96768,)\n",
      "\u001b[32m10-26 03:04:37\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ありがとうございます。Bさんが『〜すべきだ』と感じていることが、少し見えてきましたね。ご自身を苦しめているのかもしれません。\n",
      "\u001b[32m10-26 03:04:37\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(423936,)\n",
      "\u001b[32m10-26 03:04:37\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうかもしれません。でも、そう思わないようにするって、どうしたらいいんでしょうか。\n",
      "\u001b[32m10-26 03:04:38\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(231424,)\n",
      "\u001b[32m10-26 03:04:38\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、すぐに変えるのは難しいかもしれません。ですが、今日気づかれたように、まず『あ、今私、〜すべきだと思っているな』と、ご自身の考えに気づくことが大切なんです。今日はこの『〜すべき思考』が、Bさんの気持ちにどう影響しているか、少し一緒に見ていけましたね。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: JPCommonLabel_push_word() in jpcommon_label.c: First mora should not be long vowel symbol.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-26 03:04:38\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(905216,)\n",
      "\u001b[32m10-26 03:04:38\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、気づけて良かったです。\n",
      "\u001b[32m10-26 03:04:38\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(102912,)\n",
      "\u001b[32m10-26 03:04:38\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、素晴らしい気づきだと思います。次回は、この『〜すべき思考』が本当に『すべき』なのか、もう少し別の見方もできるのか、一緒に考えていきましょう。来週までに、『〜すべきだ』と思った時に、『今、〜すべきだと思ったな』とメモする簡単なホームワークはいかがでしょうか。気づくだけで構いません。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: JPCommonLabel_push_word() in jpcommon_label.c: First mora should not be long vowel symbol.\n",
      "WARNING: JPCommonLabel_push_word() in jpcommon_label.c: First mora should not be long vowel symbol.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-26 03:04:39\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(964608,)\n",
      "\u001b[32m10-26 03:04:39\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、やってみます。\n",
      "\u001b[32m10-26 03:04:39\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(79360,)\n",
      "\u001b[32m10-26 03:04:39\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ありがとうございます。では、来週の同じ時間にお待ちしておりますね。\n",
      "\u001b[32m10-26 03:04:39\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(219648,)\n",
      "\u001b[32m10-26 03:04:39\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、ありがとうございました。\n",
      "\u001b[32m10-26 03:04:39\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(102912,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:30\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_80...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_80\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m145.306\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:35\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_80...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_80\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m146.371\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-26 03:10:21\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こんにちは、Bさん。今日はどんなご様子でしたか？何か、最近気になっていることはありますか？\n",
      "\u001b[32m10-26 03:10:21\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(293888,)\n",
      "\u001b[32m10-26 03:10:21\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こんにちは、先生。最近、仕事でもプライベートでも、「もっとこうすべき」なのに、できてないなと感じることが多くて、なんだか焦っているんです。\n",
      "\u001b[32m10-26 03:10:22\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(447488,)\n",
      "\u001b[32m10-26 03:10:22\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど、「もっとこうすべき」と。ええ、焦りを感じるのですね。具体的に、どんな時にそう感じますか？\n",
      "\u001b[32m10-26 03:10:22\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(366080,)\n",
      "\u001b[32m10-26 03:10:22\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。仕事だと、もっと効率的に動いて、周りの期待に応えなければいけないのに、って。家でも、完璧にこなすべきだと思ってしまって。\n",
      "\u001b[32m10-26 03:10:22\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(437760,)\n",
      "\u001b[32m10-26 03:10:22\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうなんですね。「周りの期待に応えなければいけない」「完璧にこなすべき」。ええ。その\"すべき\"という気持ちが強いのですね。\n",
      "\u001b[32m10-26 03:10:22\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(419328,)\n",
      "\u001b[32m10-26 03:10:22\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "うーん…昔からかもしれません。完璧にしないと、認められないというか、不安になるというか…。\n",
      "\u001b[32m10-26 03:10:23\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(323584,)\n",
      "\u001b[32m10-26 03:10:23\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "認められないと不安に。はい。その「完璧にすべき」という思いが、できない時に焦りや不安を生むのでしょうか。\n",
      "\u001b[32m10-26 03:10:23\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(381440,)\n",
      "\u001b[32m10-26 03:10:23\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "まさに。疲れていても「休むべきじゃない」と思ってしまって、結局何も手につかなくなったり…。\n",
      "\u001b[32m10-26 03:10:23\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(288768,)\n",
      "\u001b[32m10-26 03:10:23\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど。休むべきじゃない、と。そんな時、心の中では、どんな声が聞こえてきますか？\n",
      "\u001b[32m10-26 03:10:23\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(329728,)\n",
      "\u001b[32m10-26 03:10:23\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "「もっと頑張れ」とか、「こんなことじゃダメだ」とか…自分を責める声が聞こえます。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: JPCommonLabel_insert_pause() in jpcommon_label.c: First mora should not be short pause.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-26 03:10:24\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(261632,)\n",
      "\u001b[32m10-26 03:10:24\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうなんですね。はい。ご自身を責めてしまう。その声は、Bさんを助けていますか？それとも、苦しくさせていますか？\n",
      "\u001b[32m10-26 03:10:24\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(446464,)\n",
      "\u001b[32m10-26 03:10:24\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "苦しい、ですね。でも、そうしないといけない、と思ってしまうんです。\n",
      "\u001b[32m10-26 03:10:24\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(207872,)\n",
      "\u001b[32m10-26 03:10:24\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、よくわかります。完璧に「すべき」という考え方は、Bさんを支えてきた面もあるでしょう。でも今、それが少し苦しめているのかもしれない、という視点も、少しだけ持ってみてはいかがでしょうか。\n",
      "\u001b[32m10-26 03:10:24\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(659968,)\n",
      "\u001b[32m10-26 03:10:24\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "苦しめている、ですか…。\n",
      "\u001b[32m10-26 03:10:25\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(68096,)\n",
      "\u001b[32m10-26 03:10:25\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ。完璧ではない自分を受け入れることは、本当に「ダメなこと」なのでしょうか？もう少しご自分に優しくなってもいいのかもしれない、と、少しだけ考えてみませんか？\n",
      "\u001b[32m10-26 03:10:25\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(543232,)\n",
      "\u001b[32m10-26 03:10:25\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "優しく…。そうですね…。難しいですけど、考えてみます。\n",
      "\u001b[32m10-26 03:10:25\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(205824,)\n",
      "\u001b[32m10-26 03:10:25\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ありがとうございます。今日は、Bさんの「すべき思考」についてお話しできましたね。次回までに、もしよかったら、「こうすべきだ」と感じた時に、本当に必要なことなのか、少しだけ立ち止まって考えてみていただけますか？\n",
      "\u001b[32m10-26 03:10:25\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(683008,)\n",
      "\u001b[32m10-26 03:10:25\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、やってみます。\n",
      "\u001b[32m10-26 03:10:25\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(78848,)\n",
      "\u001b[32m10-26 03:10:25\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ありがとうございます。焦らず、少しずつで大丈夫ですよ。では、今日はここまでにしましょうか。\n",
      "\u001b[32m10-26 03:10:26\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(309760,)\n",
      "\u001b[32m10-26 03:10:26\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ありがとうございました。\n",
      "\u001b[32m10-26 03:10:26\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(60928,)\n",
      "\u001b[32m10-26 03:10:26\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こちらこそ、ありがとうございました。来週またお会いしましょう。\n",
      "\u001b[32m10-26 03:10:26\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(197632,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:37\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_81...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_81\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m149.888\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:37\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:06\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_81...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_81\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m152.904\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-26 03:16:23\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こんにちは。今日は〇〇さんとお話できるのを楽しみにしていました。今のお気持ち、お聞かせいただけますか？\n",
      "\u001b[32m10-26 03:16:23\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(341504,)\n",
      "\u001b[32m10-26 03:16:23\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。最近、仕事もプライベートも漠然とした不安と焦りがあって、何が原因か分からなくて。\n",
      "\u001b[32m10-26 03:16:23\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(318464,)\n",
      "\u001b[32m10-26 03:16:23\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうなんですね。ええ、不安や焦りを感じていらっしゃるんですね。お辛いですね。どんな時にそう感じますか？\n",
      "\u001b[32m10-26 03:16:24\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(365568,)\n",
      "\u001b[32m10-26 03:16:24\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "仕事では締め切りに追われるようで。家ではあれこれ考えてしまい、何も手につかず時間だけが過ぎていく感じです。\n",
      "\u001b[32m10-26 03:16:24\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(357888,)\n",
      "\u001b[32m10-26 03:16:24\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど、はい。仕事もご自宅でも、心が休まらないんですね。\n",
      "\u001b[32m10-26 03:16:24\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(240128,)\n",
      "\u001b[32m10-26 03:16:24\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、週末も何かすべきと焦ってしまい、結局疲れて自己嫌悪です。悪循環だなと。\n",
      "\u001b[32m10-26 03:16:24\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(330240,)\n",
      "\u001b[32m10-26 03:16:24\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、そうだったのですね。大変な日々を過ごされていますね。この状態はいつ頃からですか？\n",
      "\u001b[32m10-26 03:16:25\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(322560,)\n",
      "\u001b[32m10-26 03:16:25\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "この1年くらいですね。部署異動で責任が増えてから、こうなってきた気がします。\n",
      "\u001b[32m10-26 03:16:25\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(244224,)\n",
      "\u001b[32m10-26 03:16:25\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "部署異動がきっかけだったのですね。今日は全体像が少し見えました。大きな一歩です。\n",
      "\u001b[32m10-26 03:16:25\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(279552,)\n",
      "\u001b[32m10-26 03:16:25\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、話せて少しすっきりしました。\n",
      "\u001b[32m10-26 03:16:25\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(128512,)\n",
      "\u001b[32m10-26 03:16:25\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "良かったです。次回は、具体的な辛さや、気分が楽になった経験なども聞かせてもらえますか？\n",
      "\u001b[32m10-26 03:16:26\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(303104,)\n",
      "\u001b[32m10-26 03:16:26\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、わかりました。次回もよろしくお願いします。\n",
      "\u001b[32m10-26 03:16:26\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(176128,)\n",
      "\u001b[32m10-26 03:16:26\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、もちろんです。今日はありがとうございました。どうぞお気をつけて。\n",
      "\u001b[32m10-26 03:16:26\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(220160,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:19\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_82...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_82\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m129.065\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:16\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_82...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_82\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m125.191\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-26 03:21:14\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こんにちは、Bさん。今日はどんな様子でいらっしゃいますか？\n",
      "\u001b[32m10-26 03:21:14\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(167424,)\n",
      "\u001b[32m10-26 03:21:14\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こんにちは、先生。なんだか、最近ずっと焦っているような気がして。あれもこれも「しなきゃいけない」って、いつも頭の中がそれでいっぱいで。\n",
      "\u001b[32m10-26 03:21:14\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(437760,)\n",
      "\u001b[32m10-26 03:21:14\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど、そうなんですね。「あれもこれも、しなきゃいけない」と、頭の中でずっと感じていらっしゃるんですね。ええ。\n",
      "\u001b[32m10-26 03:21:15\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(389120,)\n",
      "\u001b[32m10-26 03:21:15\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。仕事もそうだし、家事も、もっと効率よく「すべき」だし。趣味の時間だって、もっと有意義に「使わなきゃ」って思ってしまうんです。\n",
      "\u001b[32m10-26 03:21:15\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(422912,)\n",
      "\u001b[32m10-26 03:21:15\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、そうなんですね。普段の生活の中で、たくさんの「こうすべきだ」「こうあらねばならない」というお気持ちが、Bさんをずっとせかしているような感じ、でしょうか。\n",
      "\u001b[32m10-26 03:21:15\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(522752,)\n",
      "\u001b[32m10-26 03:21:15\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "まさに、そんな感じです。それができないと、なんだか自分はダメだって思ってしまって。\n",
      "\u001b[32m10-26 03:21:15\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(273920,)\n",
      "\u001b[32m10-26 03:21:15\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど。そうなんですね。その「できないとダメだ」というお気持ちは、どんな時に特に強く感じられますか？\n",
      "\u001b[32m10-26 03:21:16\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(359424,)\n",
      "\u001b[32m10-26 03:21:16\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "例えば、週末に何もせずゴロゴロしていると、「本当はもっと何か生産的なことをすべきだったんじゃないか」って、強い罪悪感に襲われるんです。\n",
      "\u001b[32m10-26 03:21:16\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(420352,)\n",
      "\u001b[32m10-26 03:21:16\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、ええ。せっかくの休みなのに、心から休めないような、そんな感覚ですね。はい。\n",
      "\u001b[32m10-26 03:21:16\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(317440,)\n",
      "\u001b[32m10-26 03:21:16\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうなんです。常に何かに追われているみたいで。\n",
      "\u001b[32m10-26 03:21:16\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(154624,)\n",
      "\u001b[32m10-26 03:21:16\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど。ご自身の中で「こうあるべきだ」という理想が、Bさんをとても一生懸命にさせている一方で、少し苦しめているのかもしれませんね。そういったお気持ちについて、もう少し詳しく、一緒に見ていくことはできそうでしょうか。\n",
      "\u001b[32m10-26 03:21:17\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(756736,)\n",
      "\u001b[32m10-26 03:21:17\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、なんだか、そうかもしれないです。\n",
      "\u001b[32m10-26 03:21:17\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(118784,)\n",
      "\u001b[32m10-26 03:21:17\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ありがとうございます。では、今日はその「すべき」というお気持ちについて、もう少し掘り下げて考えてみましょうか。ええ。\n",
      "\u001b[32m10-26 03:21:17\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(377856,)\n",
      "\u001b[32m10-26 03:21:17\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "お願いします。\n",
      "\u001b[32m10-26 03:21:17\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(43520,)\n",
      "\u001b[32m10-26 03:21:17\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。承知いたしました。今日は、この「すべき思考」がBさんにどんな影響を与えているか、そして、もしかしたら少しだけ、その思考を手放してみることで、どんな変化がありそうか、という点について、少しだけ考えてみましょう。いかがでしょうか。\n",
      "\u001b[32m10-26 03:21:18\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(779264,)\n",
      "\u001b[32m10-26 03:21:18\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、やってみたいです。\n",
      "\u001b[32m10-26 03:21:18\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(89600,)\n",
      "\u001b[32m10-26 03:21:18\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ありがとうございます。では、次回までに、もしよかったら、普段の生活の中で「これ、すべきって思ってるな」と感じたことを、メモしてみていただくのはいかがでしょうか。気づくだけで大丈夫ですよ。\n",
      "\u001b[32m10-26 03:21:18\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(595456,)\n",
      "\u001b[32m10-26 03:21:18\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "わかりました。やってみます。\n",
      "\u001b[32m10-26 03:21:18\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(102912,)\n",
      "\u001b[32m10-26 03:21:18\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ありがとうございます。そうすることで、少しずつご自身のパターンが見えてくるかもしれませんね。今日のところはここまでとしましょう。次回は、そのメモを見ながら、またお話ししましょうね。\n",
      "\u001b[32m10-26 03:21:18\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(576512,)\n",
      "\u001b[32m10-26 03:21:18\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、ありがとうございました。\n",
      "\u001b[32m10-26 03:21:19\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(101376,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:34\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_83...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_83\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m146.870\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:37\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_83...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_83\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m148.801\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-26 03:27:03\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "Bさん、今日はありがとうございます。どんなことでお話しいただけますか？\n",
      "\u001b[32m10-26 03:27:04\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(219648,)\n",
      "\u001b[32m10-26 03:27:04\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。最近、漠然とした不安や焦りがあって、仕事も手につきません。\n",
      "\u001b[32m10-26 03:27:04\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(232448,)\n",
      "\u001b[32m10-26 03:27:04\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうなんですね。不安や焦り、お仕事にも影響が。\n",
      "\u001b[32m10-26 03:27:04\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(183808,)\n",
      "\u001b[32m10-26 03:27:04\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、ミスが増えてしまって。それがまたストレスで。\n",
      "\u001b[32m10-26 03:27:04\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(173568,)\n",
      "\u001b[32m10-26 03:27:04\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど。以前と違うと感じて、お辛いですね。ミスがストレスに。\n",
      "\u001b[32m10-26 03:27:05\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(236032,)\n",
      "\u001b[32m10-26 03:27:05\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。仕事だけでなく、家でも休まらず、寝つきも悪くて。\n",
      "\u001b[32m10-26 03:27:05\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(217600,)\n",
      "\u001b[32m10-26 03:27:05\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、そうですか。日常生活にも影響が。寝つきが悪いのはお辛いでしょう。\n",
      "\u001b[32m10-26 03:27:05\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(274432,)\n",
      "\u001b[32m10-26 03:27:05\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "本当に。いつからか、半年くらい前からかと。\n",
      "\u001b[32m10-26 03:27:05\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(153088,)\n",
      "\u001b[32m10-26 03:27:05\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "半年くらい前からですか。不安が、お仕事や日々の生活に影響しているのですね。\n",
      "\u001b[32m10-26 03:27:06\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(274432,)\n",
      "\u001b[32m10-26 03:27:06\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "まさしく。話せて、少し楽になりました。\n",
      "\u001b[32m10-26 03:27:06\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(144896,)\n",
      "\u001b[32m10-26 03:27:06\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "それはよかったです。次回、この不安がどんな時に強くなるか、詳しくお伺いできますか？\n",
      "\u001b[32m10-26 03:27:06\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(304640,)\n",
      "\u001b[32m10-26 03:27:06\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、ぜひ。\n",
      "\u001b[32m10-26 03:27:06\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(49664,)\n",
      "\u001b[32m10-26 03:27:06\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ。では、今日はここまで。来週の同じ時間でよろしいでしょうか。\n",
      "\u001b[32m10-26 03:27:07\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(213504,)\n",
      "\u001b[32m10-26 03:27:07\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、大丈夫です。ありがとうございました。\n",
      "\u001b[32m10-26 03:27:07\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(158208,)\n",
      "\u001b[32m10-26 03:27:07\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "お疲れ様でした。次回もよろしくお願いしますね。\n",
      "\u001b[32m10-26 03:27:07\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(167424,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:12\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_84...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_84\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m124.664\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h\u001b[?25l"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:10\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:04\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_84...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_84\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m122.210\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-26 03:31:48\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "Bさん、今日はよくお越しくださいました。少し緊張されていますか？今日はゆっくりお話を聞かせていただけたらと思いますので、どうぞ楽にしてくださいね。\n",
      "\u001b[32m10-26 03:31:49\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(503296,)\n",
      "\u001b[32m10-26 03:31:49\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、ありがとうございます。なんだか、うまく話せるか不安で…。最近、仕事でもプライベートでも、漠然とした焦りや不安を感じることが多くて、どうしたらいいのか分からなくなってしまって。\n",
      "\u001b[32m10-26 03:31:49\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(624128,)\n",
      "\u001b[32m10-26 03:31:49\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうなんですね。漠然とした焦りや不安…、それはとてもお辛い気持ちですね。どんな時に特にそう感じることが多いですか？差し支えなければ、もう少し詳しくお聞かせいただけますか。\n",
      "\u001b[32m10-26 03:31:49\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(616960,)\n",
      "\u001b[32m10-26 03:31:49\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ。仕事では、常に締め切りに追われているような感覚で、完璧にこなそうとすると時間がいくらあっても足りないんです。かといって手を抜くと、評価が下がるんじゃないかと心配で。結局、いつも追われているような気持ちで、休んでも休んだ気がしなくて…。\n",
      "\u001b[32m10-26 03:31:50\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(795648,)\n",
      "\u001b[32m10-26 03:31:50\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど、常にプレッシャーを感じていらっしゃるんですね。完璧にこなしたいという気持ちと、時間の制約との間で葛藤されているのかもしれませんね。そういったお気持ちが、日常生活にも影響していると感じることはありますか？\n",
      "\u001b[32m10-26 03:31:50\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(744960,)\n",
      "\u001b[32m10-26 03:31:50\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、まさにその通りです。仕事のことが頭から離れず、家でリラックスしようとしても、なんだか落ち着かなくて。夜も寝つきが悪くなったり、朝起きても体が重かったりして…。\n",
      "\u001b[32m10-26 03:31:50\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(594432,)\n",
      "\u001b[32m10-26 03:31:50\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、そうなんですね。仕事のプレッシャーが、睡眠にも影響していると。それは本当に大変ですね。そういった状態は、いつ頃から続くようになりましたか？\n",
      "\u001b[32m10-26 03:31:51\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(552960,)\n",
      "\u001b[32m10-26 03:31:51\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はっきりとは覚えていないんですが、半年くらい前でしょうか。新しいプロジェクトが始まってから、特にひどくなったような気がします。それまでは、もう少しオンオフの切り替えができていたんですけど…。\n",
      "\u001b[32m10-26 03:31:51\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(568832,)\n",
      "\u001b[32m10-26 03:31:51\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど、新しいプロジェクトがきっかけで、より負担が増えたと感じていらっしゃるんですね。それまではできていた切り替えが難しくなったことで、心身ともに疲れが溜まっている状況なのですね。\n",
      "\u001b[32m10-26 03:31:51\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(611328,)\n",
      "\u001b[32m10-26 03:31:51\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、本当にその通りで…。このままではいけない、でもどうしたらいいのか分からなくて、今日ここに来ました。\n",
      "\u001b[32m10-26 03:31:51\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(371200,)\n",
      "\u001b[32m10-26 03:31:51\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ。今日、こうしてお話ししてくださったこと、本当にありがとうございます。まずは、今抱えている漠然とした不安の背景を少しずつ理解していくことが大切かもしれませんね。次回は、もう少し詳しく、日々の生活の中で具体的に困っていることや、どんな時に気持ちが楽になるのかなど、お伺いできたらと思うのですが、いかがでしょうか。\n",
      "\u001b[32m10-26 03:31:52\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(1128448,)\n",
      "\u001b[32m10-26 03:31:52\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、ぜひお願いします。少し話しただけでも、気持ちが楽になった気がします。\n",
      "\u001b[32m10-26 03:31:52\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(266752,)\n",
      "\u001b[32m10-26 03:31:52\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "それは良かったです。今日のところはここまでとしましょう。次回は来週の同じ時間でよろしいでしょうか。また何か気になることがあれば、いつでもご連絡くださいね。\n",
      "\u001b[32m10-26 03:31:52\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(507904,)\n",
      "\u001b[32m10-26 03:31:52\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、ありがとうございます。よろしくお願いします。\n",
      "\u001b[32m10-26 03:31:53\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(180736,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:41\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_85...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_85\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m153.740\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:43\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_85...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_85\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m155.671\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-26 03:37:36\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "〇〇さん、こんにちは。今日はこれまでのセッションを少し振り返って、これからのことを一緒に考えていきませんか？\n",
      "\u001b[32m10-26 03:37:36\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(344064,)\n",
      "\u001b[32m10-26 03:37:36\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、そうですね。もうそんな時期なんですね…早いような、長かったような。\n",
      "\u001b[32m10-26 03:37:37\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(287232,)\n",
      "\u001b[32m10-26 03:37:37\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、そうですね。色々なことをお話してくださいましたもんね。これまでのセッションを振り返ってみて、〇〇さんの中で何か変化を感じることはありますか？\n",
      "\u001b[32m10-26 03:37:37\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(475136,)\n",
      "\u001b[32m10-26 03:37:37\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうですね…最初は、仕事も日常生活も、漠然とした不安ばかりで、何から手をつけていいのかも分からなかったんですけど。\n",
      "\u001b[32m10-26 03:37:37\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(402944,)\n",
      "\u001b[32m10-26 03:37:37\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、はい、そうでしたね。漠然とした不安、というお話でしたね。\n",
      "\u001b[32m10-26 03:37:37\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(211968,)\n",
      "\u001b[32m10-26 03:37:37\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "それが、こう、具体的に何が不安なのか、どうしてそう感じるのか、少しずつ整理できるようになってきた気がします。特に、自分の考え方のクセに気づけたのは大きかったです。\n",
      "\u001b[32m10-26 03:37:38\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(553472,)\n",
      "\u001b[32m10-26 03:37:38\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど。ご自身の考え方のクセに気づかれた、ということですね。それは大きな変化ですね。\n",
      "\u001b[32m10-26 03:37:38\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(323072,)\n",
      "\u001b[32m10-26 03:37:38\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。おかげで、以前はすぐに「ダメだ」って思っていたことでも、「まあ、そういう考え方もあるか」って、少し冷静になれるようになりました。\n",
      "\u001b[32m10-26 03:37:38\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(434176,)\n",
      "\u001b[32m10-26 03:37:38\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうなんですね。それは素晴らしい気づきですね。ご自身で客観的に捉えられるようになった、ということでしょうか。\n",
      "\u001b[32m10-26 03:37:38\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(360448,)\n",
      "\u001b[32m10-26 03:37:38\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、本当に。気持ちが楽になった部分もありますし、以前よりは、次の一歩を考えられるようになった気がします。\n",
      "\u001b[32m10-26 03:37:39\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(372224,)\n",
      "\u001b[32m10-26 03:37:39\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、それは何よりです。ご自身で問題に対処する糸口を見つけられた、ということですね。ご自身で頑張ってこられた証拠ですよ。\n",
      "\u001b[32m10-26 03:37:39\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(446464,)\n",
      "\u001b[32m10-26 03:37:39\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ありがとうございます。ただ、これからは一人でやっていけるかな、という不安も少しあって…。\n",
      "\u001b[32m10-26 03:37:39\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(260608,)\n",
      "\u001b[32m10-26 03:37:39\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうですね。お気持ち、よくわかります。でも、〇〇さんの中には、もうご自身で乗り越える力、しっかり育ってきていますよ。次回のセッションで、これまでの学びをどう活かしていくか、もう少し具体的に話し合ってみませんか？\n",
      "\u001b[32m10-26 03:37:40\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(731136,)\n",
      "\u001b[32m10-26 03:37:40\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、ぜひお願いします。\n",
      "\u001b[32m10-26 03:37:40\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(90112,)\n",
      "\u001b[32m10-26 03:37:40\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、承知いたしました。では、次回の〇月〇日、またお待ちしておりますね。\n",
      "\u001b[32m10-26 03:37:40\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(250368,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:29\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_86...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_86\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m141.999\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:28\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_86...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_86\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m138.223\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-26 03:43:23\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "Bさん、こんにちは。今日はこれまでのセッションを振り返り、今後についてお話しできればと思いますが、いかがですか？\n",
      "\u001b[32m10-26 03:43:24\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(379904,)\n",
      "\u001b[32m10-26 03:43:24\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、ありがとうございます。もうそんな時期なんですね。少し寂しいですが、振り返るのは良い機会ですね。\n",
      "\u001b[32m10-26 03:43:24\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(373248,)\n",
      "\u001b[32m10-26 03:43:24\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうですね、ええ。この数ヶ月、色々なことをお話しくださいました。ご自身で、『変わったな』とか、『気づきがあったな』と思うことはありますか？\n",
      "\u001b[32m10-26 03:43:24\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(451584,)\n",
      "\u001b[32m10-26 03:43:24\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "うーん、最初は漠然とした不安ばかりだったんですけど、今は少し冷静に、『これはどういう状況だろう』って、立ち止まって考えられるようになった気がします。\n",
      "\u001b[32m10-26 03:43:24\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(461312,)\n",
      "\u001b[32m10-26 03:43:24\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど、立ち止まって考えられるようになった、と。それは大きな変化ですね。ええ、素晴らしいです。ご自身の感情や思考に、客観的に向き合えるようになった、ということですね。\n",
      "\u001b[32m10-26 03:43:25\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(608768,)\n",
      "\u001b[32m10-26 03:43:25\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、そうだと思います。以前はすぐ『だめだ』って決めつけていたのが、『もしかしたら、こんな見方も』って。気持ちも前よりずっと楽になりました。\n",
      "\u001b[32m10-26 03:43:25\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(450560,)\n",
      "\u001b[32m10-26 03:43:25\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "気持ちが楽になったんですね。それは本当に良かったです。この変化を今後も維持していくために、『こんなことを試してみようかな』というアイデアはありますか？\n",
      "\u001b[32m10-26 03:43:25\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(480768,)\n",
      "\u001b[32m10-26 03:43:25\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "日記や散歩は続けたいです。あと、小さなことからでも、自分の意見を周りの人に伝えてみようかなって。\n",
      "\u001b[32m10-26 03:43:25\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(339968,)\n",
      "\u001b[32m10-26 03:43:25\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、具体的で良いアイデアですね。小さなことから、という姿勢が大切です。Bさんならきっとできますよ。\n",
      "\u001b[32m10-26 03:43:26\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(352256,)\n",
      "\u001b[32m10-26 03:43:26\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ありがとうございます。そう言っていただけると、心強いです。\n",
      "\u001b[32m10-26 03:43:26\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(179712,)\n",
      "\u001b[32m10-26 03:43:26\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、もちろんです。今日は変化と今後の目標についてお話しいただけました。次回のセッションで終結ですが、それまでに何か試して、またお話を聞かせてもらえますか？\n",
      "\u001b[32m10-26 03:43:26\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(560128,)\n",
      "\u001b[32m10-26 03:43:26\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、分かりました！また頑張ってきます。今日はありがとうございました。\n",
      "\u001b[32m10-26 03:43:26\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(232448,)\n",
      "\u001b[32m10-26 03:43:26\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、また来週お待ちしていますね。お疲れ様でした。\n",
      "\u001b[32m10-26 03:43:26\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(187904,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:23\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_87...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_87\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m138.404\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:24\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_87...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_87\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m138.942\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-26 03:49:28\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "〇〇さん、こんにちは。今日はこれまでのセッションを振り返りながら、終結に向けて少しお話しできればと思っています。\n",
      "\u001b[32m10-26 03:49:28\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(358400,)\n",
      "\u001b[32m10-26 03:49:28\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、本当にあっという間でした。最初は漠然とした不安ばかりだったのが、今は少し整理できた気がします。\n",
      "\u001b[32m10-26 03:49:29\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(356864,)\n",
      "\u001b[32m10-26 03:49:29\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうでしたね。『性格を直したい』というお話から始まりました。特に、どのようなことが〇〇さんにとって印象に残っていますか？\n",
      "\u001b[32m10-26 03:49:29\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(380928,)\n",
      "\u001b[32m10-26 03:49:29\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "自分の認知・感情・行動が繋がっていると気づけたことです。それを図にしてもらった時、すごく腑に落ちました。\n",
      "\u001b[32m10-26 03:49:29\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(379392,)\n",
      "\u001b[32m10-26 03:49:29\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど、そうなんですね。ご自身のパターンに気づけたのは大きな一歩でしたね。最近、生活の中で何か変化はありましたか？\n",
      "\u001b[32m10-26 03:49:29\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(429568,)\n",
      "\u001b[32m10-26 03:49:29\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。以前は何かあるとすぐ落ち込んでいたのが、『これはいつもの考え方の癖だ』と客観的に見られるように。気持ちも楽になりました。\n",
      "\u001b[32m10-26 03:49:30\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(432640,)\n",
      "\u001b[32m10-26 03:49:30\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "素晴らしい変化ですね。ご自身で冷静に捉えられるようになったと。今後、何か困った時にどう対処できそうですか？\n",
      "\u001b[32m10-26 03:49:30\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(382976,)\n",
      "\u001b[32m10-26 03:49:30\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "一人で抱え込まず、まずは『何に困っているか』具体的に書き出してみる、みたいなことができると思います。家族や友人にも相談しやすくなりました。\n",
      "\u001b[32m10-26 03:49:30\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(455680,)\n",
      "\u001b[32m10-26 03:49:30\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "対処法を見つけ、周りにも頼れるように。ご自身の力で前向きに進めるようになったと、私も感じていますよ。\n",
      "\u001b[32m10-26 03:49:31\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(391680,)\n",
      "\u001b[32m10-26 03:49:31\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ありがとうございます。先生とお話しできて、本当に良かったです。\n",
      "\u001b[32m10-26 03:49:31\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(205824,)\n",
      "\u001b[32m10-26 03:49:31\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、私も発見がありました。今日の振り返りから、来週で一旦セッションを終わりにしましょうか。また何かあれば、ご連絡くださいね。\n",
      "\u001b[32m10-26 03:49:31\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(436224,)\n",
      "\u001b[32m10-26 03:49:31\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、分かりました。ありがとうございます。\n",
      "\u001b[32m10-26 03:49:31\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(166912,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:22\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_88...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_88\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m139.513\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:23\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_88...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_88\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m137.171\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-26 03:54:48\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こんにちは、今日は来てくださってありがとうございます。何か、今お話ししたいことや、気になっていることなどありますか？\n",
      "\u001b[32m10-26 03:54:48\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(371712,)\n",
      "\u001b[32m10-26 03:54:48\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、こんにちは。最近、なんだか仕事でもプライベートでも、ずっと漠然とした不安があって、落ち着かないんです。何から話せばいいのかも、よくわからなくて…。\n",
      "\u001b[32m10-26 03:54:49\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(527360,)\n",
      "\u001b[32m10-26 03:54:49\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうなんですね。漠然とした不安、ということですね。ええ、大丈夫ですよ。今、感じていらっしゃることを、ゆっくりお話しいただければと思います。\n",
      "\u001b[32m10-26 03:54:49\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(490496,)\n",
      "\u001b[32m10-26 03:54:49\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "仕事では、やることがたくさんあるのに、なかなか手につかなくて。周りの人はどんどんこなしているのに、自分だけ置いていかれているような焦りを感じてしまって…。\n",
      "\u001b[32m10-26 03:54:49\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(480256,)\n",
      "\u001b[32m10-26 03:54:49\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど。周りの方と比べて、ご自身だけが置いていかれているような焦りを感じる、ということですね。はい。それは、どんな時に特にそう感じますか？\n",
      "\u001b[32m10-26 03:54:50\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(488960,)\n",
      "\u001b[32m10-26 03:54:50\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "朝ももちろんですし、日中もずっと、何か見落としているんじゃないかとか、うまくできていないんじゃないかって、常に考えてしまって集中できないんです。\n",
      "\u001b[32m10-26 03:54:50\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(443904,)\n",
      "\u001b[32m10-26 03:54:50\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、そうなんですね。常に「できていないんじゃないか」と考えてしまって、集中するのが難しいんですね。仕事以外では、何か影響を感じることはありますか？\n",
      "\u001b[32m10-26 03:54:50\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(532992,)\n",
      "\u001b[32m10-26 03:54:50\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。夜もなかなか眠れなかったり、休日は趣味に全くやる気が出なくて、一日中だらだら過ごしてしまったり…。このままでいいのかな、って焦るばかりで。\n",
      "\u001b[32m10-26 03:54:51\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(511488,)\n",
      "\u001b[32m10-26 03:54:51\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど。お仕事での焦りが、夜の睡眠や休日の過ごし方にも影響していると感じていらっしゃるのですね。はい、そうなんですね。\n",
      "\u001b[32m10-26 03:54:51\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(436736,)\n",
      "\u001b[32m10-26 03:54:51\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。どうにかしたいとは思うんですけど、何から始めたらいいのか分からなくて。\n",
      "\u001b[32m10-26 03:54:51\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(248832,)\n",
      "\u001b[32m10-26 03:54:51\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、そうですよね。漠然とした不安や焦りの中で、どうすれば良いか分からなくなるのは自然なことだと思います。今日お話しくださった仕事の焦りや日常生活への影響、これから一緒に整理していきましょう。\n",
      "\u001b[32m10-26 03:54:51\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(698880,)\n",
      "\u001b[32m10-26 03:54:51\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "今日は、大切なことを教えてくださってありがとうございます。〇〇さんのペースで、少しずつお話しを進めていければと思います。次回は、もう少し具体的に、どんな時に一番しんどいと感じるかなど、お伺いできますでしょうか。\n",
      "\u001b[32m10-26 03:54:52\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(714752,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:34\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:06\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_89...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_89\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m150.226\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:28\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_89...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_89\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m141.734\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-26 04:00:30\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こんにちは、Bさん。今週はいかがでしたか？何か気になることはありましたか？\n",
      "\u001b[32m10-26 04:00:30\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(248320,)\n",
      "\u001b[32m10-26 04:00:30\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こんにちは、A先生。なんだか、ずっと焦っているような感覚で…。やるべきことがあるのに、手がつけられなくて、情けないな、と。\n",
      "\u001b[32m10-26 04:00:30\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(412160,)\n",
      "\u001b[32m10-26 04:00:30\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、なるほど。やるべきことがあっても手がつけられず、ご自身を情けないと感じてしまうのですね。もう少し詳しくお聞かせいただけますか？\n",
      "\u001b[32m10-26 04:00:31\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(465920,)\n",
      "\u001b[32m10-26 04:00:31\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。仕事でも「完璧にやらなきゃ」、家事も「きちんとこなすべき」って、いつも頭にあるんです。でも、疲れてしまって、思うように動けないことが多くて…。\n",
      "\u001b[32m10-26 04:00:31\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(523264,)\n",
      "\u001b[32m10-26 04:00:31\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうなんですね。「完璧に」「きちんとすべき」というお気持ちが、Bさんを支配しているように聞こえます。その「〜すべき」という考えがあると、どんな風に感じられますか？\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: JPCommonLabel_push_word() in jpcommon_label.c: First mora should not be long vowel symbol.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-26 04:00:31\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(539136,)\n",
      "\u001b[32m10-26 04:00:31\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なんだか常に追い立てられているようで、休んでいても落ち着かないんです。休むこと自体、いけないことのような気がしてしまって。\n",
      "\u001b[32m10-26 04:00:32\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(370176,)\n",
      "\u001b[32m10-26 04:00:32\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、休むことさえ、いけないことのように感じてしまうのですね。その「〜すべき」という考えは、いつ頃からBさんの心の中に強くあるように感じられますか？\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: JPCommonLabel_push_word() in jpcommon_label.c: First mora should not be long vowel symbol.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-26 04:00:32\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(524800,)\n",
      "\u001b[32m10-26 04:00:32\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "うーん…物心ついた頃から、そう考えていた気がします。両親が厳しかったのも、あるかもしれません。\n",
      "\u001b[32m10-26 04:00:32\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(351232,)\n",
      "\u001b[32m10-26 04:00:32\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうでしたか。小さい頃からの経験が、今のBさんの考え方に影響しているのですね。もし、少しだけ「〜すべき」という気持ちを緩めてみたら、どうなると思いますか？\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: JPCommonLabel_push_word() in jpcommon_label.c: First mora should not be long vowel symbol.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-26 04:00:32\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(555520,)\n",
      "\u001b[32m10-26 04:00:32\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "緩める、ですか…。少し怖いような気もします。全部崩れてしまうんじゃないかって。\n",
      "\u001b[32m10-26 04:00:33\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(262144,)\n",
      "\u001b[32m10-26 04:00:33\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、そう感じますよね。でも、もしかしたら、肩の力が抜けることで、もっと楽に物事を進められるかもしれません。今日は、「完璧にやらなければ」という考えがBさんを苦しめていること、その背景を探ることができました。次回は、この「〜すべき」という考えとどう向き合うか、一緒に考えていきましょう。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: JPCommonLabel_push_word() in jpcommon_label.c: First mora should not be long vowel symbol.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-26 04:00:33\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(1049088,)\n",
      "\u001b[32m10-26 04:00:33\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、そうですね。少し、気持ちが整理できた気がします。ぜひ、お願いします。\n",
      "\u001b[32m10-26 04:00:33\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(275968,)\n",
      "\u001b[32m10-26 04:00:33\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、良かったです。では、次回の予約ですが、いかがでしょうか？\n",
      "\u001b[32m10-26 04:00:34\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(206336,)\n",
      "\u001b[32m10-26 04:00:34\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "来週の同じ時間でお願いします。\n",
      "\u001b[32m10-26 04:00:34\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(96768,)\n",
      "\u001b[32m10-26 04:00:34\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "かしこまりました。では、また来週、お待ちしておりますね。\n",
      "\u001b[32m10-26 04:00:34\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(200192,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:27\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_90...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_90\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m141.762\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:31\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_90...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_90\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m143.799\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-26 04:06:21\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こんにちは、今日はよくいらっしゃいましたね。どうぞ、楽な姿勢でお座りください。\n",
      "\u001b[32m10-26 04:06:22\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(254976,)\n",
      "\u001b[32m10-26 04:06:22\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、ありがとうございます。\n",
      "\u001b[32m10-26 04:06:22\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(98816,)\n",
      "\u001b[32m10-26 04:06:22\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ。今日は、どのようなことについてお話ししたいと思われましたか？\n",
      "\u001b[32m10-26 04:06:22\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(201728,)\n",
      "\u001b[32m10-26 04:06:22\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "最近、仕事もそうですが、漠然とした不安や焦りを感じています。何が原因か、自分でも分からなくて。\n",
      "\u001b[32m10-26 04:06:23\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(368128,)\n",
      "\u001b[32m10-26 04:06:23\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど。漠然とした不安や焦りですね。ご自身でも原因が掴みかねていると。そうなんですね。\n",
      "\u001b[32m10-26 04:06:23\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(344064,)\n",
      "\u001b[32m10-26 04:06:23\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "うーん……特別な出来事はないのですが、いつも何かに追われるような、落ち着かない気持ちで。\n",
      "\u001b[32m10-26 04:06:23\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(299520,)\n",
      "\u001b[32m10-26 04:06:23\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、はい。何かに追われている感覚で、落ち着かない気持ちが続いていると。\n",
      "\u001b[32m10-26 04:06:23\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(260608,)\n",
      "\u001b[32m10-26 04:06:23\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "周りと比べて自分だけ取り残されているような気がして……何を頑張ればいいかも分からず、余計に焦ってしまいます。\n",
      "\u001b[32m10-26 04:06:24\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(348672,)\n",
      "\u001b[32m10-26 04:06:24\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど。周りと比べて取り残されているような感覚で焦りを感じていらっしゃるんですね。それは辛いお気持ちですね。\n",
      "\u001b[32m10-26 04:06:24\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(405504,)\n",
      "\u001b[32m10-26 04:06:24\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい……話していると、少し整理がつく気がします。\n",
      "\u001b[32m10-26 04:06:24\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(172032,)\n",
      "\u001b[32m10-26 04:06:24\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、良かったです。今日は不安や焦りについて伺いましたね。次回は、どんな時に焦りを感じやすいのか、もう少し掘り下げていけたらと思うのですが、いかがでしょうか？\n",
      "\u001b[32m10-26 04:06:24\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(552448,)\n",
      "\u001b[32m10-26 04:06:24\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、ぜひお願いします。\n",
      "\u001b[32m10-26 04:06:24\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(90112,)\n",
      "\u001b[32m10-26 04:06:24\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ありがとうございます。では、次回のカウンセリングは〇月〇日の〇時でよろしいでしょうか？\n",
      "\u001b[32m10-26 04:06:25\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(244736,)\n",
      "\u001b[32m10-26 04:06:25\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "大丈夫です。\n",
      "\u001b[32m10-26 04:06:25\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(40448,)\n",
      "\u001b[32m10-26 04:06:25\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "今日はありがとうございました。\n",
      "\u001b[32m10-26 04:06:25\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(78848,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:17\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h\u001b[?25l"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_91...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_91\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m126.624\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:17\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_91...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_91\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m131.039\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-26 04:11:16\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "〇〇さん、こんにちは。今日はこれまでのセッションを少し振り返ってみて、この先のことについてお話しできたらと思うのですが、いかがでしょうか？\n",
      "\u001b[32m10-26 04:11:17\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(445952,)\n",
      "\u001b[32m10-26 04:11:17\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、なんだかもうそんな時期なんだなあって感じですね。あっという間でした。\n",
      "\u001b[32m10-26 04:11:17\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(254464,)\n",
      "\u001b[32m10-26 04:11:17\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、そうですね。この数ヶ月を振り返ってみて、〇〇さんご自身で、何か変わったなと感じることはありますか？\n",
      "\u001b[32m10-26 04:11:17\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(346112,)\n",
      "\u001b[32m10-26 04:11:17\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "うーん…最初は、漠然と仕事も私生活も、全部うまくいってない気がして焦っていたんですけど、今は少し、何に不安を感じているのか、わかるようになってきた気がします。\n",
      "\u001b[32m10-26 04:11:18\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(540672,)\n",
      "\u001b[32m10-26 04:11:18\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど。漠然とした不安が、より具体的に見えてきたんですね。それは大きな変化ですね。具体的に、どんな時にそう感じますか？\n",
      "\u001b[32m10-26 04:11:18\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(473088,)\n",
      "\u001b[32m10-26 04:11:18\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "例えば、以前は「また失敗するかも」って思うと、何も手につかなかったんですけど、最近は「失敗しても、まあ仕方ないか」とか、「じゃあ、次どうしよう？」って、少し冷静に考えられるようになったりして。\n",
      "\u001b[32m10-26 04:11:18\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(616448,)\n",
      "\u001b[32m10-26 04:11:18\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、素晴らしい変化ですね。そうやって、ご自身で気持ちの切り替えができるようになったのですね。\n",
      "\u001b[32m10-26 04:11:19\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(300032,)\n",
      "\u001b[32m10-26 04:11:19\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、先生とのセッションで、自分の考え方のパターンに気づかせてもらってから、少しずつ意識するようにしています。\n",
      "\u001b[32m10-26 04:11:19\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(343040,)\n",
      "\u001b[32m10-26 04:11:19\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうなんですね。ご自身でしっかりと気づきを活かされているのがよくわかります。そんな〇〇さんを見ていると、そろそろ、このセッションも終わりが見えてきているのかな、とも思うのですが、〇〇さんはいかがですか？\n",
      "\u001b[32m10-26 04:11:19\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(603136,)\n",
      "\u001b[32m10-26 04:11:19\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうですね…正直、少し寂しいような、でも、自分でやっていけるかなっていう自信も、少しずつ出てきたような…複雑な気持ちです。\n",
      "\u001b[32m10-26 04:11:19\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(463872,)\n",
      "\u001b[32m10-26 04:11:19\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、そう感じられるのも、とても自然なことですよ。これまでの〇〇さんの頑張りが、今の自信につながっているのですね。残りの数回で、この学びをどう継続していくか、具体的な準備を一緒に考えていきましょう。\n",
      "\u001b[32m10-26 04:11:20\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(714240,)\n",
      "\u001b[32m10-26 04:11:20\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、ありがとうございます。なんだか、少し気持ちが楽になりました。\n",
      "\u001b[32m10-26 04:11:20\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(234496,)\n",
      "\u001b[32m10-26 04:11:20\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、よかったです。では、次回はその準備について、もう少し具体的に話し合いましょう。今日はここまでにしましょうか。\n",
      "\u001b[32m10-26 04:11:20\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(392704,)\n",
      "\u001b[32m10-26 04:11:20\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、ありがとうございました。\n",
      "\u001b[32m10-26 04:11:20\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(101888,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:30\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_92...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_92\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m143.028\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:29\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_92...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_92\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m142.315\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-26 04:16:49\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こんにちは、〇〇さん。今日は来てくださってありがとうございます。今日はどんなことについてお話ししたいですか？\n",
      "\u001b[32m10-26 04:16:49\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(299520,)\n",
      "\u001b[32m10-26 04:16:49\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、なんか、最近仕事のこともそうですけど、何となく毎日がうまくいってない気がして…漠然とした不安があるんです。\n",
      "\u001b[32m10-26 04:16:49\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(390144,)\n",
      "\u001b[32m10-26 04:16:49\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうなんですね。漠然とした不安、ですか。ええ、はい。\n",
      "\u001b[32m10-26 04:16:49\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(194560,)\n",
      "\u001b[32m10-26 04:16:49\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。仕事でも集中できないことが増えて、焦るのに何も手につかなくて…。家でも全然リラックスできなくて。\n",
      "\u001b[32m10-26 04:16:49\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(381440,)\n",
      "\u001b[32m10-26 04:16:49\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど。仕事と家、両方でつらさを感じていらっしゃるのですね。\n",
      "\u001b[32m10-26 04:16:50\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(226304,)\n",
      "\u001b[32m10-26 04:16:50\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうなんです。このままでいいのかって、ずっと考えてしまって。\n",
      "\u001b[32m10-26 04:16:50\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(197632,)\n",
      "\u001b[32m10-26 04:16:50\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、そうなんですね。「どうしたらいいか分からない」というお気持ち、よく分かります。心もお体も疲れてしまいますよね。\n",
      "\u001b[32m10-26 04:16:50\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(410112,)\n",
      "\u001b[32m10-26 04:16:50\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、まさにそんな感じです。毎日が重くて。\n",
      "\u001b[32m10-26 04:16:50\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(177664,)\n",
      "\u001b[32m10-26 04:16:50\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "今日は、そうしたお気持ちを話してくださってありがとうございます。〇〇さんの不安の背景を、これから一緒に見ていけたらと思います。\n",
      "\u001b[32m10-26 04:16:50\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(398336,)\n",
      "\u001b[32m10-26 04:16:50\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、お願いします。\n",
      "\u001b[32m10-26 04:16:51\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(78848,)\n",
      "\u001b[32m10-26 04:16:51\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ。今日はまず、お話を聞かせていただけて良かったです。次回は、どんな時に不安を感じやすいかなど、もう少し詳しくお話しできたらと思いますが、いかがでしょうか。\n",
      "\u001b[32m10-26 04:16:51\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(544256,)\n",
      "\u001b[32m10-26 04:16:51\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、大丈夫です。よろしくお願いします。\n",
      "\u001b[32m10-26 04:16:51\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(144384,)\n",
      "\u001b[32m10-26 04:16:51\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、ありがとうございます。では、本日はこれで終わりにしましょう。お気をつけてお帰りくださいね。\n",
      "\u001b[32m10-26 04:16:51\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(310272,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:19\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_93...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_93\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m131.395\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:16\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_93...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_93\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m127.130\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-26 04:21:54\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こんにちは、Bさん。今日は、どんなことをお話ししたい気分ですか？無理なく、お話しできる範囲で大丈夫ですよ。\n",
      "\u001b[32m10-26 04:21:54\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(353792,)\n",
      "\u001b[32m10-26 04:21:54\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こんにちは。最近、漠然とした不安がずっとあって、何から話したらいいか分からなくて…。\n",
      "\u001b[32m10-26 04:21:55\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(300544,)\n",
      "\u001b[32m10-26 04:21:55\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうなんですね。漠然とした不安。はい。何か、特に気になっていることはありますか？\n",
      "\u001b[32m10-26 04:21:55\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(293376,)\n",
      "\u001b[32m10-26 04:21:55\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "仕事が一番ですが、家でも落ち着かなくて。休日も休んだ気がしません。\n",
      "\u001b[32m10-26 04:21:55\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(249856,)\n",
      "\u001b[32m10-26 04:21:55\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど。お仕事がきっかけで、ご自宅でも落ち着かず、休日もすっきりしない感じなんですね。ええ。\n",
      "\u001b[32m10-26 04:21:56\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(347136,)\n",
      "\u001b[32m10-26 04:21:56\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。何をやっても集中できないし、趣味も手につかないんです。このままでいいのかなって焦りばかりで…。\n",
      "\u001b[32m10-26 04:21:56\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(330752,)\n",
      "\u001b[32m10-26 04:21:56\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうでしたか。集中できない、趣味も手につかないことで、焦りを感じていらっしゃるのですね。ええ、もしかしたら、その漠然とした不安の背景に、Bさんが頑張ってこられたことなどが関係しているのかもしれませんね。\n",
      "\u001b[32m10-26 04:21:56\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(691200,)\n",
      "\u001b[32m10-26 04:21:56\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、そうかもしれません。原因が自分でも分からなくて…。\n",
      "\u001b[32m10-26 04:21:56\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(204800,)\n",
      "\u001b[32m10-26 04:21:56\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうですね。今日は、漠然とした不安や、それが日常生活にどう影響しているかお話しいただきました。これから、もう少し詳しく伺うことで、整理されていくかもしれませんね。よろしければ、また来週、お話しする時間を設けられませんか？\n",
      "\u001b[32m10-26 04:21:57\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(798720,)\n",
      "\u001b[32m10-26 04:21:57\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、ぜひお願いします。少し話しただけでも、少し楽になった気がします。\n",
      "\u001b[32m10-26 04:21:57\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(258048,)\n",
      "\u001b[32m10-26 04:21:57\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "それはよかったです。では、次回の予約についてご案内しますね。今日はありがとうございました。\n",
      "\u001b[32m10-26 04:21:57\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(302080,)\n",
      "\u001b[32m10-26 04:21:57\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こちらこそ、ありがとうございました。\n",
      "\u001b[32m10-26 04:21:57\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(114176,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:19\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_94...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_94\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m133.075\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:21\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_94...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_94\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m137.126\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-26 04:26:58\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "Bさん、前回のお話から、日常生活で「〜すべきだ」と感じることが、少しお辛く感じられている、というお話がありましたね。今日はそのあたりを少し深掘りできたらと思うのですが、いかがでしょうか？\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: JPCommonLabel_push_word() in jpcommon_label.c: First mora should not be long vowel symbol.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-26 04:26:58\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(651264,)\n",
      "\u001b[32m10-26 04:26:58\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、なんだか、いつも「こうしなきゃ」とか「もっと頑張るべきだ」って、頭の中で繰り返している気がして…疲れてしまいます。\n",
      "\u001b[32m10-26 04:26:58\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(397824,)\n",
      "\u001b[32m10-26 04:26:58\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうなんですね。ええ。具体的に、どんな時にそう強く感じることが多いですか？ 例えば、最近あった出来事などでも構いませんよ。\n",
      "\u001b[32m10-26 04:26:59\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(438784,)\n",
      "\u001b[32m10-26 04:26:59\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええと、仕事で新しいプロジェクトを任された時なんですが、「完璧にこなさなければならない」って思ってしまって。少しでもミスがあると、もうだめだ、って。\n",
      "\u001b[32m10-26 04:26:59\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(475648,)\n",
      "\u001b[32m10-26 04:26:59\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど。完璧にこなさなければならない、というお気持ちなのですね。その時、Bさんはどんな風に感じますか？ 気持ちの部分で。\n",
      "\u001b[32m10-26 04:26:59\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(437760,)\n",
      "\u001b[32m10-26 04:26:59\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "不安が押し寄せてきて、焦ってしまいます。失敗が許されないような気がして、手が震えたりすることもあって…。\n",
      "\u001b[32m10-26 04:26:59\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(332800,)\n",
      "\u001b[32m10-26 04:26:59\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "不安と焦りを感じて、お身体にも反応が出ているんですね。はい。その「完璧にこなすべきだ」という考えが、今のBさんにとって、どのような影響を与えていると感じますか？\n",
      "\u001b[32m10-26 04:27:00\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(593920,)\n",
      "\u001b[32m10-26 04:27:00\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "うーん…正直、すごくしんどいです。いつも気が張っていて、リラックスできないし、休んでいても罪悪感を感じてしまって。\n",
      "\u001b[32m10-26 04:27:00\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(381440,)\n",
      "\u001b[32m10-26 04:27:00\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、そうでしたか。常に気が張ってしまって、リラックスも難しいというのは、本当に大変なことですね。その「完璧であるべき」という考えが、Bさんを少し苦しめているように聞こえますが、いかがでしょうか？\n",
      "\u001b[32m10-26 04:27:00\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(736768,)\n",
      "\u001b[32m10-26 04:27:00\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "言われてみれば、そうですね。でも、そう思わないと、手を抜いてしまいそうで怖いんです。\n",
      "\u001b[32m10-26 04:27:01\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(241152,)\n",
      "\u001b[32m10-26 04:27:01\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、そのお気持ちもよくわかりますよ。手を抜いてしまうのではないか、という不安があるのですね。でも、完璧ではない自分でも、 Bさんにはできること、十分頑張っていることがある、と思える瞬間はありますか？\n",
      "\u001b[32m10-26 04:27:01\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(688128,)\n",
      "\u001b[32m10-26 04:27:01\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "あ…そう言われると、確かに、周りの人には「よくやってるね」って言われることもあります。でも、自分の中ではまだまだ、って。\n",
      "\u001b[32m10-26 04:27:01\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(395264,)\n",
      "\u001b[32m10-26 04:27:01\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうなんですね。ええ。ご自身では「まだまだ」と感じていても、周りの方はBさんの頑張りを認めてくれている。その「まだ足りない」というお気持ちも大切ですが、少しだけ、その周りの方の言葉にも耳を傾けてみても良いかもしれませんね。\n",
      "\u001b[32m10-26 04:27:01\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(790528,)\n",
      "\u001b[32m10-26 04:27:01\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい…少し、考えてみます。\n",
      "\u001b[32m10-26 04:27:02\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(107008,)\n",
      "\u001b[32m10-26 04:27:02\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ。今日のセッションで、「完璧であるべき」という考えが、ご自身を苦しめていること、そして、その考えがどこから来ているのか、少し気づきがあったように思います。はい。この気づきを大切にしながら、次回はもう少し、その考え方との付き合い方について、一緒に考えていけたらと思います。\n",
      "\u001b[32m10-26 04:27:02\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(998400,)\n",
      "\u001b[32m10-26 04:27:02\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、ありがとうございます。少し、気持ちが楽になった気がします。\n",
      "\u001b[32m10-26 04:27:02\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(221184,)\n",
      "\u001b[32m10-26 04:27:02\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "それは良かったです。また来週、お会いしましょう。\n",
      "\u001b[32m10-26 04:27:02\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(155648,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:38\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_95...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_95\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m155.207\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:46\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_95...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_95\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m165.288\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-26 04:33:18\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こんにちは、Bさん。今日はどんなことをお話ししたいですか？少しでも心が軽くなるような時間になれば嬉しいのですが。\n",
      "\u001b[32m10-26 04:33:18\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(370688,)\n",
      "\u001b[32m10-26 04:33:18\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こんにちは。最近、なんだかいつも焦っていて…もっとしっかりしなきゃ、って思うんですけど、なかなかうまくいかなくて。\n",
      "\u001b[32m10-26 04:33:19\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(398848,)\n",
      "\u001b[32m10-26 04:33:19\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうなんですね。ええ、「もっとしっかりしなきゃ」というお気持ち、もう少し詳しく聞かせてもらえますか？\n",
      "\u001b[32m10-26 04:33:19\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(324608,)\n",
      "\u001b[32m10-26 04:33:19\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。仕事でもプライベートでも、「完璧にこなすべきだ」って思ってしまうんです。手を抜くとダメだって。休む時も落ち着かなくて…。\n",
      "\u001b[32m10-26 04:33:19\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(449536,)\n",
      "\u001b[32m10-26 04:33:19\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど…「完璧にこなすべきだ」というお気持ちが、Bさんを苦しめているのですね。その考えは、どんな気持ちにさせていますか？\n",
      "\u001b[32m10-26 04:33:19\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(435200,)\n",
      "\u001b[32m10-26 04:33:19\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ミスするとすごく落ち込んで。「どうしてこんなこともできないんだろう」って自分を責めてしまいます。いつも緊張して、不安で…。周りに迷惑かけたくなくて。\n",
      "\u001b[32m10-26 04:33:20\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(503296,)\n",
      "\u001b[32m10-26 04:33:20\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、お辛いですね。周りに迷惑をかけたくないというお気持ち、よくわかります。その「〜すべきだ」という考えが、本当にBさんを守っているのか、それとも窮屈にさせているのか…少し考えてみるのもいいかもしれませんね。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: JPCommonLabel_push_word() in jpcommon_label.c: First mora should not be long vowel symbol.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-26 04:33:20\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(783872,)\n",
      "\u001b[32m10-26 04:33:20\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうですね…。いつも息苦しいです。でも、他にどう考えたらいいのか、わからなくて…。\n",
      "\u001b[32m10-26 04:33:20\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(313856,)\n",
      "\u001b[32m10-26 04:33:20\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、大丈夫ですよ。今日は、Bさんが「〜すべきだ」という考えで苦しんでいるかもしれない、ということに気づけただけでも大きな一歩です。次回は、その「〜すべきだ」がどんな時に強く現れるのか、もう少し具体的に見ていきましょうか。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: JPCommonLabel_push_word() in jpcommon_label.c: First mora should not be long vowel symbol.\n",
      "WARNING: JPCommonLabel_push_word() in jpcommon_label.c: First mora should not be long vowel symbol.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-26 04:33:21\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(762368,)\n",
      "\u001b[32m10-26 04:33:21\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、わかりました。\n",
      "\u001b[32m10-26 04:33:21\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(78336,)\n",
      "\u001b[32m10-26 04:33:21\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ありがとうございます。今日のところはここまでとしましょうか。次回の予約は、来週の同じ曜日、時間でよろしいでしょうか？\n",
      "\u001b[32m10-26 04:33:21\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(392704,)\n",
      "\u001b[32m10-26 04:33:21\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、大丈夫です。よろしくお願いします。\n",
      "\u001b[32m10-26 04:33:21\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(146432,)\n",
      "\u001b[32m10-26 04:33:21\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こちらこそ、また来週お待ちしておりますね。ゆっくりお過ごしください。\n",
      "\u001b[32m10-26 04:33:21\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(230912,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:26\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_96...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_96\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m142.550\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:25\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:06\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_96...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_96\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m145.721\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-26 04:39:06\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "〇〇さん、こんにちは。前回の漠然とした焦りの部分、今日は少し掘り下げてみませんか？\n",
      "\u001b[32m10-26 04:39:06\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(296960,)\n",
      "\u001b[32m10-26 04:39:06\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。最近も、『もっとこうすべきだ』とか、『これくらいはできて当然だ』と、自分を追い込んでしまうんです。\n",
      "\u001b[32m10-26 04:39:07\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(309248,)\n",
      "\u001b[32m10-26 04:39:07\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど。その『〜すべきだ』という考え方が、焦りにつながっているんですね。どんな時にそう感じますか？\n",
      "\u001b[32m10-26 04:39:07\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(343040,)\n",
      "\u001b[32m10-26 04:39:07\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "仕事で新しいプロジェクトを任されたり、家事をしている時でも、『完璧にこなすべき』って。うまくいかないと、すごく落ち込みます。\n",
      "\u001b[32m10-26 04:39:07\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(401408,)\n",
      "\u001b[32m10-26 04:39:07\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "『完璧にすべき』という考えが、〇〇さんを苦しめているのかもしれませんね。この『〜すべき思考』、少し見ていきましょうか。\n",
      "\u001b[32m10-26 04:39:07\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(368640,)\n",
      "\u001b[32m10-26 04:39:07\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい…。確かに、いつも自分を追い込んでいる気がします。\n",
      "\u001b[32m10-26 04:39:08\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(166912,)\n",
      "\u001b[32m10-26 04:39:08\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ。例えば、『できる範囲で最善を尽くそう』と、少し言葉を変えてみたら、どんな気持ちになりそうでしょうか？\n",
      "\u001b[32m10-26 04:39:08\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(334336,)\n",
      "\u001b[32m10-26 04:39:08\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "うーん…最初は不安ですけど、少し楽になる気もします。いつも『〜すべき』ばかりで疲れているんだなって。\n",
      "\u001b[32m10-26 04:39:08\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(334848,)\n",
      "\u001b[32m10-26 04:39:08\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうですね。完璧でなくても、十分頑張っているご自身を認めること、大切ですよ。捉え方を変えられたら、気持ちも楽になるかもしれません。\n",
      "\u001b[32m10-26 04:39:08\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(489472,)\n",
      "\u001b[32m10-26 04:39:08\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。少しずつ、そう思えるようにしていきたいです。\n",
      "\u001b[32m10-26 04:39:09\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(157696,)\n",
      "\u001b[32m10-26 04:39:09\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "素晴らしいですね。では次回までに、『〜すべきだ』と考えそうな時に、『こうするのもアリかな？』と、少しだけ選択肢を広げてみることを試してみませんか？\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: JPCommonLabel_push_word() in jpcommon_label.c: First mora should not be long vowel symbol.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-26 04:39:09\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(500224,)\n",
      "\u001b[32m10-26 04:39:09\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、やってみます。\n",
      "\u001b[32m10-26 04:39:09\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(77824,)\n",
      "\u001b[32m10-26 04:39:09\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ありがとうございます。今日は、ご自身の『〜すべき思考』について見つめる良い機会になったと思います。また次回、その時の気持ちの変化など、お聞かせくださいね。\n",
      "\u001b[32m10-26 04:39:09\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(508928,)\n",
      "\u001b[32m10-26 04:39:09\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、ありがとうございました。\n",
      "\u001b[32m10-26 04:39:09\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(100352,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:19\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_97...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_97\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m130.065\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:20\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_97...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_97\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m137.284\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-26 04:44:18\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こんにちは、〇〇さん。今日は何かお話ししたいことはありますか？\n",
      "\u001b[32m10-26 04:44:18\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(179200,)\n",
      "\u001b[32m10-26 04:44:18\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こんにちは、先生。最近、どうも落ち着かなくて。何をしていても「もっとちゃんとしないと」って、焦ってしまうんです。\n",
      "\u001b[32m10-26 04:44:18\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(367104,)\n",
      "\u001b[32m10-26 04:44:18\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "「もっとちゃんとしないと」ですね。どんな時にそう思われますか？\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: JPCommonLabel_insert_pause() in jpcommon_label.c: First mora should not be short pause.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-26 04:44:19\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(190464,)\n",
      "\u001b[32m10-26 04:44:19\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "仕事でも家事でも、「完璧にやるべきだ」って。でも、それができなくて落ち込んだり、不安になったり…。\n",
      "\u001b[32m10-26 04:44:19\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(323072,)\n",
      "\u001b[32m10-26 04:44:19\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "「完璧にやるべきだ」というお気持ち。その時、どんな感情が湧きますか？\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: JPCommonLabel_insert_pause() in jpcommon_label.c: First mora should not be short pause.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-26 04:44:19\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(224768,)\n",
      "\u001b[32m10-26 04:44:19\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "不安と焦りですね。ずっと頑張っていないといけないような気がして、苦しいです。\n",
      "\u001b[32m10-26 04:44:20\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(237056,)\n",
      "\u001b[32m10-26 04:44:20\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、不安や焦りですね。「〜すべきだ」という考えが、〇〇さんを苦しめているのかもしれませんね。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: JPCommonLabel_push_word() in jpcommon_label.c: First mora should not be long vowel symbol.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-26 04:44:20\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(314368,)\n",
      "\u001b[32m10-26 04:44:20\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "確かに、そうかも…。いつも自分を追い立てている気がします。\n",
      "\u001b[32m10-26 04:44:20\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(193536,)\n",
      "\u001b[32m10-26 04:44:20\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "もし、「完璧でなくても大丈夫」だとしたら、どう感じられるでしょう？\n",
      "\u001b[32m10-26 04:44:20\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(212992,)\n",
      "\u001b[32m10-26 04:44:20\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そう思えたら、少し楽になるかな…でも、なかなかそうは思えなくて。\n",
      "\u001b[32m10-26 04:44:21\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(254976,)\n",
      "\u001b[32m10-26 04:44:21\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。すぐに変わらなくても大丈夫ですよ。まず、その「〜すべきだ」という考えに気づくことが大切です。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: JPCommonLabel_push_word() in jpcommon_label.c: First mora should not be long vowel symbol.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-26 04:44:21\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(322048,)\n",
      "\u001b[32m10-26 04:44:21\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。自分では気づいていなかったです。\n",
      "\u001b[32m10-26 04:44:21\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(122368,)\n",
      "\u001b[32m10-26 04:44:21\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "素晴らしい気づきです。次回まで、日常生活で「〜すべきだ」と思った時に、どんな気持ちになるか、少し意識して観察してみるのはいかがでしょう？\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: JPCommonLabel_push_word() in jpcommon_label.c: First mora should not be long vowel symbol.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-26 04:44:21\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(465920,)\n",
      "\u001b[32m10-26 04:44:21\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "わかりました。試してみます。\n",
      "\u001b[32m10-26 04:44:21\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(107520,)\n",
      "\u001b[32m10-26 04:44:21\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、ありがとうございます。では、今日のところはここまで。来週またお話し聞かせてくださいね。\n",
      "\u001b[32m10-26 04:44:22\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(317952,)\n",
      "\u001b[32m10-26 04:44:22\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、来週もよろしくお願いします。\n",
      "\u001b[32m10-26 04:44:22\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(120832,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:17\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_98...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_98\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m126.462\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:17\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_98...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_98\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m134.706\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m10-26 04:49:19\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "〇〇さん、こんにちは。今日は来てくださってありがとうございます。少し緊張されているかもしれませんが、ゆっくり、〇〇さんのペースでお話しくださいね。今日はどんなことをお話ししたい気分ですか？\n",
      "\u001b[32m10-26 04:49:19\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(587264,)\n",
      "\u001b[32m10-26 04:49:19\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こんにちは。あの、なんだか最近、仕事のこともそうですし、家でもなんだか落ち着かないというか、漠然とした不安があるんです。特に何かあったわけじゃないんですけど…\n",
      "\u001b[32m10-26 04:49:20\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(529408,)\n",
      "\u001b[32m10-26 04:49:20\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、そうなんですね。漠然とした不安が、仕事だけでなく、日常生活でも感じていらっしゃるのですね。はい。何か具体的なきっかけというよりは、日々の中で、なんとなくそういった気持ちが募ってきた、ということでしょうか。\n",
      "\u001b[32m10-26 04:49:20\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(734208,)\n",
      "\u001b[32m10-26 04:49:20\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうですね、まさにそんな感じです。朝起きてもスッキリしないですし、夜もなかなか寝付けなくて…このままじゃいけないってばかり考えてしまって。\n",
      "\u001b[32m10-26 04:49:20\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(477696,)\n",
      "\u001b[32m10-26 04:49:20\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど。朝もスッキリせず、夜も寝付けない時もあるのですね。はい。このままじゃいけない、という焦るお気持ちも感じていらっしゃるのですね。それは、お辛いですね。\n",
      "\u001b[32m10-26 04:49:21\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(585728,)\n",
      "\u001b[32m10-26 04:49:21\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい…でも、どうしたらいいか分からなくて。誰かに相談するのも、大したことじゃないのにって思ってしまって。\n",
      "\u001b[32m10-26 04:49:21\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(345088,)\n",
      "\u001b[32m10-26 04:49:21\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ。どうしたらいいか分からない、というお気持ち、とてもよく分かります。そして、大したことじゃない、と思ってしまうのですね。お一人で抱えられていたのですね。こうしてお話ししてくださって、ありがとうございます。\n",
      "\u001b[32m10-26 04:49:21\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(683520,)\n",
      "\u001b[32m10-26 04:49:21\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こちらこそ、ありがとうございます。話しているうちに、少し落ち着いてきました。\n",
      "\u001b[32m10-26 04:49:22\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(232960,)\n",
      "\u001b[32m10-26 04:49:22\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そう言っていただけて、私も嬉しいです。ええ。今日は、そうした〇〇さんの今のお気持ちを、もう少し詳しくお聞かせいただけますか。その漠然とした不安や焦りが、どんな時に強く感じられるのか、など、一緒に考えていけたらと思っています。\n",
      "\u001b[32m10-26 04:49:22\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(808448,)\n",
      "\u001b[32m10-26 04:49:22\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、そうですね…。どんな時に…\n",
      "\u001b[32m10-26 04:49:22\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(146944,)\n",
      "\u001b[32m10-26 04:49:22\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ。焦るお気持ち、とてもよく伝わってきますよ。大丈夫ですから、ゆっくりお話しくださいね。今日は、〇〇さんが感じている漠然とした不安や、焦るお気持ちについて、お話しいただきありがとうございました。少しでも、お気持ちを話せて、安心できた部分があれば嬉しいのですが。\n",
      "\u001b[32m10-26 04:49:23\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(947712,)\n",
      "\u001b[32m10-26 04:49:23\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、なんだか少し、気持ちが軽くなった気がします。\n",
      "\u001b[32m10-26 04:49:23\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(177664,)\n",
      "\u001b[32m10-26 04:49:23\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "よかったです。次回は、今日お話しいただいたことの中から、特に気になっている部分をもう少し深掘りしていけたらと思います。また来週の同じ時間でよろしいでしょうか。\n",
      "\u001b[32m10-26 04:49:23\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(519680,)\n",
      "\u001b[32m10-26 04:49:23\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、大丈夫です。よろしくお願いします。\n",
      "\u001b[32m10-26 04:49:23\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(143360,)\n",
      "\u001b[32m10-26 04:49:23\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、ではまた来週、お待ちしておりますね。\n",
      "\u001b[32m10-26 04:49:23\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "(158720,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m   0%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:36\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_99...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mA_99\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m151.608\u001b[0m seconds                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m   1%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/100 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:02\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:40\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:05\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/1 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_99...\u001b[0m                                                               \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/Github/jmoshi-ft/gen_dialogue/data/sbv/mfa_output/\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95mB_99\u001b[0m!                                                                 \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m155.041\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 44min 21s, sys: 1min 16s, total: 45min 37s\n",
      "Wall time: 9h 10min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import soundfile as sf\n",
    "import json\n",
    "\n",
    "for i in range(file_name_num+1, gen_dial_num+file_name_num+1):\n",
    "\n",
    "    txt_dialogue_list = gen_txt_dialogue()\n",
    "    stereo = gen_audio_dialogue(txt_dialogue_list)\n",
    "    \n",
    "    wav_name = f\"{i}.wav\"\n",
    "    audio_file_path = os.path.join(audio_dir_path, wav_name)\n",
    "\n",
    "    # wavファイル出力\n",
    "    sf.write(audio_file_path, stereo, setting_sr)\n",
    "\n",
    "    json_data = alignment_audio_dialogue(txt_dialogue_list, audio_file_path, i)\n",
    "\n",
    "    json_name = f\"{i}.json\"\n",
    "    json_file_path = os.path.join(json_dir_path, json_name)\n",
    "    \n",
    "    # JSON出力\n",
    "    with open(json_file_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(json_data, f, ensure_ascii=False, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
